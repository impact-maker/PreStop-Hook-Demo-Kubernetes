I1009 09:44:19.805243  106109 factory.go:193] Registered Plugin "containerd"
  I1009 09:44:19.830035  106109 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  I1009 09:44:19.831962  106109 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  I1009 09:44:19.833783  106109 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  W1009 09:44:19.854813  106109 test_context.go:538] Unable to find in-cluster config, using default host : https://127.0.0.1:6443
  I1009 09:44:19.854859  106109 test_context.go:553] Tolerating taints "node-role.kubernetes.io/control-plane" when considering if nodes are ready
  I1009 09:44:19.854866 106109 test_context.go:561] The --provider flag is not set. Continuing as if --provider=skeleton had been used.
  I1009 09:44:19.854989  106109 feature_gate.go:387] feature gates: {map[]}
  I1009 09:44:19.855019  106109 feature_gate.go:387] feature gates: {map[]}
  I1009 09:44:19.855100  106109 internal_services.go:62] Starting e2e services...
  I1009 09:44:19.855107  106109 internal_services.go:116] Starting etcd
  I1009 09:44:20.782659  106109 internal_services.go:125] Starting API server
  I1009 09:44:20.782787  106109 plugins.go:83] "Registered admission plugin" plugin="NamespaceLifecycle"
  I1009 09:44:20.782805  106109 plugins.go:83] "Registered admission plugin" plugin="ValidatingAdmissionWebhook"
  I1009 09:44:20.782817  106109 plugins.go:83] "Registered admission plugin" plugin="MutatingAdmissionWebhook"
  I1009 09:44:20.782828  106109 plugins.go:83] "Registered admission plugin" plugin="ValidatingAdmissionPolicy"
  I1009 09:44:20.782840  106109 plugins.go:83] "Registered admission plugin" plugin="AlwaysAdmit"
  I1009 09:44:20.782850  106109 plugins.go:83] "Registered admission plugin" plugin="AlwaysPullImages"
  I1009 09:44:20.782860  106109 plugins.go:83] "Registered admission plugin" plugin="LimitPodHardAntiAffinityTopology"
  I1009 09:44:20.782872  106109 plugins.go:83] "Registered admission plugin" plugin="DefaultTolerationSeconds"
  I1009 09:44:20.782883  106109 plugins.go:83] "Registered admission plugin" plugin="DefaultIngressClass"
  I1009 09:44:20.782893  106109 plugins.go:83] "Registered admission plugin" plugin="DenyServiceExternalIPs"
  I1009 09:44:20.782901  106109 plugins.go:83] "Registered admission plugin" plugin="AlwaysDeny"
  I1009 09:44:20.782908  106109 plugins.go:83] "Registered admission plugin" plugin="EventRateLimit"
  I1009 09:44:20.782921  106109 plugins.go:83] "Registered admission plugin" plugin="ExtendedResourceToleration"
  I1009 09:44:20.782937  106109 plugins.go:83] "Registered admission plugin" plugin="OwnerReferencesPermissionEnforcement"
  I1009 09:44:20.782950  106109 plugins.go:83] "Registered admission plugin" plugin="ImagePolicyWebhook"
  I1009 09:44:20.782962  106109 plugins.go:83] "Registered admission plugin" plugin="LimitRanger"
  I1009 09:44:20.782970  106109 plugins.go:83] "Registered admission plugin" plugin="NamespaceAutoProvision"
  I1009 09:44:20.782978  106109 plugins.go:83] "Registered admission plugin" plugin="NamespaceExists"
  I1009 09:44:20.782986  106109 plugins.go:83] "Registered admission plugin" plugin="NodeRestriction"
  I1009 09:44:20.782994  106109 plugins.go:83] "Registered admission plugin" plugin="TaintNodesByCondition"
  I1009 09:44:20.783002  106109 plugins.go:83] "Registered admission plugin" plugin="PodNodeSelector"
  I1009 09:44:20.783014  106109 plugins.go:83] "Registered admission plugin" plugin="PodTolerationRestriction"
  I1009 09:44:20.783031  106109 plugins.go:83] "Registered admission plugin" plugin="RuntimeClass"
  I1009 09:44:20.783044  106109 plugins.go:83] "Registered admission plugin" plugin="ResourceQuota"
  I1009 09:44:20.783055  106109 plugins.go:83] "Registered admission plugin" plugin="PodSecurity"
  I1009 09:44:20.783064  106109 plugins.go:83] "Registered admission plugin" plugin="Priority"
  I1009 09:44:20.783072  106109 plugins.go:83] "Registered admission plugin" plugin="ServiceAccount"
  I1009 09:44:20.783082  106109 plugins.go:83] "Registered admission plugin" plugin="DefaultStorageClass"
  I1009 09:44:20.783103  106109 plugins.go:83] "Registered admission plugin" plugin="PersistentVolumeClaimResize"
  I1009 09:44:20.783112  106109 plugins.go:83] "Registered admission plugin" plugin="StorageObjectInUseProtection"
  I1009 09:44:20.783143  106109 plugins.go:83] "Registered admission plugin" plugin="CertificateApproval"
  I1009 09:44:20.783151  106109 plugins.go:83] "Registered admission plugin" plugin="CertificateSigning"
  I1009 09:44:20.783160  106109 plugins.go:83] "Registered admission plugin" plugin="ClusterTrustBundleAttest"
  I1009 09:44:20.783168  106109 plugins.go:83] "Registered admission plugin" plugin="CertificateSubjectRestriction"
  I1009 09:44:20.783484  106109 util.go:48] Running readiness check for service "apiserver"
  I1009 09:44:20.783537  106109 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  W1009 09:44:20.783583  106109 registry.go:345] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
  I1009 09:44:20.783594  106109 registry.go:379] setting kube:feature gate emulation version to 1.32
  I1009 09:44:20.783723  106109 interface.go:432] Looking for default routes with IPv4 addresses
  I1009 09:44:20.783732  106109 interface.go:437] Default route transits interface "eth0"
  I1009 09:44:20.783831  106109 interface.go:209] Interface eth0 is up
  I1009 09:44:20.783872  106109 interface.go:257] Interface "eth0" has 3 addresses :[82.112.230.236/24 2a02:4780:12:eebf::1/48 fe80::be24:11ff:fe5a:b1d8/64].
  I1009 09:44:20.783883  106109 interface.go:224] Checking addr  82.112.230.236/24.
  I1009 09:44:20.783891  106109 interface.go:231] IP found 82.112.230.236
  I1009 09:44:20.783900  106109 interface.go:263] Found valid IPv4 address 82.112.230.236 for interface "eth0".
  I1009 09:44:20.783909  106109 interface.go:443] Found active IP 82.112.230.236 
  I1009 09:44:20.783942  106109 options.go:228] external host was not specified, using 82.112.230.236
  W1009 09:44:20.783951  106109 authentication.go:804] AnonymousAuth is not allowed with the AlwaysAllow authorizer. Resetting AnonymousAuth to false. You should use a different authorizer
  I1009 09:44:20.784138  106109 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  I1009 09:44:20.784163  106109 server.go:142] Version: v1.32.0-alpha.1.139+b2031b3cb46e94-dirty
  I1009 09:44:20.784174  106109 server.go:144] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
  I1009 09:44:20.784477  106109 dynamic_serving_content.go:116] "Loaded a new cert/key pair" name="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key"
  I1009 09:44:21.053403  106109 apf_controller.go:292] NewTestableController "Controller" with serverConcurrencyLimit=600, name=Controller, asFieldManager="api-priority-and-fairness-config-consumer-v1"
  I1009 09:44:21.053531  106109 apf_controller.go:992] No exempt PriorityLevelConfiguration found, imagining one
  I1009 09:44:21.053574  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","exempt"]
  I1009 09:44:21.053590  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","exempt"]
  I1009 09:44:21.053600  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["exempt"]
  I1009 09:44:21.053613  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["exempt"]
  I1009 09:44:21.053630  106109 apf_controller.go:992] No catch-all PriorityLevelConfiguration found, imagining one
  I1009 09:44:21.053648  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","catch-all"]
  I1009 09:44:21.053662  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","catch-all"]
  I1009 09:44:21.053670  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["catch-all"]
  I1009 09:44:21.053679  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["catch-all"]
  I1009 09:44:21.053689  106109 apf_controller.go:898] Introducing queues for priority level "exempt": config={"type":"Exempt","exempt":{"nominalConcurrencyShares":0,"lendablePercent":0}}, nominalCL=0, lendableCL=0, borrowingCL=600, currentCL=0, quiescing=false (shares=0xc000133f10, shareSum=5)
  I1009 09:44:21.053749  106109 apf_controller.go:898] Introducing queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=600, lendableCL=0, borrowingCL=600, currentCL=600, quiescing=false (shares=0xc000582238, shareSum=5)
  I1009 09:44:21.053766  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=0 concurrencyDenominator=60 backstop=false
  I1009 09:44:21.053787  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=600 concurrencyDenominator=600 backstop=false
  I1009 09:44:21.053801  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["waiting","readOnly"]
  I1009 09:44:21.053810  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["waiting","mutating"]
  I1009 09:44:21.053818  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","readOnly"]
  I1009 09:44:21.053826  106109 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","mutating"]
  I1009 09:44:21.066947  106109 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
  I1009 09:44:21.069372  106109 plugins.go:157] Loaded 9 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
  I1009 09:44:21.069405  106109 plugins.go:160] Loaded 12 validating admission controller(s) successfully in the following order: LimitRanger,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
  I1009 09:44:21.069590  106109 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  I1009 09:44:21.069610  106109 instance.go:232] Using reconciler: lease
  I1009 09:44:21.069747  106109 storage_factory.go:270] storing apiServerIPInfo in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.073138  106109 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 09:44:21.074751  106109 reflector.go:341] Listing and watching *apiextensions.CustomResourceDefinition from storage/cacher.go:/apiextensions.k8s.io/customresourcedefinitions
  I1009 09:44:21.075828  106109 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 09:44:21.075958  106109 cacher.go:463] cacher (customresourcedefinitions.apiextensions.k8s.io): initialized
  I1009 09:44:21.076000  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.076010  106109 reflector.go:368] Caches populated for *apiextensions.CustomResourceDefinition from storage/cacher.go:/apiextensions.k8s.io/customresourcedefinitions
  I1009 09:44:21.076041  106109 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 09:44:21.076108  106109 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
  W1009 09:44:21.076123  106109 genericapiserver.go:765] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.078429  106109 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.080800  106109 deleted_kinds.go:60] NewResourceExpirationEvaluator with currentVersion: 1.32.
  I1009 09:44:21.081077  106109 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.081142  106109 etcd.go:416] "Not using watch cache" resource="events"
  I1009 09:44:21.082297  106109 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.082380  106109 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 09:44:21.083376  106109 reflector.go:341] Listing and watching *core.ResourceQuota from storage/cacher.go:/resourcequotas
  I1009 09:44:21.083512  106109 storage_factory.go:270] storing secrets in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.083555  106109 etcd.go:419] "Using watch cache" resource="secrets"
  I1009 09:44:21.083957  106109 cacher.go:463] cacher (resourcequotas): initialized
  I1009 09:44:21.083977  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.083984  106109 reflector.go:368] Caches populated for *core.ResourceQuota from storage/cacher.go:/resourcequotas
  I1009 09:44:21.084438  106109 reflector.go:341] Listing and watching *core.Secret from storage/cacher.go:/secrets
  I1009 09:44:21.084520  106109 storage_factory.go:270] storing configmaps in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.084553  106109 etcd.go:419] "Using watch cache" resource="configmaps"
  I1009 09:44:21.084794  106109 cacher.go:463] cacher (secrets): initialized
  I1009 09:44:21.084809  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.084819  106109 reflector.go:368] Caches populated for *core.Secret from storage/cacher.go:/secrets
  I1009 09:44:21.085393  106109 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.085417  106109 reflector.go:341] Listing and watching *core.ConfigMap from storage/cacher.go:/configmaps
  I1009 09:44:21.085482  106109 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 09:44:21.085965  106109 cacher.go:463] cacher (configmaps): initialized
  I1009 09:44:21.086000  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.086009  106109 reflector.go:368] Caches populated for *core.ConfigMap from storage/cacher.go:/configmaps
  I1009 09:44:21.086637  106109 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.086684  106109 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 09:44:21.086714  106109 reflector.go:341] Listing and watching *core.Namespace from storage/cacher.go:/namespaces
  I1009 09:44:21.087182  106109 cacher.go:463] cacher (namespaces): initialized
  I1009 09:44:21.087198  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.087208  106109 reflector.go:368] Caches populated for *core.Namespace from storage/cacher.go:/namespaces
  I1009 09:44:21.087545  106109 reflector.go:341] Listing and watching *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 09:44:21.087689  106109 storage_factory.go:270] storing podtemplates in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.087716  106109 etcd.go:419] "Using watch cache" resource="podtemplates"
  I1009 09:44:21.087902  106109 cacher.go:463] cacher (serviceaccounts): initialized
  I1009 09:44:21.087984  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.087999  106109 reflector.go:368] Caches populated for *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 09:44:21.088553  106109 storage_factory.go:270] storing limitranges in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.088598  106109 etcd.go:419] "Using watch cache" resource="limitranges"
  I1009 09:44:21.088569  106109 reflector.go:341] Listing and watching *core.PodTemplate from storage/cacher.go:/podtemplates
  I1009 09:44:21.088964  106109 cacher.go:463] cacher (podtemplates): initialized
  I1009 09:44:21.088976  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.088983  106109 reflector.go:368] Caches populated for *core.PodTemplate from storage/cacher.go:/podtemplates
  I1009 09:44:21.089310  106109 reflector.go:341] Listing and watching *core.LimitRange from storage/cacher.go:/limitranges
  I1009 09:44:21.089310  106109 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.089364  106109 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 09:44:21.089755  106109 cacher.go:463] cacher (limitranges): initialized
  I1009 09:44:21.089771  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.089778  106109 reflector.go:368] Caches populated for *core.LimitRange from storage/cacher.go:/limitranges
  I1009 09:44:21.090591  106109 reflector.go:341] Listing and watching *core.PersistentVolume from storage/cacher.go:/persistentvolumes
  I1009 09:44:21.090609  106109 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.090643  106109 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 09:44:21.093945  106109 cacher.go:463] cacher (persistentvolumes): initialized
  I1009 09:44:21.093975  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.093986  106109 reflector.go:368] Caches populated for *core.PersistentVolume from storage/cacher.go:/persistentvolumes
  I1009 09:44:21.094286  106109 storage_factory.go:270] storing endpoints in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.094337  106109 etcd.go:419] "Using watch cache" resource="endpoints"
  I1009 09:44:21.094377  106109 reflector.go:341] Listing and watching *core.PersistentVolumeClaim from storage/cacher.go:/persistentvolumeclaims
  I1009 09:44:21.094918  106109 cacher.go:463] cacher (persistentvolumeclaims): initialized
  I1009 09:44:21.094935  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.094942  106109 reflector.go:368] Caches populated for *core.PersistentVolumeClaim from storage/cacher.go:/persistentvolumeclaims
  I1009 09:44:21.095548  106109 reflector.go:341] Listing and watching *core.Endpoints from storage/cacher.go:/services/endpoints
  I1009 09:44:21.095569  106109 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.095605  106109 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 09:44:21.095856  106109 cacher.go:463] cacher (endpoints): initialized
  I1009 09:44:21.095866  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.095873  106109 reflector.go:368] Caches populated for *core.Endpoints from storage/cacher.go:/services/endpoints
  I1009 09:44:21.096267  106109 reflector.go:341] Listing and watching *core.Node from storage/cacher.go:/minions
  I1009 09:44:21.096291  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.096318  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.096520  106109 cacher.go:463] cacher (nodes): initialized
  I1009 09:44:21.096535  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.096541  106109 reflector.go:368] Caches populated for *core.Node from storage/cacher.go:/minions
  I1009 09:44:21.097091  106109 reflector.go:341] Listing and watching *core.Pod from storage/cacher.go:/pods
  I1009 09:44:21.097151  106109 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.097176  106109 etcd.go:419] "Using watch cache" resource="services"
  I1009 09:44:21.097505  106109 cacher.go:463] cacher (pods): initialized
  I1009 09:44:21.097522  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.097529  106109 reflector.go:368] Caches populated for *core.Pod from storage/cacher.go:/pods
  I1009 09:44:21.097967  106109 reflector.go:341] Listing and watching *core.Service from storage/cacher.go:/services/specs
  I1009 09:44:21.098012  106109 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.098047  106109 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 09:44:21.098325  106109 cacher.go:463] cacher (services): initialized
  I1009 09:44:21.098338  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.098345  106109 reflector.go:368] Caches populated for *core.Service from storage/cacher.go:/services/specs
  I1009 09:44:21.098960  106109 reflector.go:341] Listing and watching *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 09:44:21.099000  106109 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.099031  106109 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 09:44:21.099265  106109 cacher.go:463] cacher (serviceaccounts): initialized
  I1009 09:44:21.099277  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.099285  106109 reflector.go:368] Caches populated for *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 09:44:21.099733  106109 reflector.go:341] Listing and watching *core.ReplicationController from storage/cacher.go:/controllers
  I1009 09:44:21.100010  106109 cacher.go:463] cacher (replicationcontrollers): initialized
  I1009 09:44:21.100023  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.100030  106109 reflector.go:368] Caches populated for *core.ReplicationController from storage/cacher.go:/controllers
  I1009 09:44:21.100114  106109 apis.go:118] Enabling API group "".
  I1009 09:44:21.106235  106109 storage_factory.go:270] storing bindings in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.106285  106109 etcd.go:419] "Using watch cache" resource="bindings"
  I1009 09:44:21.106373  106109 storage_factory.go:270] storing componentstatuses in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.106403  106109 etcd.go:419] "Using watch cache" resource="componentstatuses"
  I1009 09:44:21.106707  106109 storage_factory.go:270] storing configmaps in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.106733  106109 etcd.go:419] "Using watch cache" resource="configmaps"
  I1009 09:44:21.106995  106109 storage_factory.go:270] storing endpoints in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.107015  106109 etcd.go:419] "Using watch cache" resource="endpoints"
  I1009 09:44:21.107312  106109 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.107333  106109 etcd.go:416] "Not using watch cache" resource="events"
  I1009 09:44:21.107657  106109 storage_factory.go:270] storing limitranges in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.107682  106109 etcd.go:419] "Using watch cache" resource="limitranges"
  I1009 09:44:21.107850  106109 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.107875  106109 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 09:44:21.107939  106109 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.107960  106109 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 09:44:21.108055  106109 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.108075  106109 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 09:44:21.108268  106109 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.108286  106109 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 09:44:21.108572  106109 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.108603  106109 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 09:44:21.108681  106109 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.108702  106109 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 09:44:21.109069  106109 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.109091  106109 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 09:44:21.109223  106109 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.109240  106109 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 09:44:21.109466  106109 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.109489  106109 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 09:44:21.109609  106109 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.109631  106109 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 09:44:21.109926  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.109945  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110034  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110054  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110112  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110127  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110259  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110276  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110332  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110352  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110418  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110445  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110522  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110540  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.110622  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.110648  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.111021  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.111042  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.111155  106109 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.111176  106109 etcd.go:419] "Using watch cache" resource="pods"
  I1009 09:44:21.111578  106109 storage_factory.go:270] storing podtemplates in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.111614  106109 etcd.go:419] "Using watch cache" resource="podtemplates"
  I1009 09:44:21.112028  106109 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.112049  106109 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 09:44:21.112166  106109 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.112184  106109 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 09:44:21.112347  106109 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.112375  106109 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 09:44:21.115146  106109 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.115190  106109 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 09:44:21.115326  106109 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.115347  106109 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 09:44:21.115778  106109 storage_factory.go:270] storing secrets in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.115806  106109 etcd.go:419] "Using watch cache" resource="secrets"
  I1009 09:44:21.116195  106109 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.116221  106109 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 09:44:21.116306  106109 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.116329  106109 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 09:44:21.116702  106109 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.116822  106109 etcd.go:419] "Using watch cache" resource="services"
  I1009 09:44:21.117167  106109 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.117190  106109 etcd.go:419] "Using watch cache" resource="services"
  I1009 09:44:21.117306  106109 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.117324  106109 etcd.go:419] "Using watch cache" resource="services"
  I1009 09:44:21.117391  106109 handler.go:286] Adding GroupVersion  v1 to ResourceManager
  I1009 09:44:21.117625  106109 apis.go:105] API group "internal.apiserver.k8s.io" is not enabled, skipping.
  I1009 09:44:21.117658  106109 apis.go:118] Enabling API group "authentication.k8s.io".
  I1009 09:44:21.117688  106109 apis.go:118] Enabling API group "authorization.k8s.io".
  I1009 09:44:21.117854  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.117902  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.119206  106109 reflector.go:341] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 09:44:21.119225  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.119259  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.119706  106109 cacher.go:463] cacher (horizontalpodautoscalers.autoscaling): initialized
  I1009 09:44:21.119725  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.119733  106109 reflector.go:368] Caches populated for *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 09:44:21.120471  106109 apis.go:118] Enabling API group "autoscaling".
  I1009 09:44:21.120479  106109 reflector.go:341] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 09:44:21.120831  106109 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.120901  106109 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 09:44:21.120998  106109 cacher.go:463] cacher (horizontalpodautoscalers.autoscaling): initialized
  I1009 09:44:21.121011  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.121019  106109 reflector.go:368] Caches populated for *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 09:44:21.128097  106109 reflector.go:341] Listing and watching *batch.Job from storage/cacher.go:/jobs
  I1009 09:44:21.128174  106109 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.128227  106109 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 09:44:21.128702  106109 cacher.go:463] cacher (jobs.batch): initialized
  I1009 09:44:21.128721  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.128728  106109 reflector.go:368] Caches populated for *batch.Job from storage/cacher.go:/jobs
  I1009 09:44:21.129181  106109 apis.go:118] Enabling API group "batch".
  I1009 09:44:21.129223  106109 reflector.go:341] Listing and watching *batch.CronJob from storage/cacher.go:/cronjobs
  I1009 09:44:21.129503  106109 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.129567  106109 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 09:44:21.129689  106109 cacher.go:463] cacher (cronjobs.batch): initialized
  I1009 09:44:21.129705  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.129712  106109 reflector.go:368] Caches populated for *batch.CronJob from storage/cacher.go:/cronjobs
  I1009 09:44:21.130502  106109 apis.go:118] Enabling API group "certificates.k8s.io".
  I1009 09:44:21.130530  106109 reflector.go:341] Listing and watching *certificates.CertificateSigningRequest from storage/cacher.go:/certificatesigningrequests
  I1009 09:44:21.130700  106109 storage_factory.go:270] storing leases.coordination.k8s.io in coordination.k8s.io/v1, reading as coordination.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.130735  106109 etcd.go:419] "Using watch cache" resource="leases.coordination.k8s.io"
  I1009 09:44:21.131109  106109 cacher.go:463] cacher (certificatesigningrequests.certificates.k8s.io): initialized
  I1009 09:44:21.131126  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.131138  106109 reflector.go:368] Caches populated for *certificates.CertificateSigningRequest from storage/cacher.go:/certificatesigningrequests
  I1009 09:44:21.131521  106109 apis.go:118] Enabling API group "coordination.k8s.io".
  I1009 09:44:21.131624  106109 reflector.go:341] Listing and watching *coordination.Lease from storage/cacher.go:/leases
  I1009 09:44:21.131692  106109 storage_factory.go:270] storing endpointslices.discovery.k8s.io in discovery.k8s.io/v1, reading as discovery.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.131730  106109 etcd.go:419] "Using watch cache" resource="endpointslices.discovery.k8s.io"
  I1009 09:44:21.132073  106109 cacher.go:463] cacher (leases.coordination.k8s.io): initialized
  I1009 09:44:21.132089  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.132097  106109 reflector.go:368] Caches populated for *coordination.Lease from storage/cacher.go:/leases
  I1009 09:44:21.132417  106109 apis.go:118] Enabling API group "discovery.k8s.io".
  I1009 09:44:21.132566  106109 reflector.go:341] Listing and watching *discovery.EndpointSlice from storage/cacher.go:/endpointslices
  I1009 09:44:21.132595  106109 storage_factory.go:270] storing networkpolicies.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.132631  106109 etcd.go:419] "Using watch cache" resource="networkpolicies.networking.k8s.io"
  I1009 09:44:21.132920  106109 cacher.go:463] cacher (endpointslices.discovery.k8s.io): initialized
  I1009 09:44:21.132933  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.132940  106109 reflector.go:368] Caches populated for *discovery.EndpointSlice from storage/cacher.go:/endpointslices
  I1009 09:44:21.134226  106109 reflector.go:341] Listing and watching *networking.NetworkPolicy from storage/cacher.go:/networkpolicies
  I1009 09:44:21.134300  106109 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.134339  106109 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 09:44:21.134798  106109 cacher.go:463] cacher (networkpolicies.networking.k8s.io): initialized
  I1009 09:44:21.134817  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.134826  106109 reflector.go:368] Caches populated for *networking.NetworkPolicy from storage/cacher.go:/networkpolicies
  I1009 09:44:21.135597  106109 reflector.go:341] Listing and watching *networking.Ingress from storage/cacher.go:/ingress
  I1009 09:44:21.135852  106109 storage_factory.go:270] storing ingressclasses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.135906  106109 etcd.go:419] "Using watch cache" resource="ingressclasses.networking.k8s.io"
  I1009 09:44:21.135940  106109 cacher.go:463] cacher (ingresses.networking.k8s.io): initialized
  I1009 09:44:21.135954  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.135964  106109 reflector.go:368] Caches populated for *networking.Ingress from storage/cacher.go:/ingress
  I1009 09:44:21.136964  106109 apis.go:118] Enabling API group "networking.k8s.io".
  I1009 09:44:21.137231  106109 storage_factory.go:270] storing runtimeclasses.node.k8s.io in node.k8s.io/v1, reading as node.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.137273  106109 etcd.go:419] "Using watch cache" resource="runtimeclasses.node.k8s.io"
  I1009 09:44:21.137370  106109 reflector.go:341] Listing and watching *networking.IngressClass from storage/cacher.go:/ingressclasses
  I1009 09:44:21.137795  106109 cacher.go:463] cacher (ingressclasses.networking.k8s.io): initialized
  I1009 09:44:21.137839  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.137850  106109 reflector.go:368] Caches populated for *networking.IngressClass from storage/cacher.go:/ingressclasses
  I1009 09:44:21.139003  106109 apis.go:118] Enabling API group "node.k8s.io".
  I1009 09:44:21.139125  106109 reflector.go:341] Listing and watching *node.RuntimeClass from storage/cacher.go:/runtimeclasses
  I1009 09:44:21.139310  106109 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.139345  106109 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 09:44:21.139526  106109 cacher.go:463] cacher (runtimeclasses.node.k8s.io): initialized
  I1009 09:44:21.139545  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.139558  106109 reflector.go:368] Caches populated for *node.RuntimeClass from storage/cacher.go:/runtimeclasses
  I1009 09:44:21.140215  106109 apis.go:118] Enabling API group "policy".
  I1009 09:44:21.140232  106109 reflector.go:341] Listing and watching *policy.PodDisruptionBudget from storage/cacher.go:/poddisruptionbudgets
  I1009 09:44:21.140363  106109 storage_factory.go:270] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.140414  106109 etcd.go:419] "Using watch cache" resource="roles.rbac.authorization.k8s.io"
  I1009 09:44:21.140642  106109 cacher.go:463] cacher (poddisruptionbudgets.policy): initialized
  I1009 09:44:21.140662  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.140672  106109 reflector.go:368] Caches populated for *policy.PodDisruptionBudget from storage/cacher.go:/poddisruptionbudgets
  I1009 09:44:21.141593  106109 reflector.go:341] Listing and watching *rbac.Role from storage/cacher.go:/roles
  I1009 09:44:21.141773  106109 storage_factory.go:270] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.141819  106109 etcd.go:419] "Using watch cache" resource="rolebindings.rbac.authorization.k8s.io"
  I1009 09:44:21.142049  106109 cacher.go:463] cacher (roles.rbac.authorization.k8s.io): initialized
  I1009 09:44:21.142078  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.142094  106109 reflector.go:368] Caches populated for *rbac.Role from storage/cacher.go:/roles
  I1009 09:44:21.142844  106109 storage_factory.go:270] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.142886  106109 etcd.go:419] "Using watch cache" resource="clusterroles.rbac.authorization.k8s.io"
  I1009 09:44:21.142888  106109 reflector.go:341] Listing and watching *rbac.RoleBinding from storage/cacher.go:/rolebindings
  I1009 09:44:21.143219  106109 cacher.go:463] cacher (rolebindings.rbac.authorization.k8s.io): initialized
  I1009 09:44:21.143246  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.143256  106109 reflector.go:368] Caches populated for *rbac.RoleBinding from storage/cacher.go:/rolebindings
  I1009 09:44:21.143782  106109 reflector.go:341] Listing and watching *rbac.ClusterRole from storage/cacher.go:/clusterroles
  I1009 09:44:21.143902  106109 storage_factory.go:270] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.143952  106109 etcd.go:419] "Using watch cache" resource="clusterrolebindings.rbac.authorization.k8s.io"
  I1009 09:44:21.144137  106109 cacher.go:463] cacher (clusterroles.rbac.authorization.k8s.io): initialized
  I1009 09:44:21.144157  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.144170  106109 reflector.go:368] Caches populated for *rbac.ClusterRole from storage/cacher.go:/clusterroles
  I1009 09:44:21.145022  106109 apis.go:118] Enabling API group "rbac.authorization.k8s.io".
  I1009 09:44:21.145265  106109 reflector.go:341] Listing and watching *rbac.ClusterRoleBinding from storage/cacher.go:/clusterrolebindings
  I1009 09:44:21.145738  106109 cacher.go:463] cacher (clusterrolebindings.rbac.authorization.k8s.io): initialized
  I1009 09:44:21.145761  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.145770  106109 reflector.go:368] Caches populated for *rbac.ClusterRoleBinding from storage/cacher.go:/clusterrolebindings
  I1009 09:44:21.145915  106109 hooks.go:96] skipping "rbac/bootstrap-roles" because it was explicitly disabled
  I1009 09:44:21.146104  106109 storage_factory.go:270] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io/v1, reading as scheduling.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.146148  106109 etcd.go:419] "Using watch cache" resource="priorityclasses.scheduling.k8s.io"
  I1009 09:44:21.147091  106109 apis.go:118] Enabling API group "scheduling.k8s.io".
  I1009 09:44:21.147222  106109 reflector.go:341] Listing and watching *scheduling.PriorityClass from storage/cacher.go:/priorityclasses
  I1009 09:44:21.147286  106109 storage_factory.go:270] storing storageclasses.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.147316  106109 etcd.go:419] "Using watch cache" resource="storageclasses.storage.k8s.io"
  I1009 09:44:21.147670  106109 cacher.go:463] cacher (priorityclasses.scheduling.k8s.io): initialized
  I1009 09:44:21.147687  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.147695  106109 reflector.go:368] Caches populated for *scheduling.PriorityClass from storage/cacher.go:/priorityclasses
  I1009 09:44:21.148114  106109 reflector.go:341] Listing and watching *storage.StorageClass from storage/cacher.go:/storageclasses
  I1009 09:44:21.148219  106109 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.148257  106109 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 09:44:21.148502  106109 cacher.go:463] cacher (storageclasses.storage.k8s.io): initialized
  I1009 09:44:21.148522  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.148529  106109 reflector.go:368] Caches populated for *storage.StorageClass from storage/cacher.go:/storageclasses
  I1009 09:44:21.149203  106109 reflector.go:341] Listing and watching *storage.VolumeAttachment from storage/cacher.go:/volumeattachments
  I1009 09:44:21.149239  106109 storage_factory.go:270] storing csinodes.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.149267  106109 etcd.go:419] "Using watch cache" resource="csinodes.storage.k8s.io"
  I1009 09:44:21.149536  106109 cacher.go:463] cacher (volumeattachments.storage.k8s.io): initialized
  I1009 09:44:21.149554  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.149565  106109 reflector.go:368] Caches populated for *storage.VolumeAttachment from storage/cacher.go:/volumeattachments
  I1009 09:44:21.149948  106109 reflector.go:341] Listing and watching *storage.CSINode from storage/cacher.go:/csinodes
  I1009 09:44:21.149980  106109 storage_factory.go:270] storing csidrivers.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.150008  106109 etcd.go:419] "Using watch cache" resource="csidrivers.storage.k8s.io"
  I1009 09:44:21.150289  106109 cacher.go:463] cacher (csinodes.storage.k8s.io): initialized
  I1009 09:44:21.150308  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.150318  106109 reflector.go:368] Caches populated for *storage.CSINode from storage/cacher.go:/csinodes
  I1009 09:44:21.150667  106109 storage_factory.go:270] storing csistoragecapacities.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.150695  106109 reflector.go:341] Listing and watching *storage.CSIDriver from storage/cacher.go:/csidrivers
  I1009 09:44:21.150705  106109 etcd.go:419] "Using watch cache" resource="csistoragecapacities.storage.k8s.io"
  I1009 09:44:21.151082  106109 cacher.go:463] cacher (csidrivers.storage.k8s.io): initialized
  I1009 09:44:21.151095  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.151102  106109 reflector.go:368] Caches populated for *storage.CSIDriver from storage/cacher.go:/csidrivers
  I1009 09:44:21.151528  106109 apis.go:118] Enabling API group "storage.k8s.io".
  I1009 09:44:21.151551  106109 apis.go:105] API group "storagemigration.k8s.io" is not enabled, skipping.
  I1009 09:44:21.151585  106109 reflector.go:341] Listing and watching *storage.CSIStorageCapacity from storage/cacher.go:/csistoragecapacities
  I1009 09:44:21.151720  106109 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.151754  106109 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.151971  106109 cacher.go:463] cacher (csistoragecapacities.storage.k8s.io): initialized
  I1009 09:44:21.151988  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.151997  106109 reflector.go:368] Caches populated for *storage.CSIStorageCapacity from storage/cacher.go:/csistoragecapacities
  I1009 09:44:21.152717  106109 reflector.go:341] Listing and watching *flowcontrol.FlowSchema from storage/cacher.go:/flowschemas
  I1009 09:44:21.152741  106109 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.152770  106109 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.153085  106109 cacher.go:463] cacher (flowschemas.flowcontrol.apiserver.k8s.io): initialized
  I1009 09:44:21.153102  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.153111  106109 reflector.go:368] Caches populated for *flowcontrol.FlowSchema from storage/cacher.go:/flowschemas
  I1009 09:44:21.153393  106109 apis.go:118] Enabling API group "flowcontrol.apiserver.k8s.io".
  I1009 09:44:21.153480  106109 reflector.go:341] Listing and watching *flowcontrol.PriorityLevelConfiguration from storage/cacher.go:/prioritylevelconfigurations
  I1009 09:44:21.153637  106109 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.153670  106109 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 09:44:21.153888  106109 cacher.go:463] cacher (prioritylevelconfigurations.flowcontrol.apiserver.k8s.io): initialized
  I1009 09:44:21.153904  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.153914  106109 reflector.go:368] Caches populated for *flowcontrol.PriorityLevelConfiguration from storage/cacher.go:/prioritylevelconfigurations
  I1009 09:44:21.154419  106109 reflector.go:341] Listing and watching *apps.Deployment from storage/cacher.go:/deployments
  I1009 09:44:21.154506  106109 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.154546  106109 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 09:44:21.154782  106109 cacher.go:463] cacher (deployments.apps): initialized
  I1009 09:44:21.154796  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.154803  106109 reflector.go:368] Caches populated for *apps.Deployment from storage/cacher.go:/deployments
  I1009 09:44:21.155409  106109 reflector.go:341] Listing and watching *apps.StatefulSet from storage/cacher.go:/statefulsets
  I1009 09:44:21.155448  106109 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.155503  106109 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 09:44:21.155778  106109 cacher.go:463] cacher (statefulsets.apps): initialized
  I1009 09:44:21.155794  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.155804  106109 reflector.go:368] Caches populated for *apps.StatefulSet from storage/cacher.go:/statefulsets
  I1009 09:44:21.156271  106109 reflector.go:341] Listing and watching *apps.DaemonSet from storage/cacher.go:/daemonsets
  I1009 09:44:21.156292  106109 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.156329  106109 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 09:44:21.156679  106109 cacher.go:463] cacher (daemonsets.apps): initialized
  I1009 09:44:21.156701  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.156712  106109 reflector.go:368] Caches populated for *apps.DaemonSet from storage/cacher.go:/daemonsets
  I1009 09:44:21.157155  106109 storage_factory.go:270] storing controllerrevisions.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.157182  106109 etcd.go:419] "Using watch cache" resource="controllerrevisions.apps"
  I1009 09:44:21.157200  106109 reflector.go:341] Listing and watching *apps.ReplicaSet from storage/cacher.go:/replicasets
  I1009 09:44:21.157626  106109 cacher.go:463] cacher (replicasets.apps): initialized
  I1009 09:44:21.157647  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.157656  106109 reflector.go:368] Caches populated for *apps.ReplicaSet from storage/cacher.go:/replicasets
  I1009 09:44:21.157943  106109 apis.go:118] Enabling API group "apps".
  I1009 09:44:21.158054  106109 reflector.go:341] Listing and watching *apps.ControllerRevision from storage/cacher.go:/controllerrevisions
  I1009 09:44:21.158077  106109 storage_factory.go:270] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.158140  106109 etcd.go:419] "Using watch cache" resource="validatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 09:44:21.158606  106109 cacher.go:463] cacher (controllerrevisions.apps): initialized
  I1009 09:44:21.158622  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.158629  106109 reflector.go:368] Caches populated for *apps.ControllerRevision from storage/cacher.go:/controllerrevisions
  I1009 09:44:21.158959  106109 reflector.go:341] Listing and watching *admissionregistration.ValidatingWebhookConfiguration from storage/cacher.go:/validatingwebhookconfigurations
  I1009 09:44:21.158974  106109 storage_factory.go:270] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.159010  106109 etcd.go:419] "Using watch cache" resource="mutatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 09:44:21.159309  106109 cacher.go:463] cacher (validatingwebhookconfigurations.admissionregistration.k8s.io): initialized
  I1009 09:44:21.159329  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.159339  106109 reflector.go:368] Caches populated for *admissionregistration.ValidatingWebhookConfiguration from storage/cacher.go:/validatingwebhookconfigurations
  I1009 09:44:21.159781  106109 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.159799  106109 reflector.go:341] Listing and watching *admissionregistration.MutatingWebhookConfiguration from storage/cacher.go:/mutatingwebhookconfigurations
  I1009 09:44:21.159811  106109 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 09:44:21.160149  106109 cacher.go:463] cacher (mutatingwebhookconfigurations.admissionregistration.k8s.io): initialized
  I1009 09:44:21.160164  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.160173  106109 reflector.go:368] Caches populated for *admissionregistration.MutatingWebhookConfiguration from storage/cacher.go:/mutatingwebhookconfigurations
  I1009 09:44:21.160514  106109 reflector.go:341] Listing and watching *admissionregistration.ValidatingAdmissionPolicy from storage/cacher.go:/validatingadmissionpolicies
  I1009 09:44:21.160507  106109 storage_factory.go:270] storing validatingadmissionpolicybindings.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.160570  106109 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicybindings.admissionregistration.k8s.io"
  I1009 09:44:21.160849  106109 cacher.go:463] cacher (validatingadmissionpolicies.admissionregistration.k8s.io): initialized
  I1009 09:44:21.160867  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.160876  106109 reflector.go:368] Caches populated for *admissionregistration.ValidatingAdmissionPolicy from storage/cacher.go:/validatingadmissionpolicies
  I1009 09:44:21.161208  106109 apis.go:118] Enabling API group "admissionregistration.k8s.io".
  I1009 09:44:21.161332  106109 reflector.go:341] Listing and watching *admissionregistration.ValidatingAdmissionPolicyBinding from storage/cacher.go:/validatingadmissionpolicybindings
  I1009 09:44:21.161396  106109 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.161514  106109 etcd.go:416] "Not using watch cache" resource="events"
  I1009 09:44:21.161755  106109 cacher.go:463] cacher (validatingadmissionpolicybindings.admissionregistration.k8s.io): initialized
  I1009 09:44:21.161773  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.161783  106109 reflector.go:368] Caches populated for *admissionregistration.ValidatingAdmissionPolicyBinding from storage/cacher.go:/validatingadmissionpolicybindings
  I1009 09:44:21.162271  106109 apis.go:118] Enabling API group "events.k8s.io".
  I1009 09:44:21.162289  106109 apis.go:105] API group "resource.k8s.io" is not enabled, skipping.
  I1009 09:44:21.171109  106109 storage_factory.go:270] storing selfsubjectreviews.authentication.k8s.io in authentication.k8s.io/v1, reading as authentication.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171182  106109 etcd.go:419] "Using watch cache" resource="selfsubjectreviews.authentication.k8s.io"
  I1009 09:44:21.171249  106109 storage_factory.go:270] storing tokenreviews.authentication.k8s.io in authentication.k8s.io/v1, reading as authentication.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171280  106109 etcd.go:419] "Using watch cache" resource="tokenreviews.authentication.k8s.io"
  I1009 09:44:21.171326  106109 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
  W1009 09:44:21.171333  106109 genericapiserver.go:765] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.171338  106109 genericapiserver.go:765] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.171483  106109 storage_factory.go:270] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171512  106109 etcd.go:419] "Using watch cache" resource="localsubjectaccessreviews.authorization.k8s.io"
  I1009 09:44:21.171568  106109 storage_factory.go:270] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171586  106109 etcd.go:419] "Using watch cache" resource="selfsubjectaccessreviews.authorization.k8s.io"
  I1009 09:44:21.171634  106109 storage_factory.go:270] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171650  106109 etcd.go:419] "Using watch cache" resource="selfsubjectrulesreviews.authorization.k8s.io"
  I1009 09:44:21.171701  106109 storage_factory.go:270] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.171723  106109 etcd.go:419] "Using watch cache" resource="subjectaccessreviews.authorization.k8s.io"
  I1009 09:44:21.171759  106109 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
  W1009 09:44:21.171765  106109 genericapiserver.go:765] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.172158  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.172185  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.172293  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.172311  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.172348  106109 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
  I1009 09:44:21.172759  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.172787  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.172923  106109 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.172945  106109 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 09:44:21.172971  106109 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
  W1009 09:44:21.172977  106109 genericapiserver.go:765] Skipping API autoscaling/v2beta1 because it has no resources.
  W1009 09:44:21.172982  106109 genericapiserver.go:765] Skipping API autoscaling/v2beta2 because it has no resources.
  I1009 09:44:21.173345  106109 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.173369  106109 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 09:44:21.173514  106109 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.173541  106109 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 09:44:21.173795  106109 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.173814  106109 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 09:44:21.173925  106109 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.173952  106109 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 09:44:21.173984  106109 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
  W1009 09:44:21.173990  106109 genericapiserver.go:765] Skipping API batch/v1beta1 because it has no resources.
  I1009 09:44:21.174312  106109 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.174568  106109 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 09:44:21.174737  106109 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.174760  106109 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 09:44:21.174848  106109 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.174865  106109 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 09:44:21.174900  106109 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
  W1009 09:44:21.174907  106109 genericapiserver.go:765] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.174917  106109 genericapiserver.go:765] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.175243  106109 storage_factory.go:270] storing leases.coordination.k8s.io in coordination.k8s.io/v1, reading as coordination.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.175268  106109 etcd.go:419] "Using watch cache" resource="leases.coordination.k8s.io"
  I1009 09:44:21.175293  106109 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
  W1009 09:44:21.175299  106109 genericapiserver.go:765] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.175304  106109 genericapiserver.go:765] Skipping API coordination.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.175715  106109 storage_factory.go:270] storing endpointslices.discovery.k8s.io in discovery.k8s.io/v1, reading as discovery.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.175743  106109 etcd.go:419] "Using watch cache" resource="endpointslices.discovery.k8s.io"
  I1009 09:44:21.175771  106109 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
  W1009 09:44:21.175778  106109 genericapiserver.go:765] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.176042  106109 storage_factory.go:270] storing ingressclasses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.176083  106109 etcd.go:419] "Using watch cache" resource="ingressclasses.networking.k8s.io"
  I1009 09:44:21.176407  106109 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.176491  106109 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 09:44:21.176605  106109 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.176629  106109 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 09:44:21.176939  106109 storage_factory.go:270] storing networkpolicies.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.176959  106109 etcd.go:419] "Using watch cache" resource="networkpolicies.networking.k8s.io"
  I1009 09:44:21.176992  106109 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
  W1009 09:44:21.176999  106109 genericapiserver.go:765] Skipping API networking.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.177004  106109 genericapiserver.go:765] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.177305  106109 storage_factory.go:270] storing runtimeclasses.node.k8s.io in node.k8s.io/v1, reading as node.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.177330  106109 etcd.go:419] "Using watch cache" resource="runtimeclasses.node.k8s.io"
  I1009 09:44:21.177363  106109 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
  W1009 09:44:21.177370  106109 genericapiserver.go:765] Skipping API node.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.177376  106109 genericapiserver.go:765] Skipping API node.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.177873  106109 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.177913  106109 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 09:44:21.178045  106109 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.178082  106109 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 09:44:21.178118  106109 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
  W1009 09:44:21.178125  106109 genericapiserver.go:765] Skipping API policy/v1beta1 because it has no resources.
  I1009 09:44:21.178440  106109 storage_factory.go:270] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.178497  106109 etcd.go:419] "Using watch cache" resource="clusterrolebindings.rbac.authorization.k8s.io"
  I1009 09:44:21.178738  106109 storage_factory.go:270] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.178759  106109 etcd.go:419] "Using watch cache" resource="clusterroles.rbac.authorization.k8s.io"
  I1009 09:44:21.179085  106109 storage_factory.go:270] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.179112  106109 etcd.go:419] "Using watch cache" resource="rolebindings.rbac.authorization.k8s.io"
  I1009 09:44:21.179434  106109 storage_factory.go:270] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.179478  106109 etcd.go:419] "Using watch cache" resource="roles.rbac.authorization.k8s.io"
  I1009 09:44:21.179505  106109 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
  W1009 09:44:21.179511  106109 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.179517  106109 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.179820  106109 storage_factory.go:270] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io/v1, reading as scheduling.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.179841  106109 etcd.go:419] "Using watch cache" resource="priorityclasses.scheduling.k8s.io"
  I1009 09:44:21.179864  106109 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
  W1009 09:44:21.179873  106109 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.179879  106109 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.180161  106109 storage_factory.go:270] storing csidrivers.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.180196  106109 etcd.go:419] "Using watch cache" resource="csidrivers.storage.k8s.io"
  I1009 09:44:21.180430  106109 storage_factory.go:270] storing csinodes.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.180472  106109 etcd.go:419] "Using watch cache" resource="csinodes.storage.k8s.io"
  I1009 09:44:21.180842  106109 storage_factory.go:270] storing csistoragecapacities.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.180866  106109 etcd.go:419] "Using watch cache" resource="csistoragecapacities.storage.k8s.io"
  I1009 09:44:21.181100  106109 storage_factory.go:270] storing storageclasses.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.181119  106109 etcd.go:419] "Using watch cache" resource="storageclasses.storage.k8s.io"
  I1009 09:44:21.181419  106109 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.181496  106109 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 09:44:21.181648  106109 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.181681  106109 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 09:44:21.181719  106109 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
  W1009 09:44:21.181725  106109 genericapiserver.go:765] Skipping API storage.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.181731  106109 genericapiserver.go:765] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.181992  106109 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.182012  106109 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.182116  106109 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.182136  106109 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.182388  106109 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.182407  106109 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.182577  106109 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.182612  106109 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 09:44:21.182655  106109 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
  W1009 09:44:21.182661  106109 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
  W1009 09:44:21.182667  106109 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
  W1009 09:44:21.182672  106109 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.183084  106109 storage_factory.go:270] storing controllerrevisions.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.183107  106109 etcd.go:419] "Using watch cache" resource="controllerrevisions.apps"
  I1009 09:44:21.183445  106109 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.183490  106109 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 09:44:21.183610  106109 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.183633  106109 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 09:44:21.183961  106109 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.183984  106109 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 09:44:21.184124  106109 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.184143  106109 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 09:44:21.184252  106109 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.184274  106109 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 09:44:21.185509  106109 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.185543  106109 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 09:44:21.185657  106109 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.185680  106109 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 09:44:21.185774  106109 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.185791  106109 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 09:44:21.186161  106109 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.186180  106109 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 09:44:21.186291  106109 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.186311  106109 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 09:44:21.186415  106109 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.186440  106109 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 09:44:21.186487  106109 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
  W1009 09:44:21.186504  106109 genericapiserver.go:765] Skipping API apps/v1beta2 because it has no resources.
  W1009 09:44:21.186510  106109 genericapiserver.go:765] Skipping API apps/v1beta1 because it has no resources.
  I1009 09:44:21.186886  106109 storage_factory.go:270] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.186907  106109 etcd.go:419] "Using watch cache" resource="mutatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 09:44:21.187205  106109 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.187224  106109 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 09:44:21.187343  106109 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.187360  106109 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 09:44:21.195809  106109 storage_factory.go:270] storing validatingadmissionpolicybindings.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.195920  106109 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicybindings.admissionregistration.k8s.io"
  I1009 09:44:21.196331  106109 storage_factory.go:270] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.196375  106109 etcd.go:419] "Using watch cache" resource="validatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 09:44:21.196489  106109 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
  W1009 09:44:21.196503  106109 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
  W1009 09:44:21.196509  106109 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
  I1009 09:44:21.196840  106109 storage_factory.go:270] storing events.events.k8s.io in v1, reading as events.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:44081"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc000773320)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc001015980)}
  I1009 09:44:21.196870  106109 etcd.go:416] "Not using watch cache" resource="events.events.k8s.io"
  I1009 09:44:21.196905  106109 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
  W1009 09:44:21.196912  106109 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.197665  106109 deleted_kinds.go:60] NewResourceExpirationEvaluator with currentVersion: 1.32.
  I1009 09:44:21.197701  106109 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 09:44:21.204019  106109 reflector.go:341] Listing and watching *apiregistration.APIService from storage/cacher.go:/apiregistration.k8s.io/apiservices
  I1009 09:44:21.204538  106109 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 09:44:21.204632  106109 cacher.go:463] cacher (apiservices.apiregistration.k8s.io): initialized
  I1009 09:44:21.204654  106109 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 09:44:21.204665  106109 reflector.go:368] Caches populated for *apiregistration.APIService from storage/cacher.go:/apiregistration.k8s.io/apiservices
  I1009 09:44:21.204676  106109 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 09:44:21.204712  106109 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
  W1009 09:44:21.204719  106109 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
  I1009 09:44:21.485756  106109 tlsconfig.go:203] "Loaded serving cert" certName="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key" certDetail="\"82.112.230.236@1728428978\" [serving] validServingFor=[82.112.230.236,10.0.0.1,127.0.0.1,kubernetes.default.svc,kubernetes.default,kubernetes] issuer=\"82.112.230.236-ca@1728428978\" (2024-10-08 22:09:38 +0000 UTC to 2025-10-08 22:09:38 +0000 UTC (now=2024-10-09 09:44:21.485717594 +0000 UTC))"
  I1009 09:44:21.485764  106109 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key"
  I1009 09:44:21.485917  106109 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1728467061\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1728467061\" (2024-10-09 08:44:20 +0000 UTC to 2025-10-09 08:44:20 +0000 UTC (now=2024-10-09 09:44:21.485903996 +0000 UTC))"
  I1009 09:44:21.486000  106109 secure_serving.go:213] Serving securely on 127.0.0.1:6443
  I1009 09:44:21.486053  106109 genericapiserver.go:683] [graceful-termination] waiting for shutdown to be initiated
  I1009 09:44:21.486120  106109 controller.go:119] Starting legacy_token_tracking_controller
  I1009 09:44:21.486135  106109 cluster_authentication_trust_controller.go:454] Starting cluster_authentication_trust_controller controller
  I1009 09:44:21.486145  106109 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
  I1009 09:44:21.486174  106109 tlsconfig.go:243] "Starting DynamicServingCertificateController"
  I1009 09:44:21.486186  106109 gc_controller.go:78] Starting apiserver lease garbage collector
  I1009 09:44:21.486201  106109 apiservice_controller.go:100] Starting APIServiceRegistrationController
  I1009 09:44:21.486209  106109 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
  I1009 09:44:21.486214  106109 system_namespaces_controller.go:66] Starting system namespaces controller
  I1009 09:44:21.486224  106109 apf_controller.go:377] Starting API Priority and Fairness config controller
  I1009 09:44:21.486232  106109 controller.go:78] Starting OpenAPI AggregationController
  I1009 09:44:21.486139  106109 shared_informer.go:313] Waiting for caches to sync for configmaps
  I1009 09:44:21.486268  106109 local_available_controller.go:156] Starting LocalAvailability controller
  I1009 09:44:21.486279  106109 cache.go:32] Waiting for caches to sync for LocalAvailability controller
  I1009 09:44:21.486289  106109 reflector.go:305] Starting reflector *v1.Lease (0s) from runtime/asm_amd64.s:1700
  I1009 09:44:21.486297  106109 remote_available_controller.go:411] Starting RemoteAvailability controller
  I1009 09:44:21.486186  106109 reflector.go:305] Starting reflector *v1.ConfigMap (12h0m0s) from runtime/asm_amd64.s:1700
  I1009 09:44:21.486310  106109 reflector.go:341] Listing and watching *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 09:44:21.486321  106109 aggregator.go:169] waiting for initial CRD sync...
  I1009 09:44:21.486338  106109 crdregistration_controller.go:114] Starting crd-autoregister controller
  I1009 09:44:21.486344  106109 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
  I1009 09:44:21.486364  106109 reflector.go:305] Starting reflector *v1.Service (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486377  106109 reflector.go:341] Listing and watching *v1.Service from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486413  106109 customresource_discovery_controller.go:292] Starting DiscoveryController
  I1009 09:44:21.486482  106109 controller.go:142] Starting OpenAPI controller
  I1009 09:44:21.486511  106109 controller.go:90] Starting OpenAPI V3 controller
  I1009 09:44:21.486522  106109 naming_controller.go:294] Starting NamingConditionController
  I1009 09:44:21.486532  106109 establishing_controller.go:81] Starting EstablishingController
  I1009 09:44:21.486543  106109 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
  I1009 09:44:21.486553  106109 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
  I1009 09:44:21.486582  106109 crd_finalizer.go:269] Starting CRDFinalizer
  I1009 09:44:21.486297  106109 reflector.go:341] Listing and watching *v1.Lease from runtime/asm_amd64.s:1700
  I1009 09:44:21.486645  106109 reflector.go:305] Starting reflector *v1.ResourceQuota (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486655  106109 reflector.go:341] Listing and watching *v1.ResourceQuota from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486686  106109 reflector.go:305] Starting reflector *v1.Node (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486699  106109 reflector.go:341] Listing and watching *v1.Node from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486268  106109 reflector.go:305] Starting reflector *v1.ConfigMap (12h0m0s) from runtime/asm_amd64.s:1700
  I1009 09:44:21.486765  106109 reflector.go:341] Listing and watching *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 09:44:21.486844  106109 reflector.go:305] Starting reflector *v1.APIService (30s) from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.486858  106109 reflector.go:341] Listing and watching *v1.APIService from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.486904  106109 reflector.go:305] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486915  106109 reflector.go:341] Listing and watching *v1.Secret from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487033  106109 reflector.go:305] Starting reflector *v1.FlowSchema (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487044  106109 reflector.go:341] Listing and watching *v1.FlowSchema from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487067  106109 reflector.go:305] Starting reflector *v1.PriorityClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487080  106109 reflector.go:341] Listing and watching *v1.PriorityClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487182  106109 reflector.go:305] Starting reflector *v1.StorageClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487191  106109 reflector.go:341] Listing and watching *v1.StorageClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487260  106109 reflector.go:305] Starting reflector *v1.RuntimeClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487273  106109 reflector.go:341] Listing and watching *v1.RuntimeClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487304  106109 reflector.go:305] Starting reflector *v1.ValidatingAdmissionPolicyBinding (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487312  106109 reflector.go:341] Listing and watching *v1.ValidatingAdmissionPolicyBinding from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487440  106109 reflector.go:305] Starting reflector *v1.ValidatingWebhookConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487467  106109 reflector.go:341] Listing and watching *v1.ValidatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487505  106109 reflector.go:305] Starting reflector *v1.IngressClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487519  106109 reflector.go:341] Listing and watching *v1.IngressClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487582  106109 reflector.go:305] Starting reflector *v1.Endpoints (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487590  106109 reflector.go:341] Listing and watching *v1.Endpoints from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487728  106109 reflector.go:305] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487739  106109 reflector.go:341] Listing and watching *v1.ServiceAccount from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487758  106109 reflector.go:305] Starting reflector *v1.PriorityLevelConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487771  106109 reflector.go:341] Listing and watching *v1.PriorityLevelConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487898  106109 reflector.go:305] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487908  106109 reflector.go:341] Listing and watching *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487939  106109 reflector.go:305] Starting reflector *v1.Pod (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.487951  106109 reflector.go:341] Listing and watching *v1.Pod from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.488026  106109 reflector.go:305] Starting reflector *v1.LimitRange (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.488034  106109 reflector.go:341] Listing and watching *v1.LimitRange from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486374  106109 reflector.go:305] Starting reflector *v1.CustomResourceDefinition (5m0s) from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.488090  106109 reflector.go:341] Listing and watching *v1.CustomResourceDefinition from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.488125  106109 reflector.go:305] Starting reflector *v1.MutatingWebhookConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.488132  106109 reflector.go:341] Listing and watching *v1.MutatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.486306  106109 controller.go:80] Starting OpenAPI V3 AggregationController
  I1009 09:44:21.486305  106109 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
  I1009 09:44:21.487732  106109 reflector.go:305] Starting reflector *v1.ValidatingAdmissionPolicy (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.488680  106109 reflector.go:341] Listing and watching *v1.ValidatingAdmissionPolicy from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.494770  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services?limit=500&resourceVersion=0" latency="437.712µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8cfbfee5-2ed9-4208-a444-7eee27e58597" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="312.952µs" resp=200
  I1009 09:44:21.495178  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&limit=500&resourceVersion=0" latency="281.421µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="76327c77-41b9-4a10-9570-59be0cf93e24" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="194.92µs" resp=200
  I1009 09:44:21.495754  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=apiserver.kubernetes.io%2Fidentity%3Dkube-apiserver&limit=500&resourceVersion=0" latency="241.112µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4cee2174-3586-4af2-a4a7-74ad402c99fa" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="136.091µs" resp=200
  I1009 09:44:21.496114  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/resourcequotas?limit=500&resourceVersion=0" latency="263.702µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b4b24ec2-6ffa-4c9c-9845-4513c874139e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="172.491µs" resp=200
  I1009 09:44:21.496438  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes?limit=500&resourceVersion=0" latency="221.652µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="64a7242a-2252-484b-9b74-5af01b368014" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="157.102µs" resp=200
  I1009 09:44:21.496717  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-system/configmaps?limit=500&resourceVersion=0" latency="173.223µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bea330fa-264a-4c3f-8cd1-980721992df2" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="106.231µs" resp=200
  I1009 09:44:21.496946  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/apiregistration.k8s.io/v1/apiservices?limit=500&resourceVersion=0" latency="162.841µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3cbf4d4e-633a-4b2f-b9da-87d129960154" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="106.701µs" resp=200
  I1009 09:44:21.497240  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/secrets?limit=500&resourceVersion=0" latency="203.062µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="875e8793-366a-43ef-a0f1-6638fa20f732" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="145.952µs" resp=200
  I1009 09:44:21.497635  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?limit=500&resourceVersion=0" latency="317.063µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1ec60b5b-1538-48a3-8661-b5a9edd737f4" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="254.401µs" resp=200
  I1009 09:44:21.497897  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/scheduling.k8s.io/v1/priorityclasses?limit=500&resourceVersion=0" latency="188.532µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="84890ed4-4310-4081-bead-7ea6971bbba3" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="126.5µs" resp=200
  I1009 09:44:21.498207  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0" latency="241.852µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7262f5da-9698-464b-8050-ac9ed7dd01f9" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="162.571µs" resp=200
  I1009 09:44:21.498516  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0" latency="236.122µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2e414e28-3a44-4f8e-ad06-1ea570942f67" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="170.741µs" resp=200
  I1009 09:44:21.498776  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicybindings?limit=500&resourceVersion=0" latency="190.801µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="525debf5-9b86-40ed-889e-64aae86274d1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="109.241µs" resp=200
  I1009 09:44:21.498996  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?limit=500&resourceVersion=0" latency="162.331µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="746506eb-5cd5-471a-bf8d-e24c01b627f5" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="107.831µs" resp=200
  I1009 09:44:21.499266  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/networking.k8s.io/v1/ingressclasses?limit=500&resourceVersion=0" latency="184.462µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c0973c38-579a-4c44-b4df-5054f13e976e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="118.201µs" resp=200
  I1009 09:44:21.499605  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/endpoints?limit=500&resourceVersion=0" latency="264.502µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b02d3ff0-5b14-4d77-87de-26319ed7dc10" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="185.591µs" resp=200
  I1009 09:44:21.499829  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/serviceaccounts?limit=500&resourceVersion=0" latency="158.801µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5a7ce0ea-d427-46d4-a5d3-b920f381ad5f" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="105.372µs" resp=200
  I1009 09:44:21.500031  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?limit=500&resourceVersion=0" latency="148.05µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="852ca13a-fab1-4d1b-9841-9507e304af4b" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="99.379µs" resp=200
  I1009 09:44:21.500340  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces?limit=500&resourceVersion=0" latency="225.021µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8307e4d4-8559-4b93-8f61-334ffce71c89" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="143.461µs" resp=200
  I1009 09:44:21.500646  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/pods?limit=500&resourceVersion=0" latency="242.302µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="06d245fd-37a5-4c6c-a46b-7e9672f908aa" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="140.031µs" resp=200
  I1009 09:44:21.500944  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/limitranges?limit=500&resourceVersion=0" latency="236.392µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0f7aecd0-473c-46fc-af57-d0853f1a7f1f" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="158.861µs" resp=200
  I1009 09:44:21.501175  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/apiextensions.k8s.io/v1/customresourcedefinitions?limit=500&resourceVersion=0" latency="173.32µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="708189f8-2317-4e27-b744-51384f150c80" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="118.511µs" resp=200
  I1009 09:44:21.501474  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?limit=500&resourceVersion=0" latency="223.412µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f5cd2ca1-58c3-40c1-8291-8768e2d3efc3" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="154.791µs" resp=200
  I1009 09:44:21.501771  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicies?limit=500&resourceVersion=0" latency="205.1µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="78e13616-8ed6-44eb-9c67-479cf0503679" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="123.111µs" resp=200
  I1009 09:44:21.505568  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services" latency="11.935ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="93c67f68-4804-40f2-bb56-d0339e1b3176" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="11.63663ms" resp=200
  I1009 09:44:21.506849  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="11.545889ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="59a79205-176e-4d60-a0de-b81754ad4f5e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="11.461678ms" resp=404
  I1009 09:44:21.507124  106109 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.507335  106109 reflector.go:368] Caches populated for *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 09:44:21.507502  106109 reflector.go:368] Caches populated for *v1.Lease from runtime/asm_amd64.s:1700
  I1009 09:44:21.507517  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services" latency="5.657718ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0ea582ae-fb27-4f27-b51a-a21286b80ade" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.576649ms" resp=200
  I1009 09:44:21.507649  106109 reflector.go:368] Caches populated for *v1.ResourceQuota from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.507790  106109 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.507913  106109 reflector.go:368] Caches populated for *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 09:44:21.508063  106109 reflector.go:368] Caches populated for *v1.APIService from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.508071  106109 get.go:278] "Starting watch" path="/api/v1/resourcequotas" resourceVersion="1" labels="" fields="" timeout="8m52s"
  I1009 09:44:21.508173  106109 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508269  106109 reflector.go:368] Caches populated for *v1.FlowSchema from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508375  106109 reflector.go:368] Caches populated for *v1.PriorityClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508472  106109 get.go:278] "Starting watch" path="/api/v1/services" resourceVersion="1" labels="" fields="" timeout="9m37s"
  I1009 09:44:21.508517  106109 get.go:278] "Starting watch" path="/api/v1/nodes" resourceVersion="1" labels="" fields="" timeout="7m55s"
  I1009 09:44:21.508554  106109 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508676  106109 reflector.go:368] Caches populated for *v1.RuntimeClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508786  106109 reflector.go:368] Caches populated for *v1.ValidatingAdmissionPolicyBinding from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.508897  106109 get.go:278] "Starting watch" path="/apis/scheduling.k8s.io/v1/priorityclasses" resourceVersion="1" labels="" fields="" timeout="9m14s"
  I1009 09:44:21.508927  106109 get.go:278] "Starting watch" path="/api/v1/namespaces/kube-system/configmaps" resourceVersion="1" labels="" fields="metadata.name=kube-apiserver-legacy-service-account-token-tracking" timeout="5m51s"
  I1009 09:44:21.508967  106109 reflector.go:368] Caches populated for *v1.ValidatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509127  106109 reflector.go:368] Caches populated for *v1.IngressClass from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509285  106109 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509307  106109 get.go:278] "Starting watch" path="/api/v1/secrets" resourceVersion="1" labels="" fields="" timeout="5m7s"
  I1009 09:44:21.509370  106109 get.go:278] "Starting watch" path="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" resourceVersion="1" labels="apiserver.kubernetes.io/identity=kube-apiserver" fields="" timeout="6m4s"
  I1009 09:44:21.509523  106109 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicybindings" resourceVersion="1" labels="" fields="" timeout="6m7s"
  I1009 09:44:21.509593  106109 get.go:278] "Starting watch" path="/api/v1/namespaces/kube-system/configmaps" resourceVersion="1" labels="" fields="" timeout="5m39s"
  I1009 09:44:21.509668  106109 get.go:278] "Starting watch" path="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas" resourceVersion="1" labels="" fields="" timeout="9m9s"
  I1009 09:44:21.509381  106109 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509767  106109 get.go:278] "Starting watch" path="/apis/storage.k8s.io/v1/storageclasses" resourceVersion="1" labels="" fields="" timeout="8m37s"
  I1009 09:44:21.509794  106109 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations" resourceVersion="1" labels="" fields="" timeout="6m29s"
  I1009 09:44:21.509812  106109 get.go:278] "Starting watch" path="/apis/apiregistration.k8s.io/v1/apiservices" resourceVersion="1" labels="" fields="" timeout="5m21s"
  I1009 09:44:21.509403  106109 reflector.go:368] Caches populated for *v1.PriorityLevelConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509383  106109 get.go:278] "Starting watch" path="/apis/node.k8s.io/v1/runtimeclasses" resourceVersion="1" labels="" fields="" timeout="5m56s"
  I1009 09:44:21.509440  106109 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509966  106109 get.go:278] "Starting watch" path="/apis/networking.k8s.io/v1/ingressclasses" resourceVersion="1" labels="" fields="" timeout="5m0s"
  I1009 09:44:21.509502  106109 reflector.go:368] Caches populated for *v1.Pod from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.510077  106109 get.go:278] "Starting watch" path="/api/v1/endpoints" resourceVersion="1" labels="" fields="" timeout="9m54s"
  I1009 09:44:21.509520  106109 reflector.go:368] Caches populated for *v1.LimitRange from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.509551  106109 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from pkg/client/informers/externalversions/factory.go:141
  I1009 09:44:21.510199  106109 get.go:278] "Starting watch" path="/api/v1/serviceaccounts" resourceVersion="1" labels="" fields="" timeout="6m48s"
  I1009 09:44:21.509573  106109 reflector.go:368] Caches populated for *v1.MutatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.510316  106109 get.go:278] "Starting watch" path="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations" resourceVersion="1" labels="" fields="" timeout="8m27s"
  I1009 09:44:21.509593  106109 reflector.go:368] Caches populated for *v1.ValidatingAdmissionPolicy from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:21.510354  106109 get.go:278] "Starting watch" path="/api/v1/namespaces" resourceVersion="1" labels="" fields="" timeout="8m8s"
  I1009 09:44:21.510503  106109 get.go:278] "Starting watch" path="/api/v1/pods" resourceVersion="1" labels="" fields="" timeout="9m30s"
  I1009 09:44:21.510628  106109 get.go:278] "Starting watch" path="/api/v1/limitranges" resourceVersion="1" labels="" fields="" timeout="7m18s"
  I1009 09:44:21.510678  106109 get.go:278] "Starting watch" path="/apis/apiextensions.k8s.io/v1/customresourcedefinitions" resourceVersion="1" labels="" fields="" timeout="7m41s"
  I1009 09:44:21.510733  106109 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations" resourceVersion="1" labels="" fields="" timeout="8m55s"
  I1009 09:44:21.510824  106109 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicies" resourceVersion="1" labels="" fields="" timeout="8m23s"
  I1009 09:44:21.567857  106109 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
  I1009 09:44:21.567932  106109 policy_source.go:240] refreshing policies
  I1009 09:44:21.587227  106109 shared_informer.go:320] Caches are synced for configmaps
  I1009 09:44:21.587351  106109 cache.go:39] Caches are synced for APIServiceRegistrationController controller
  I1009 09:44:21.587401  106109 apf_controller.go:382] Running API Priority and Fairness config worker
  I1009 09:44:21.587417  106109 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
  I1009 09:44:21.587487  106109 genericapiserver.go:533] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
  I1009 09:44:21.587503  106109 handler_discovery.go:451] Starting ResourceDiscoveryManager
  I1009 09:44:21.587549  106109 shared_informer.go:320] Caches are synced for crd-autoregister
  I1009 09:44:21.587548  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=600 concurrencyDenominator=600 backstop=false
  I1009 09:44:21.587587  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=6 seatDemandAvg=0.221335496809972 seatDemandStdev=0.5601132214500006 seatDemandSmoothed=0.7814487182599726 fairFrac=0 currentCL=6 concurrencyDenominator=6 backstop=false
  I1009 09:44:21.587412  106109 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
  I1009 09:44:21.587356  106109 cache.go:39] Caches are synced for LocalAvailability controller
  I1009 09:44:21.587740  106109 aggregator.go:171] initial CRD sync complete...
  I1009 09:44:21.587753  106109 autoregister_controller.go:144] Starting autoregister controller
  I1009 09:44:21.587761  106109 cache.go:32] Waiting for caches to sync for autoregister controller
  I1009 09:44:21.587768  106109 cache.go:39] Caches are synced for autoregister controller
  I1009 09:44:21.588743  106109 controller.go:615] quota admission added evaluator for: namespaces
  I1009 09:44:21.588806  106109 cache.go:39] Caches are synced for RemoteAvailability controller
  I1009 09:44:21.589921  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.566761ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ed3c30d8-2ff6-4bdb-8052-19498865dc85" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.48239ms" resp=201
  I1009 09:44:21.589946  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.545761ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="800efcea-ec59-4907-8a5b-29ffc2ffa8d5" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.42608ms" resp=201
  I1009 09:44:21.590061  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.868423ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a4f29069-800a-4d43-b251-bf8f49e2883d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.800363ms" resp=201
  I1009 09:44:21.589917  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.959593ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d5307918-08e3-494d-ab9b-596fbb645ba1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.876092ms" resp=201
  I1009 09:44:21.591184  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes,poststarthook/priority-and-fairness-config-producer,autoregister-completion check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  [-]poststarthook/priority-and-fairness-config-producer failed: not finished
  [-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v2.autoscaling]
  I1009 09:44:21.591229  106109 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 1 objects queued in incoming channel.
  I1009 09:44:21.591244  106109 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 2 objects queued in incoming channel.
  I1009 09:44:21.591253  106109 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 3 objects queued in incoming channel.
  I1009 09:44:21.591259  106109 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 4 objects queued in incoming channel.
  I1009 09:44:21.591490  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="2.272746ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6f90e32a-2f86-4b51-b0ad-7337cc9e0399" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.147025ms" resp=500
  I1009 09:44:21.592392  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="system"
  I1009 09:44:21.592911  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","system"]
  I1009 09:44:21.592946  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","system"]
  I1009 09:44:21.592958  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["system"]
  I1009 09:44:21.593003  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["system"]
  I1009 09:44:21.593017  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.593025  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.593050  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=86, lendableCL=0, borrowingCL=600, currentCL=600, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=35)
  I1009 09:44:21.593090  106109 apf_controller.go:898] Introducing queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=515, lendableCL=170, borrowingCL=600, currentCL=430, quiescing=false (shares=0xc003e48418, shareSum=35)
  I1009 09:44:21.593143  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.394431554524362 currentCL=481 concurrencyDenominator=481 backstop=false
  I1009 09:44:21.593186  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=11 seatDemandAvg=6.438061004952265 seatDemandStdev=2.2770812736409995 seatDemandSmoothed=8.715142278593264 fairFrac=1.394431554524362 currentCL=11 concurrencyDenominator=11 backstop=false
  I1009 09:44:21.593229  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.394431554524362 currentCL=120 concurrencyDenominator=120 backstop=false
  I1009 09:44:21.593324  106109 remote_available_controller.go:456] Adding v1.admissionregistration.k8s.io
  I1009 09:44:21.593385  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="4.391539ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5aa7c6ae-c3a0-4e51-b866-346814da3860" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.308809ms" resp=201
  I1009 09:44:21.593433  106109 local_available_controller.go:201] Adding v1.admissionregistration.k8s.io
  I1009 09:44:21.593646  106109 apiservice_controller.go:173] Adding v1.admissionregistration.k8s.io
  I1009 09:44:21.593672  106109 apiservice_controller.go:173] Adding v1.
  I1009 09:44:21.593688  106109 local_available_controller.go:201] Adding v1.
  I1009 09:44:21.593748  106109 remote_available_controller.go:456] Adding v1.
  I1009 09:44:21.593746  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="4.846763ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b7a91023-13d2-4524-b11f-14935dfea56d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.782294ms" resp=201
  I1009 09:44:21.593795  106109 remote_available_controller.go:456] Adding v1.apiextensions.k8s.io
  I1009 09:44:21.594392  106109 remote_available_controller.go:456] Adding v1.authentication.k8s.io
  I1009 09:44:21.595055  106109 apiservice_controller.go:173] Adding v1.apiextensions.k8s.io
  I1009 09:44:21.595171  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-system/resourcequotas" latency="5.452367ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a6c21596-c963-49d9-b41a-8ac5c60b32bc" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.359716ms" resp=200
  I1009 09:44:21.595737  106109 local_available_controller.go:201] Adding v1.apiextensions.k8s.io
  I1009 09:44:21.595898  106109 local_available_controller.go:201] Adding v1.authentication.k8s.io
  I1009 09:44:21.595965  106109 apiservice_controller.go:173] Adding v1.authentication.k8s.io
  I1009 09:44:21.595973  106109 apiservice_controller.go:173] Adding v1.apps
  I1009 09:44:21.595982  106109 local_available_controller.go:201] Adding v1.apps
  I1009 09:44:21.596017  106109 remote_available_controller.go:456] Adding v1.apps
  I1009 09:44:21.596317  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.342785ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1ce4b8fa-0ccd-43f6-8dd3-89b2a3b2d4a6" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.152724ms" resp=201
  I1009 09:44:21.597116  106109 local_available_controller.go:201] Adding v1.authorization.k8s.io
  I1009 09:44:21.597275  106109 remote_available_controller.go:456] Adding v1.authorization.k8s.io
  I1009 09:44:21.597300  106109 apiservice_controller.go:173] Adding v1.authorization.k8s.io
  I1009 09:44:21.597692  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.53583ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d16702d6-8192-4793-9a92-0a49b0e0913c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.420699ms" resp=201
  I1009 09:44:21.597909  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.987924ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0eb2d80f-a310-453c-971e-2b8797580714" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.825352ms" resp=201
  I1009 09:44:21.597951  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.328285ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="eba7f937-0e41-4ed5-8fb5-c21450147b02" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.174415ms" resp=201
  I1009 09:44:21.598528  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="10.713133ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0d8dc54c-5a2e-4901-a124-1a0e4892eccd" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="10.550852ms" resp=201
  I1009 09:44:21.598632  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="3.602393ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="22f5c3a6-0f48-4a44-b58a-f6537ebfa04a" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.495864ms" resp=201
  I1009 09:44:21.598976  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="3.779275ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="51a17660-9e56-4feb-b343-7e40c25f455c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.639704ms" resp=201
  I1009 09:44:21.599178  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.317878ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="263df1b2-5ccf-45ac-a754-0f5cb5336de1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.193599ms" resp=201
  I1009 09:44:21.599501  106109 apiservice_controller.go:173] Adding v1.certificates.k8s.io
  I1009 09:44:21.599518  106109 apiservice_controller.go:173] Adding v1.batch
  I1009 09:44:21.599528  106109 local_available_controller.go:201] Adding v1.certificates.k8s.io
  I1009 09:44:21.599563  106109 remote_available_controller.go:456] Adding v1.certificates.k8s.io
  I1009 09:44:21.599585  106109 local_available_controller.go:201] Adding v1.batch
  I1009 09:44:21.599591  106109 local_available_controller.go:201] Adding v2.autoscaling
  I1009 09:44:21.599617  106109 remote_available_controller.go:456] Adding v1.batch
  I1009 09:44:21.599640  106109 apiservice_controller.go:173] Adding v2.autoscaling
  I1009 09:44:21.599650  106109 remote_available_controller.go:456] Adding v2.autoscaling
  I1009 09:44:21.599824  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","node-high"]
  I1009 09:44:21.599854  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","node-high"]
  I1009 09:44:21.599872  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["node-high"]
  I1009 09:44:21.599887  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["node-high"]
  I1009 09:44:21.599902  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.599910  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.599923  106109 apf_controller.go:898] Introducing queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=320, lendableCL=80, borrowingCL=600, currentCL=280, quiescing=false (shares=0xc003f1a000, shareSum=75)
  I1009 09:44:21.599957  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=40, lendableCL=0, borrowingCL=600, currentCL=120, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=75)
  I1009 09:44:21.599980  106109 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=240, lendableCL=79, borrowingCL=600, currentCL=481, quiescing=false, numPending=0 (shares=0xc003e48418, shareSum=75)
  I1009 09:44:21.600009  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=219 concurrencyDenominator=219 backstop=false
  I1009 09:44:21.600036  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="node-high"
  I1009 09:44:21.600061  106109 remote_available_controller.go:456] Adding v1.autoscaling
  I1009 09:44:21.600091  106109 local_available_controller.go:201] Adding v1.autoscaling
  I1009 09:44:21.600099  106109 local_available_controller.go:201] Adding v1.coordination.k8s.io
  I1009 09:44:21.600141  106109 apiservice_controller.go:173] Adding v1.autoscaling
  I1009 09:44:21.600148  106109 apiservice_controller.go:173] Adding v1.coordination.k8s.io
  I1009 09:44:21.600161  106109 remote_available_controller.go:456] Adding v1.coordination.k8s.io
  I1009 09:44:21.600042  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=327 concurrencyDenominator=327 backstop=false
  I1009 09:44:21.600336  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=9 seatDemandAvg=5.5734751874014625 seatDemandStdev=1.8485532255257822 seatDemandSmoothed=8.685400659682946 fairFrac=1.3605442176870748 currentCL=9 concurrencyDenominator=9 backstop=false
  I1009 09:44:21.600374  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=54 concurrencyDenominator=54 backstop=false
  I1009 09:44:21.600811  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.236948ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="78489546-e738-4c5a-be8a-6f2aa72d8ac1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.128847ms" resp=201
  I1009 09:44:21.601322  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.231398ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d64249de-2e00-4a86-af84-86ab29cb1037" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="848.955µs" resp=201
  I1009 09:44:21.601793  106109 remote_available_controller.go:456] Adding v1.discovery.k8s.io
  I1009 09:44:21.601802  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.45409ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b2327ae7-7ffb-4e0a-b9c2-70c10a17d48e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.27316ms" resp=201
  I1009 09:44:21.601834  106109 apiservice_controller.go:173] Adding v1.discovery.k8s.io
  I1009 09:44:21.601845  106109 apiservice_controller.go:173] Adding v1.networking.k8s.io
  I1009 09:44:21.601854  106109 local_available_controller.go:201] Adding v1.discovery.k8s.io
  I1009 09:44:21.601899  106109 remote_available_controller.go:456] Adding v1.networking.k8s.io
  I1009 09:44:21.601909  106109 local_available_controller.go:201] Adding v1.networking.k8s.io
  I1009 09:44:21.602440  106109 apiservice_controller.go:173] Adding v1.node.k8s.io
  I1009 09:44:21.602466  106109 local_available_controller.go:201] Adding v1.node.k8s.io
  I1009 09:44:21.602498  106109 remote_available_controller.go:456] Adding v1.node.k8s.io
  I1009 09:44:21.602465  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","leader-election"]
  I1009 09:44:21.602630  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="2.409116ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e7cd524e-9035-4101-bf42-e46e4ee97d36" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.788282ms" resp=201
  I1009 09:44:21.602676  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","leader-election"]
  I1009 09:44:21.602703  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["leader-election"]
  I1009 09:44:21.602715  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["leader-election"]
  I1009 09:44:21.602731  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.602745  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.602763  106109 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=212, lendableCL=70, borrowingCL=600, currentCL=219, quiescing=false, numPending=0 (shares=0xc003e48418, shareSum=85)
  I1009 09:44:21.602791  106109 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=283, lendableCL=71, borrowingCL=600, currentCL=327, quiescing=false, numPending=0 (shares=0xc003f1a000, shareSum=85)
  I1009 09:44:21.602812  106109 apf_controller.go:898] Introducing queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=71, lendableCL=0, borrowingCL=600, currentCL=71, quiescing=false (shares=0xc003ee1118, shareSum=85)
  I1009 09:44:21.602828  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=36, lendableCL=0, borrowingCL=600, currentCL=54, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=85)
  I1009 09:44:21.602857  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=277 concurrencyDenominator=277 backstop=false
  I1009 09:44:21.602895  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=93 concurrencyDenominator=93 backstop=false
  I1009 09:44:21.602919  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=47 concurrencyDenominator=47 backstop=false
  I1009 09:44:21.602933  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=8 seatDemandAvg=6.067560130771059 seatDemandStdev=1.5834564405344076 seatDemandSmoothed=8.661609825650263 fairFrac=1.3058568329718003 currentCL=8 concurrencyDenominator=8 backstop=false
  I1009 09:44:21.602950  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=185 concurrencyDenominator=185 backstop=false
  I1009 09:44:21.603039  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="3.203812ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d5b56671-1709-4b2b-863a-7072bf910b65" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.323666ms" resp=201
  I1009 09:44:21.603069  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="leader-election"
  I1009 09:44:21.603324  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="3.395143ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ac0dc3c0-790b-48cb-99d9-298baefa56dd" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.538468ms" resp=201
  I1009 09:44:21.604962  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.53419ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="63726f14-28f8-472c-941f-f5a0f8046c7b" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.423629ms" resp=201
  I1009 09:44:21.605161  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.091907ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b1ba223a-f442-4e7e-b0ea-e7adca00ec63" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="998.468µs" resp=201
  I1009 09:44:21.605222  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.794739ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="abb5ec93-2738-4b4c-bad0-02426c16e9f7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.837062ms" resp=201
  I1009 09:44:21.605565  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.354856ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="27186974-ddf2-4849-aaeb-4e4a239c6fde" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.250916ms" resp=201
  I1009 09:44:21.605588  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","workload-high"]
  I1009 09:44:21.605612  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","workload-high"]
  I1009 09:44:21.605630  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["workload-high"]
  I1009 09:44:21.605645  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["workload-high"]
  I1009 09:44:21.605656  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.605664  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.605681  106109 apf_controller.go:898] Introducing queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=192, lendableCL=96, borrowingCL=600, currentCL=144, quiescing=false (shares=0xc003f1b888, shareSum=125)
  I1009 09:44:21.605699  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=24, lendableCL=0, borrowingCL=600, currentCL=47, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=125)
  I1009 09:44:21.605716  106109 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=144, lendableCL=48, borrowingCL=600, currentCL=185, quiescing=false, numPending=0 (shares=0xc003e48418, shareSum=125)
  I1009 09:44:21.605725  106109 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=192, lendableCL=48, borrowingCL=600, currentCL=277, quiescing=false, numPending=0 (shares=0xc003f1a000, shareSum=125)
  I1009 09:44:21.605734  106109 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=48, lendableCL=0, borrowingCL=600, currentCL=93, quiescing=false, numPending=0 (shares=0xc003ee1118, shareSum=125)
  I1009 09:44:21.605789  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=35 concurrencyDenominator=35 backstop=false
  I1009 09:44:21.605815  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=9 seatDemandAvg=6.750751738604769 seatDemandStdev=1.3052559374389838 seatDemandSmoothed=8.647680976209314 fairFrac=1.4705882352941178 currentCL=9 concurrencyDenominator=9 backstop=false
  I1009 09:44:21.605832  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=141 concurrencyDenominator=141 backstop=false
  I1009 09:44:21.605863  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=212 concurrencyDenominator=212 backstop=false
  I1009 09:44:21.605882  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=71 concurrencyDenominator=71 backstop=false
  I1009 09:44:21.605901  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=141 concurrencyDenominator=141 backstop=false
  I1009 09:44:21.606000  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="workload-high"
  I1009 09:44:21.606010  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.361689ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="611fdaeb-2f47-414d-a87e-eb1b4cfedbd1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.250948ms" resp=201
  I1009 09:44:21.606153  106109 local_available_controller.go:201] Adding v1.events.k8s.io
  I1009 09:44:21.606167  106109 local_available_controller.go:201] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 09:44:21.606228  106109 remote_available_controller.go:456] Adding v1.events.k8s.io
  I1009 09:44:21.606246  106109 apiservice_controller.go:173] Adding v1.events.k8s.io
  I1009 09:44:21.606253  106109 apiservice_controller.go:173] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 09:44:21.606267  106109 remote_available_controller.go:456] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 09:44:21.606389  106109 local_available_controller.go:201] Adding v1.rbac.authorization.k8s.io
  I1009 09:44:21.606433  106109 remote_available_controller.go:456] Adding v1.rbac.authorization.k8s.io
  I1009 09:44:21.606467  106109 apiservice_controller.go:173] Adding v1.rbac.authorization.k8s.io
  I1009 09:44:21.606481  106109 apiservice_controller.go:173] Adding v1.scheduling.k8s.io
  I1009 09:44:21.606491  106109 local_available_controller.go:201] Adding v1.scheduling.k8s.io
  I1009 09:44:21.606507  106109 local_available_controller.go:201] Adding v1.policy
  I1009 09:44:21.606514  106109 local_available_controller.go:201] Adding v1.storage.k8s.io
  I1009 09:44:21.606494  106109 remote_available_controller.go:456] Adding v1.scheduling.k8s.io
  I1009 09:44:21.606547  106109 remote_available_controller.go:456] Adding v1.policy
  I1009 09:44:21.606496  106109 apiservice_controller.go:173] Adding v1.policy
  I1009 09:44:21.606557  106109 remote_available_controller.go:456] Adding v1.storage.k8s.io
  I1009 09:44:21.606558  106109 apiservice_controller.go:173] Adding v1.storage.k8s.io
  I1009 09:44:21.607321  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.089507ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c87c3e54-56b0-4ac0-8563-641ebf4255f2" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="959.287µs" resp=201
  I1009 09:44:21.607408  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","workload-low"]
  I1009 09:44:21.607538  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","workload-low"]
  I1009 09:44:21.607555  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["workload-low"]
  I1009 09:44:21.607577  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["workload-low"]
  I1009 09:44:21.607605  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.607615  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.607635  106109 apf_controller.go:906] Retaining queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=107, lendableCL=54, borrowingCL=600, currentCL=141, quiescing=false, numPending=0 (shares=0xc003f1b888, shareSum=225)
  I1009 09:44:21.607669  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="workload-low"
  I1009 09:44:21.607666  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=14, lendableCL=0, borrowingCL=600, currentCL=35, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=225)
  I1009 09:44:21.607696  106109 apf_controller.go:898] Introducing queues for priority level "workload-low": config={"type":"Limited","limited":{"nominalConcurrencyShares":100,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":90}}, nominalCL=267, lendableCL=240, borrowingCL=600, currentCL=147, quiescing=false (shares=0xc003f787f0, shareSum=225)
  I1009 09:44:21.607712  106109 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=80, lendableCL=26, borrowingCL=600, currentCL=141, quiescing=false, numPending=0 (shares=0xc003e48418, shareSum=225)
  I1009 09:44:21.607735  106109 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=107, lendableCL=27, borrowingCL=600, currentCL=212, quiescing=false, numPending=0 (shares=0xc003f1a000, shareSum=225)
  I1009 09:44:21.607746  106109 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=27, lendableCL=0, borrowingCL=600, currentCL=71, quiescing=false, numPending=0 (shares=0xc003ee1118, shareSum=225)
  I1009 09:44:21.607775  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=64 concurrencyDenominator=64 backstop=false
  I1009 09:44:21.607812  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=125 concurrencyDenominator=125 backstop=false
  I1009 09:44:21.607845  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.599398793204581 seatDemandStdev=0.4900202852020225 seatDemandSmoothed=8.565840952559851 fairFrac=2.3607843137254902 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.607863  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=33 concurrencyDenominator=33 backstop=false
  I1009 09:44:21.607911  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=64 concurrencyDenominator=64 backstop=false
  I1009 09:44:21.607986  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=127 concurrencyDenominator=127 backstop=false
  I1009 09:44:21.608013  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=189 concurrencyDenominator=189 backstop=false
  I1009 09:44:21.609328  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","global-default"]
  I1009 09:44:21.609369  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","global-default"]
  I1009 09:44:21.609377  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.43573ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9d38d618-f814-490c-a87a-838c0932c4de" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.268579ms" resp=201
  I1009 09:44:21.609388  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["global-default"]
  I1009 09:44:21.609541  106109 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["global-default"]
  I1009 09:44:21.609571  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.609579  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.609591  106109 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=13, lendableCL=0, borrowingCL=600, currentCL=33, quiescing=false, numPending=0 (shares=0xc000582238, shareSum=245)
  I1009 09:44:21.609629  106109 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=74, lendableCL=24, borrowingCL=600, currentCL=127, quiescing=false, numPending=0 (shares=0xc003e48418, shareSum=245)
  I1009 09:44:21.609662  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="global-default"
  I1009 09:44:21.609668  106109 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=98, lendableCL=25, borrowingCL=600, currentCL=189, quiescing=false, numPending=0 (shares=0xc003f1a000, shareSum=245)
  I1009 09:44:21.609842  106109 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=25, lendableCL=0, borrowingCL=600, currentCL=64, quiescing=false, numPending=0 (shares=0xc003ee1118, shareSum=245)
  I1009 09:44:21.609871  106109 apf_controller.go:906] Retaining queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=98, lendableCL=49, borrowingCL=600, currentCL=125, quiescing=false, numPending=0 (shares=0xc003f1b888, shareSum=245)
  I1009 09:44:21.609899  106109 apf_controller.go:906] Retaining queues for priority level "workload-low": config={"type":"Limited","limited":{"nominalConcurrencyShares":100,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":90}}, nominalCL=245, lendableCL=221, borrowingCL=600, currentCL=64, quiescing=false, numPending=0 (shares=0xc003f787f0, shareSum=245)
  I1009 09:44:21.609928  106109 apf_controller.go:898] Introducing queues for priority level "global-default": config={"type":"Limited","limited":{"nominalConcurrencyShares":20,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=49, lendableCL=25, borrowingCL=600, currentCL=37, quiescing=false (shares=0xc003f78df0, shareSum=245)
  I1009 09:44:21.609967  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.609999  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.610016  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.610038  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.610065  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.610084  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.610112  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.609857  106109 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
  I1009 09:44:21.610176  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.584112855900022 seatDemandStdev=0.49287425117604367 seatDemandSmoothed=8.485597314113724 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.611140  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" latency="102.488168ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="51bf4d57-1b8c-4ca3-a350-7ed2a4596a5d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="102.420277ms" resp=201
  I1009 09:44:21.611890  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.611909  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.611939  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.957912ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0f7faa6f-4362-44e2-83be-dacfdf164d8d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.659802ms" resp=201
  I1009 09:44:21.611985  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.612033  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.612063  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.612089  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.612108  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.612134  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.423343762753366 seatDemandStdev=0.546860511222938 seatDemandSmoothed=8.404743274190562 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.612180  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.612201  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.612243  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-nodes"
  I1009 09:44:21.612436  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.611885254 +0000 UTC m=+1.849736447 Found This FlowSchema references the PriorityLevelConfiguration object named "system" and it exists} to FlowSchema system-nodes, which had ResourceVersion=32, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.611885254 +0000 UTC m=+1.849736447"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "system" and it exists`,
    }
  I1009 09:44:21.613929  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.4749ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="faa074b8-4619-4344-88d2-7f2f032bfc25" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.227518ms" resp=201
  I1009 09:44:21.614312  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-node-high"
  I1009 09:44:21.615744  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.186187ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bfa2dadc-df1e-4aad-865c-4fba7bb6eae7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.052207ms" resp=201
  I1009 09:44:21.616040  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="probes"
  I1009 09:44:21.617022  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-nodes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="4.080758ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3b89892c-6245-499c-894b-2295a53112b2" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.943137ms" resp=200
  I1009 09:44:21.617710  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.617726  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.617768  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.617793  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.617809  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.617825  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.617840  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.617846  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.563511ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="320e6009-9a4b-4dde-b71d-14f025eef061" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.43415ms" resp=201
  I1009 09:44:21.617856  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.618021  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.320632385709062 seatDemandStdev=0.6817440990469926 seatDemandSmoothed=8.326488838033569 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.618055  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.618146  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-leader-election"
  I1009 09:44:21.618213  106109 apf_controller.go:609] Controller writing Condition {Dangling True 2024-10-09 09:44:21.617693363 +0000 UTC m=+1.855544557 NotFound This FlowSchema references the PriorityLevelConfiguration object named "exempt" but there is no such object} to FlowSchema probes, which had ResourceVersion=34, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "True",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.617693363 +0000 UTC m=+1.855544557"},
  - 	Reason:             "",
  + 	Reason:             "NotFound",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "exempt" but there is no such object`,
    }
  I1009 09:44:21.619472  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.112027ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8e3b48e3-222d-4c9d-b229-a930cb071b18" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.010788ms" resp=201
  I1009 09:44:21.619738  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="workload-leader-election"
  I1009 09:44:21.621193  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.255079ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bd3c329d-39d3-4437-aaf6-cb00d3857771" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.131859ms" resp=201
  I1009 09:44:21.621342  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/probes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.382677ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="875de65a-f753-4278-be0a-742384109f31" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.263676ms" resp=200
  I1009 09:44:21.621438  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="endpoint-controller"
  I1009 09:44:21.621549  106109 cacher.go:1028] cacher (flowschemas.flowcontrol.apiserver.k8s.io): 1 objects queued in incoming channel.
  I1009 09:44:21.621573  106109 cacher.go:1028] cacher (flowschemas.flowcontrol.apiserver.k8s.io): 2 objects queued in incoming channel.
  I1009 09:44:21.621812  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.617697243 +0000 UTC m=+1.855548437 Found This FlowSchema references the PriorityLevelConfiguration object named "node-high" and it exists} to FlowSchema system-node-high, which had ResourceVersion=33, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.617697243 +0000 UTC m=+1.855548437"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "node-high" and it exists`,
    }
  I1009 09:44:21.622872  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.069037ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="cc0d116a-49fa-467b-9cfe-84271fd75fca" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="944.547µs" resp=201
  I1009 09:44:21.623197  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-controller-manager"
  I1009 09:44:21.624346  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-node-high/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.176385ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5a826857-915e-48c7-bb8b-f8945ddc28f1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.023294ms" resp=200
  I1009 09:44:21.624719  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.32028ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="42832e78-0941-4dd7-8f57-65312c4a1159" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.168689ms" resp=201
  I1009 09:44:21.624816  106109 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 09:44:21.624830  106109 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 09:44:21.624883  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.624907  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.624926  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.624942  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.624958  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.624976  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-scheduler"
  I1009 09:44:21.624976  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.218744524857166 seatDemandStdev=0.7349528328545932 seatDemandSmoothed=8.248914633986168 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.625168  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.625201  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.625328  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.624790902 +0000 UTC m=+1.862642095 Found This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists} to FlowSchema system-leader-election, which had ResourceVersion=36, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.624790902 +0000 UTC m=+1.862642095"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists`,
    }
  I1009 09:44:21.626228  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.027747ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c944a4af-0f2e-4bb1-9209-faa1371f4fad" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="903.657µs" resp=201
  I1009 09:44:21.626732  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-system-service-accounts"
  I1009 09:44:21.628085  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.173699ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2a0b7e22-8c82-4371-9514-d8f7ef71bb12" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="997.898µs" resp=201
  I1009 09:44:21.628235  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-leader-election/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.618287ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b0df7387-9847-4bc0-a3da-68ccbdb7e78f" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.356596ms" resp=200
  I1009 09:44:21.628352  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="service-accounts"
  I1009 09:44:21.628601  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.624791862 +0000 UTC m=+1.862643055 Found This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists} to FlowSchema workload-leader-election, which had ResourceVersion=37, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.624791862 +0000 UTC m=+1.862643055"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists`,
    }
  I1009 09:44:21.629910  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.338329ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="95b87f98-fadc-4756-b83d-1444b055f6c7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.209649ms" resp=201
  I1009 09:44:21.630078  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="global-default"
  I1009 09:44:21.630950  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/workload-leader-election/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.012854ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9fb37943-f744-42ab-a73e-7b5da1be83b7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.926853ms" resp=200
  I1009 09:44:21.631221  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="951.967µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="94c5bdde-eeb3-45c8-9dbb-b846e94bb7ca" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="854.186µs" resp=201
  I1009 09:44:21.631263  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.624792891 +0000 UTC m=+1.862644086 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema endpoint-controller, which had ResourceVersion=39, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.624792891 +0000 UTC m=+1.862644086"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 09:44:21.631409  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="mandatory" name="catch-all"
  I1009 09:44:21.632586  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.043797ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="290ee828-ef48-4d64-a9b9-6c3b8af3945e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="848.725µs" resp=201
  I1009 09:44:21.632773  106109 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="mandatory" name="exempt"
  I1009 09:44:21.632901  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/endpoint-controller/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.364889ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="23487781-4d4d-4271-8cb9-5f46d74fb206" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.25703ms" resp=200
  I1009 09:44:21.633181  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.624799531 +0000 UTC m=+1.862650716 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-controller-manager, which had ResourceVersion=40, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.624799531 +0000 UTC m=+1.862650716"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 09:44:21.633998  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="932.166µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="39596989-3959-444f-8b29-ac8f3b4bdaf3" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="811.795µs" resp=201
  I1009 09:44:21.634255  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="mandatory" name="exempt"
  I1009 09:44:21.634432  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-controller-manager/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.052356ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a40e284c-0b9f-4ea4-b7d0-1a9d8f87c971" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="980.247µs" resp=200
  I1009 09:44:21.634734  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.624799952 +0000 UTC m=+1.862651135 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-scheduler, which had ResourceVersion=42, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.624799952 +0000 UTC m=+1.862651135"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 09:44:21.635361  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="946.996µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="93128dc8-5923-431c-9dc5-f87f17c10ad5" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="830.465µs" resp=201
  I1009 09:44:21.635590  106109 strategy.go:270] "Successfully created *v1.FlowSchema" type="mandatory" name="catch-all"
  I1009 09:44:21.636516  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-scheduler/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.47815ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d840faa2-ebcd-4bbe-86c4-7729849b434e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.30641ms" resp=200
  I1009 09:44:21.636789  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.636824  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.636845  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.636860  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.224219677760309 seatDemandStdev=0.709168212333705 seatDemandSmoothed=8.172657518876647 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 09:44:21.636873  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.636888  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.636903  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.636919  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.637039  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636727392 +0000 UTC m=+1.874578587 Found This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists} to FlowSchema probes, which had ResourceVersion=38, because its previous value was {"type":"Dangling","status":"True","lastTransitionTime":"2024-10-09T09:44:21Z","reason":"NotFound","message":"This FlowSchema references the PriorityLevelConfiguration object named \"exempt\" but there is no such object"}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "True",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21 +0000 UTC"},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636727392 +0000 UTC m=+1.874578587"},
  - 	Reason:             "NotFound",
  + 	Reason:             "Found",
    	Message: strings.Join({
    		"This FlowSchema references the PriorityLevelConfiguration object",
    		` named "exempt" `,
  - 		"but there is no such object",
  + 		"and it exists",
    	}, ""),
    }
  I1009 09:44:21.638694  106109 admission.go:143] "Namespace existed in cache after waiting" namespace="kube-system"
  I1009 09:44:21.639040  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/probes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.768702ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7b63bf9d-cc69-4f1e-8f6c-c189bfaa0108" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.676581ms" resp=200
  I1009 09:44:21.639338  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/kube-system/configmaps" latency="51.547691ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="881c2022-4b96-4acd-a1ee-6f427b17ce83" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="51.20199ms" resp=201
  I1009 09:44:21.639350  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636731682 +0000 UTC m=+1.874582866 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-low" and it exists} to FlowSchema service-accounts, which had ResourceVersion=44, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636731682 +0000 UTC m=+1.874582866"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-low" and it exists`,
    }
  I1009 09:44:21.641050  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/service-accounts/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.350979ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="43207e9d-04a3-4d3b-9429-669e42a496c5" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.251279ms" resp=200
  I1009 09:44:21.641275  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636732882 +0000 UTC m=+1.874584076 Found This FlowSchema references the PriorityLevelConfiguration object named "catch-all" and it exists} to FlowSchema catch-all, which had ResourceVersion=53, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636732882 +0000 UTC m=+1.874584076"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "catch-all" and it exists`,
    }
  I1009 09:44:21.642646  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/catch-all/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.115708ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1450f42e-e0bb-4909-b3da-86ff0984fe21" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.027598ms" resp=200
  I1009 09:44:21.642962  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636734622 +0000 UTC m=+1.874585856 Found This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists} to FlowSchema exempt, which had ResourceVersion=51, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636734622 +0000 UTC m=+1.874585856"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists`,
    }
  I1009 09:44:21.645631  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/exempt/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.404687ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="55f76057-188d-4550-b24c-b4242e829d8b" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.283746ms" resp=200
  I1009 09:44:21.646132  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636738492 +0000 UTC m=+1.874589676 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-system-service-accounts, which had ResourceVersion=43, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636738492 +0000 UTC m=+1.874589676"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 09:44:21.648389  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-system-service-accounts/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.843083ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6032c8c3-656a-4bbf-9262-3f556e53d060" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.677501ms" resp=200
  I1009 09:44:21.648774  106109 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 09:44:21.636740172 +0000 UTC m=+1.874591356 Found This FlowSchema references the PriorityLevelConfiguration object named "global-default" and it exists} to FlowSchema global-default, which had ResourceVersion=46, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 09:44:21.636740172 +0000 UTC m=+1.874591356"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "global-default" and it exists`,
    }
  I1009 09:44:21.650786  106109 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/global-default/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.729721ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c41d3ba5-adc8-489b-bf7e-f28b2279a19c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.619432ms" resp=200
  I1009 09:44:21.651055  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.651108  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:21.651132  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=2.8534079353709445 seatDemandStdev=0.6011947498957604 seatDemandSmoothed=8.064142257703619 fairFrac=2.3333333333333335 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 09:44:21.651151  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:21.651175  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:21.651213  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:21.651234  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:21.651264  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:21.689510  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.689626  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="486.684µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5fba2ab2-8e41-4793-8647-2253b1917485" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="270.362µs" resp=500
  I1009 09:44:21.693881  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-public/resourcequotas" latency="91.325142ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="827c6635-0812-44ec-bc2d-8c04c64185ab" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="90.764928ms" resp=200
  I1009 09:44:21.695212  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="95.46178ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6717f906-e4c5-44fd-8ebd-c631b3418ed7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="94.593914ms" resp=201
  I1009 09:44:21.763760  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: healthz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.763890  106109 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="612.595µs" userAgent="Go-http-client/2.0" audit-ID="e00907dd-514c-458a-b42d-f2f5ccd6c6d0" srcIP="127.0.0.1:36146" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="279.872µs" resp=500
  I1009 09:44:21.787334  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: healthz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.787479  106109 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="390.023µs" userAgent="Go-http-client/2.0" audit-ID="dad38635-37e7-425d-b78d-39938756d43b" srcIP="127.0.0.1:36156" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="162.69µs" resp=500
  W1009 09:44:21.787839  106109 util.go:106] Health check on "https://127.0.0.1:6443/healthz" failed, status=500
  I1009 09:44:21.789569  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.789698  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="524.364µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c7969c00-e4f5-4b5d-bc89-99d2dd2c685c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="310.913µs" resp=500
  I1009 09:44:21.794591  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/default/resourcequotas" latency="98.124318ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3cba92c4-d290-417a-aa4e-a6f5ac04c199" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="97.995427ms" resp=200
  I1009 09:44:21.795693  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="100.00734ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="33cabd06-ecd3-4458-b73d-c9b995055048" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="99.887731ms" resp=201
  I1009 09:44:21.889927  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.890040  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="437.683µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7e75a429-37fb-4726-be2e-15110467d610" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="225.512µs" resp=500
  I1009 09:44:21.895291  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-node-lease/resourcequotas" latency="98.070577ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="43ec95ec-5b3d-457b-bd25-f8481b0abdc7" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="97.858137ms" resp=200
  I1009 09:44:21.896771  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="100.572225ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="71d3b407-4c3d-4c27-81d5-aa43e628b698" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="100.355183ms" resp=201
  I1009 09:44:21.989022  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:21.989135  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="417.242µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3385af18-a4ef-4e72-879d-9d0daf632d7b" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="213.801µs" resp=500
  I1009 09:44:22.089025  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:22.089154  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="452.403µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7862c3e7-cdf5-44c2-93d9-e92c7c5e522c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="240.742µs" resp=500
  I1009 09:44:22.188967  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:22.189070  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="410.763µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9b3c96e2-ef97-4d05-827c-ed339f4cfe6c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="202.331µs" resp=500
  I1009 09:44:22.289736  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:22.289851  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="467.403µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7803a32f-104c-44d8-9c1e-fb4e294139e1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="215.902µs" resp=500
  I1009 09:44:22.389573  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:22.389712  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="568.514µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4e7da880-51c5-4e62-ba0c-3306a38c9483" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="330.873µs" resp=500
  I1009 09:44:22.486222  106109 controller.go:106] OpenAPI AggregationController: Processing item k8s_internal_local_kube_aggregator_types
  I1009 09:44:22.487100  106109 controller.go:106] OpenAPI AggregationController: Processing item openapiv2converter
  I1009 09:44:22.488169  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/scheduling.k8s.io/v1/priorityclasses/system-node-critical" latency="1.082267ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dd43749e-1397-4082-b64b-0e3098da85d1" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="860.396µs" resp=404
  I1009 09:44:22.488643  106109 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 09:44:22.488722  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="344.722µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0561c93b-4a39-4650-b823-2dac2d649926" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="173.451µs" resp=500
  I1009 09:44:22.489881  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/scheduling.k8s.io/v1/priorityclasses" latency="1.282249ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6fac69c2-7ee0-4321-8940-654293f25879" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.177358ms" resp=201
  I1009 09:44:22.490092  106109 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
  I1009 09:44:22.490856  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/scheduling.k8s.io/v1/priorityclasses/system-cluster-critical" latency="598.864µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6b70df87-9c9e-4ba2-a267-4ea1016fc9b4" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="512.504µs" resp=404
  I1009 09:44:22.492969  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/scheduling.k8s.io/v1/priorityclasses" latency="1.54635ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3f63c472-715b-4021-b31b-0cd01c045852" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.231678ms" resp=201
  I1009 09:44:22.493197  106109 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
  I1009 09:44:22.493210  106109 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
  I1009 09:44:22.590805  106109 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="1.50286ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4d3a98e7-25d7-42e6-a5f2-588c7b52854f" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.179008ms" resp=200
  I1009 09:44:22.593391  106109 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.0.0.1"}
  I1009 09:44:22.593627  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/services" latency="2.381066ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d9103925-bba1-48d2-98d5-4d5df9bf6193" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.281045ms" resp=201
  I1009 09:44:22.595532  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="612.903µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ed1106cf-f55d-4511-838f-f6fdc0f7d28a" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="505.823µs" resp=404
  W1009 09:44:22.596019  106109 lease.go:265] Resetting endpoints for master service "kubernetes" to [82.112.230.236]
  I1009 09:44:22.596764  106109 controller.go:615] quota admission added evaluator for: endpoints
  I1009 09:44:22.597377  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/endpoints" latency="1.199438ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="155d1876-e0c4-4327-ae33-217b81244bb6" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.118788ms" resp=201
  I1009 09:44:22.598358  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="564.433µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f8633600-3841-4824-b653-2e67e071fe31" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="453.913µs" resp=404
  I1009 09:44:22.599938  106109 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
  I1009 09:44:22.601293  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices" latency="2.309895ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2970cee4-b690-485e-b43b-1abf772fd4e9" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.053924ms" resp=201
  I1009 09:44:22.765038  106109 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="400.103µs" userAgent="Go-http-client/2.0" audit-ID="355652de-8c38-4f72-9b9c-72beb35079fb" srcIP="127.0.0.1:36146" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="144.661µs" resp=200
  I1009 09:44:22.789114  106109 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="411.713µs" userAgent="Go-http-client/2.0" audit-ID="a1e3f66c-c877-4716-b36a-8e7cac2198d9" srcIP="127.0.0.1:36156" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="158.921µs" resp=200
  I1009 09:44:22.793174  106109 httplog.go:134] "HTTP" verb="GET" URI="/api" latency="953.816µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="e7310029-6c56-4b8a-9f2e-c77240d622ae" srcIP="127.0.0.1:36158" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="784.266µs" resp=200
  I1009 09:44:22.794978  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis" latency="803.075µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="764b4add-e718-4324-8b54-1fc2d2aa791b" srcIP="127.0.0.1:36158" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="622.103µs" resp=200
  I1009 09:44:22.795586  106109 internal_services.go:75] E2E services started.
  I1009 09:44:22.795654  106109 reflector.go:305] Starting reflector *v1.Namespace (5m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:22.795671  106109 reflector.go:341] Listing and watching *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:22.795749  106109 shared_informer.go:313] Waiting for caches to sync for namespace
  I1009 09:44:22.796336  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces?limit=500&resourceVersion=0" latency="460.604µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="9ccca773-c376-4503-aa06-66d9af161917" srcIP="127.0.0.1:36158" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="309.233µs" resp=200
  I1009 09:44:22.796802  106109 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 09:44:22.797260  106109 get.go:278] "Starting watch" path="/api/v1/namespaces" resourceVersion="64" labels="" fields="" timeout="6m44s"
  I1009 09:44:22.855870  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes?fieldSelector=metadata.name%3Dsrv579909&limit=500&resourceVersion=0" latency="512.124µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6ee369a3-b575-4775-96e8-0eb79327508b" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="252.241µs" resp=200
  I1009 09:44:22.856217  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0" latency="217.471µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c6fc6a60-088f-487c-8e62-e6285488383b" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="127.491µs" resp=200
  I1009 09:44:22.856491  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909?resourceVersion=0" latency="193.08µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d120397a-8009-473e-8762-1331a3f2db8f" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="120.152µs" resp=404
  I1009 09:44:22.856736  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0" latency="179.961µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ee13f3b8-41b3-4686-8b35-c5895de497ea" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="109.751µs" resp=200
  I1009 09:44:22.857743  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="949.796µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="79534d36-27b3-4fb6-b5ae-90344ba4682f" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="871.976µs" resp=404
  I1009 09:44:22.858387  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="5.440577ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="25db0b7c-4ded-4ef0-98bc-88da17c6ce2c" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.195646ms" resp=201
  I1009 09:44:22.861306  106109 get.go:278] "Starting watch" path="/api/v1/nodes" resourceVersion="1" labels="" fields="metadata.name=srv579909" timeout="5m54s"
  I1009 09:44:22.861593  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909?timeout=10s" latency="780.136µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="27d6eed8-7549-4b88-8759-38c6cd005c0e" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="653.365µs" resp=404
  I1009 09:44:22.861737  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="970.366µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="11ad31ea-0a99-4940-8d3f-931a53e8b97f" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="586.854µs" resp=404
  I1009 09:44:22.862001  106109 get.go:278] "Starting watch" path="/api/v1/services" resourceVersion="68" labels="" fields="spec.clusterIP!=None" timeout="5m16s"
  I1009 09:44:22.862388  106109 get.go:278] "Starting watch" path="/apis/storage.k8s.io/v1/csidrivers" resourceVersion="1" labels="" fields="" timeout="6m58s"
  I1009 09:44:22.863194  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="579.714µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="416c6bc3-e08d-4dbd-aeed-f502824019c5" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="489.484µs" resp=404
  I1009 09:44:22.876568  106109 httplog.go:134] "HTTP" verb="LIST" URI="/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0" latency="577.034µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="181b3a67-2a39-4207-bef0-baab465c97bf" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="331.431µs" resp=200
  I1009 09:44:22.877562  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="676.605µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="53b9b2cb-d812-40ec-a454-426e03fadd94" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="532.004µs" resp=404
  I1009 09:44:22.878253  106109 get.go:278] "Starting watch" path="/apis/node.k8s.io/v1/runtimeclasses" resourceVersion="1" labels="" fields="" timeout="5m33s"
  I1009 09:44:22.879257  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="823.326µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d9647347-915e-4631-b40d-94e83112ac48" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="520.703µs" resp=404
  I1009 09:44:22.882613  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.969804ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fe811753-436f-4594-a4f8-7aefdfe9b69c" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.840623ms" resp=201
  I1009 09:44:22.884673  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.420419ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6a23c886-a46f-4184-969b-e3015724ff26" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.29795ms" resp=201
  I1009 09:44:22.886257  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.157997ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="79138cba-5acb-4e48-a636-04dde01f9f6d" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.051037ms" resp=201
  I1009 09:44:22.887554  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="886.186µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ae555606-3e9e-46c5-94e0-ec28ba0a2c87" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="795.855µs" resp=201
  I1009 09:44:22.896119  106109 shared_informer.go:320] Caches are synced for namespace
  I1009 09:44:22.934823  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="1.407801ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="22c3474f-68a9-426a-a352-14d79f779951" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.070177ms" resp=404
  I1009 09:44:22.936735  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="1.073447ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="648b9b1b-2fc7-4761-b050-9614bcaef369" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="824.367µs" resp=404
  I1009 09:44:22.988264  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/nodes" latency="2.136154ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="537d324e-f385-4c4e-a6b1-985fd06b062f" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.853843ms" resp=201
  I1009 09:44:22.990137  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fcbfa2c8f0c1ad" latency="3.665634ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="618ccead-90d6-46ac-a36c-266a3a493e21" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.326012ms" resp=200
  I1009 09:44:22.992661  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fcbfa2c8f10497" latency="1.842473ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="40a41d34-8d64-433f-b4a9-fbba6e0a76ea" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.720062ms" resp=200
  I1009 09:44:22.993581  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/nodes/srv579909/status?timeout=10s" latency="2.577038ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dba71bce-1fd2-416f-9b78-889027aa139d" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.448297ms" resp=200
  I1009 09:44:22.994904  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fcbfa2c8f122bf" latency="1.609869ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a23c269c-d0d7-408a-a3ea-66a92529bfd1" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.52063ms" resp=200
  I1009 09:44:23.197400  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="1.188418ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="322d9095-47e7-446d-a9f1-cda35d41ecef" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="921.217µs" resp=404
  I1009 09:44:23.198824  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="774.894µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="faf667e4-c653-4a96-9d7f-4a513db9af5d" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="643.815µs" resp=200
  I1009 09:44:23.201784  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/storage.k8s.io/v1/csinodes" latency="2.356335ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="509cc1f2-4c59-4ba6-9f71-368e7450928a" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.175865ms" resp=201
  I1009 09:44:23.306531  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="2.814178ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="036d47a2-ce86-4365-be49-93255eb5764c" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.345086ms" resp=201
  I1009 09:44:23.307017  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/nodes/srv579909/status?timeout=10s" latency="2.353116ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f019ed27-da45-42bd-a305-fa246e1ed3cc" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.168354ms" resp=200
  I1009 09:44:23.806023  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="3.646835ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8a2d5b94-f1ea-406d-b6f0-7f888e4614dc" srcIP="127.0.0.1:36210" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.01044ms" resp=200
  I1009 09:44:23.812523  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.835983ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c979e78b-ab29-4dbc-bea6-ec9e39437519" srcIP="127.0.0.1:36210" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.46193ms" resp=200
  I1009 09:44:23.825938  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="8.834879ms" userAgent="Go-http-client/1.1" audit-ID="11e9e96d-bdb9-45cc-bfd5-9804bbad6f18" srcIP="127.0.0.1:36220" resp=200
  I1009 09:44:23.828757  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.346319ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a06d7f20-7ce6-4802-bd5a-a29951067e16" srcIP="127.0.0.1:36222" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.097487ms" resp=200
  I1009 09:44:23.830667  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="2.645867ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bccabb79-726c-4d49-afe1-e18374a5cca2" srcIP="127.0.0.1:36250" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.512607ms" resp=200
  I1009 09:44:23.832487  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.766702ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="933e9aef-8060-40ba-ab48-0ddcad3a23fa" srcIP="127.0.0.1:36238" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.44637ms" resp=200
  I1009 09:44:23.841498  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.559552ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4258e386-f449-4b27-86eb-262401ef1f9e" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.3268ms" resp=200
  I1009 09:44:23.841999  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="2.420947ms" userAgent="Go-http-client/1.1" audit-ID="5ce13800-0ab4-49a7-9b89-161e314f7a8b" srcIP="127.0.0.1:36274" resp=200
  I1009 09:44:23.844021  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="4.49949ms" userAgent="Go-http-client/1.1" audit-ID="dabb091c-b94a-45c9-8d7d-5f5498972451" srcIP="127.0.0.1:36288" resp=200
  I1009 09:44:23.851623  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/pods?fieldSelector=spec.nodeName%3Dsrv579909&limit=500&resourceVersion=0" latency="672.874µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="39da1929-23ec-478e-bb7f-b8f9cc91696d" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="452.994µs" resp=200
  I1009 09:44:23.852978  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="4.798382ms" userAgent="Go-http-client/1.1" audit-ID="d64b7815-8852-4227-a4d5-7f36e3651b26" srcIP="127.0.0.1:36294" resp=200
  I1009 09:44:23.853615  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="1.4169ms" userAgent="Go-http-client/1.1" audit-ID="f5e48bfc-da0d-43e7-b27e-0a7e21210a92" srcIP="127.0.0.1:36300" resp=200
  I1009 09:44:23.855038  106109 get.go:278] "Starting watch" path="/api/v1/pods" resourceVersion="1" labels="" fields="spec.nodeName=srv579909" timeout="6m31s"
  I1009 09:44:23.888646  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="3.990116ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c74b1150-546e-4169-a0de-23f9d8f72625" srcIP="127.0.0.1:36306" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.521274ms" resp=200
  I1009 09:44:23.898362  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.153308ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1068b6d1-7e2c-4d37-af0e-7fae64981e96" srcIP="127.0.0.1:36314" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="954.186µs" resp=200
  I1009 09:44:23.905216  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="2.544457ms" userAgent="Go-http-client/1.1" audit-ID="8e5101b2-8124-4f3b-8694-eb42017b34d5" srcIP="127.0.0.1:36326" resp=200
  I1009 09:44:23.913115  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.767862ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="00ca091d-2caf-4798-a87c-5172a6ffad44" srcIP="127.0.0.1:36334" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.563561ms" resp=200
  I1009 09:44:23.918157  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="3.396043ms" userAgent="Go-http-client/1.1" audit-ID="2b4983d4-65a8-466d-b3b7-c9e38a43d45b" srcIP="127.0.0.1:36342" resp=200
  I1009 09:44:23.928018  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="5.84104ms" userAgent="Go-http-client/1.1" audit-ID="5c2fd9c9-412e-4489-b0fd-418ef4ac10a2" srcIP="127.0.0.1:36348" resp=200
  I1009 09:44:23.947024  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/prestop-hook-test-3521/resourcequotas" latency="765.955µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="72a0fd15-34a3-45db-9a78-45f6554ce439" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="680.005µs" resp=200
  I1009 09:44:23.953660  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="8.627948ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="d3ec9119-811b-4c38-83b3-2ddde4618ee1" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="8.403408ms" resp=201
  I1009 09:44:23.963069  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/prestop-hook-test-3521/limitranges" latency="1.47562ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e0e96b08-2396-4a03-9b8c-c16bfabaf7ca" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.355319ms" resp=200
  I1009 09:44:23.969255  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/pods" latency="11.61423ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="656dd8ae-636e-46db-baf7-41a476724fc9" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="11.385527ms" resp=201
  I1009 09:44:23.969995  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.099967ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="41300723-ce96-432a-b03b-371ee724f18f" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="915.126µs" resp=200
  I1009 09:44:23.971596  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="818.945µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="48010a61-53a1-4321-8c29-272e922c0123" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="556.904µs" resp=200
  I1009 09:44:23.985176  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod/status" latency="12.298294ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ec043dc2-00e7-4795-9569-cf8b4d1595c3" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="12.145532ms" resp=200
  I1009 09:44:24.387316  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/events" latency="3.134572ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e20f1ba9-ed4b-423c-b277-a6bbe65e3e30" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.830711ms" resp=201
  I1009 09:44:24.402089  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/events" latency="3.377373ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d99acd33-e306-414d-9236-539e99d6f5c5" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.077122ms" resp=201
  I1009 09:44:24.458172  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/events" latency="4.878063ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b874582e-d875-4141-a1c5-ceefc70e2c1a" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.385213ms" resp=201
  I1009 09:44:24.886642  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.40205ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9f2b9518-57d8-4257-a90e-4b3843818072" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.079228ms" resp=200
  I1009 09:44:24.891791  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod/status" latency="4.219768ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ae43d687-484d-4618-ab68-5d803a6a29e0" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.946456ms" resp=200
  I1009 09:44:25.974116  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.46128ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="0cf276a8-fe95-487a-870a-c5d7c1969ddb" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.187937ms" resp=200
  I1009 09:44:27.977680  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.787292ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="262d07d8-8a3b-4c85-b7d8-cf205b8c8c68" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.4846ms" resp=200
  I1009 09:44:29.981614  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.697282ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="c2f8e158-89a0-43f1-8eef-122debea433f" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.40579ms" resp=200
  I1009 09:44:31.588162  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=0.06157820349455323 seatDemandStdev=0.3299045765741285 seatDemandSmoothed=7.887671089718015 fairFrac=2.3333333333333335 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 09:44:31.588262  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:31.588283  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:31.588309  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:31.588332  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:31.588368  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:31.588412  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:31.588492  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:31.856622  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.008934ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7cccf62c-4bf0-4887-ba76-dabf5913e173" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.778302ms" resp=200
  I1009 09:44:31.985180  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.765252ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="d348a4ea-b274-4857-8bf4-83b87bae3bde" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.369109ms" resp=200
  I1009 09:44:32.595262  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.178769ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c6b79525-8d0a-452c-b9df-728ae1847e04" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="962.457µs" resp=200
  I1009 09:44:32.596950  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="848.185µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="27427368-5776-43d6-8fb6-8d5466cb00f9" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="705.695µs" resp=200
  I1009 09:44:33.200995  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.088956ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="eaad6d85-0595-49e2-a0fb-05077f02105e" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="851.096µs" resp=404
  I1009 09:44:33.202653  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909?timeout=10s" latency="897.427µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a324c060-82a6-430e-a579-795bc8f68c7c" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="772.885µs" resp=200
  I1009 09:44:33.204294  106109 httplog.go:134] "HTTP" verb="POST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases?timeout=10s" latency="1.198359ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ebfc20a6-3446-4601-be8c-37ba60a13964" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.101687ms" resp=201
  I1009 09:44:33.988905  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.915113ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="1d470812-627b-4e9d-9e91-e9d68c6bba4a" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.575041ms" resp=200
  I1009 09:44:35.992654  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.743132ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="72e9d069-eb0d-4803-9d7d-e8390c740012" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.45278ms" resp=200
  I1009 09:44:37.995085  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.401829ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="01af36aa-7713-4553-95d1-37b060ecf8b1" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.144138ms" resp=200
  I1009 09:44:39.998696  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.616691ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="dfd04d25-ab70-4d82-941a-795892c9ecaa" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.317748ms" resp=200
  I1009 09:44:41.588853  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:41.588947  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:41.588970  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:41.588998  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:41.589019  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0013222305603552331 seatDemandStdev=0.03633844062009948 seatDemandSmoothed=7.707120850091651 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:44:41.589042  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:41.589062  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:41.589086  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:41.882502  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.182814ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="737dd7c8-7a96-4640-ae5b-75459f1fd5ce" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.857263ms" resp=200
  I1009 09:44:42.002422  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.810692ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a8fdebd3-878f-45b1-824b-e8f699eea710" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.514021ms" resp=200
  I1009 09:44:42.596975  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.341049ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7b97df83-e1f0-4954-b9af-f4a6eef2c0df" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.085377ms" resp=200
  I1009 09:44:42.598647  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="808.425µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a702caee-61fe-4cd7-88bc-7c9862813c63" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="656.575µs" resp=200
  I1009 09:44:43.321413  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.146035ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="47544878-0516-4229-841b-5001356ef713" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.826343ms" resp=200
  I1009 09:44:43.714300  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/events" latency="2.283936ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="571292f7-2c32-4355-a57e-7e64d11e06c7" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.008614ms" resp=201
  I1009 09:44:43.716705  106109 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-3521/events" latency="1.717851ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2320e29e-dfee-4333-a834-ae1d0d470030" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.55017ms" resp=201
  I1009 09:44:44.005775  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.653663ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b264cb24-1ba8-460f-9414-9f830dfe866f" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.311409ms" resp=200
  I1009 09:44:46.008361  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.47775ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="e53a35a4-47cd-4718-b0ea-185f4a2b5861" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.236878ms" resp=200
  I1009 09:44:48.011766  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.907653ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a993164c-4aac-4986-bebf-a4077d8c1089" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.607801ms" resp=200
  I1009 09:44:50.014986  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.970953ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="674471f6-43ed-480f-9592-b37df150e963" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.67275ms" resp=200
  I1009 09:44:51.509817  106109 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 09:44:51.509986  106109 remote_available_controller.go:466] Updating v1.
  I1009 09:44:51.510047  106109 local_available_controller.go:207] Updating v1.
  I1009 09:44:51.510071  106109 local_available_controller.go:207] Updating v1.batch
  I1009 09:44:51.510122  106109 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 09:44:51.510141  106109 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 09:44:51.510129  106109 remote_available_controller.go:466] Updating v1.batch
  I1009 09:44:51.510202  106109 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 09:44:51.510230  106109 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 09:44:51.510217  106109 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 09:44:51.510303  106109 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 09:44:51.510240  106109 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 09:44:51.510337  106109 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 09:44:51.510351  106109 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 09:44:51.510346  106109 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 09:44:51.510369  106109 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 09:44:51.510373  106109 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 09:44:51.510382  106109 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 09:44:51.510391  106109 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 09:44:51.510400  106109 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 09:44:51.510405  106109 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 09:44:51.510414  106109 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 09:44:51.510421  106109 remote_available_controller.go:466] Updating v1.policy
  I1009 09:44:51.510415  106109 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 09:44:51.510436  106109 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 09:44:51.510513  106109 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 09:44:51.510535  106109 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 09:44:51.510544  106109 local_available_controller.go:207] Updating v1.policy
  I1009 09:44:51.510540  106109 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 09:44:51.510556  106109 remote_available_controller.go:466] Updating v1.apps
  I1009 09:44:51.510571  106109 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 09:44:51.510583  106109 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 09:44:51.510572  106109 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 09:44:51.510601  106109 local_available_controller.go:207] Updating v2.autoscaling
  I1009 09:44:51.510608  106109 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 09:44:51.510622  106109 local_available_controller.go:207] Updating v1.apps
  I1009 09:44:51.510631  106109 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 09:44:51.510627  106109 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:44:51.510659  106109 local_available_controller.go:207] Updating v1.autoscaling
  I1009 09:44:51.510691  106109 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 09:44:51.510697  106109 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:44:51.589525  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.001655023631302062 seatDemandStdev=0.04064830289301011 seatDemandSmoothed=7.530830047049602 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:44:51.589593  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:44:51.589612  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:44:51.589629  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:44:51.589673  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:44:51.589691  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:51.589712  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:44:51.589729  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:44:52.019789  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.799662ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="95817026-a30b-461f-8cb2-556636c239f4" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.491621ms" resp=200
  I1009 09:44:52.056003  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.165964ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d1ceffb9-8970-4a73-aa57-a8287ef74ff2" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.937073ms" resp=200
  I1009 09:44:52.596837  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.094097ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8dab1ef5-98e2-446f-a576-1f8207dc4e41" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="891.626µs" resp=200
  I1009 09:44:52.598445  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="755.975µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="68ed385f-fc94-4d3b-a6c5-d4749020ad5d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="590.254µs" resp=200
  I1009 09:44:53.707562  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.831273ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0f76c897-f7ca-4290-8e1b-f8fcd88c62b3" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.578461ms" resp=200
  I1009 09:44:54.023684  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.609131ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="41cb75bf-54d6-46de-ba34-23e049f61dec" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.3432ms" resp=200
  I1009 09:44:56.027023  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.411469ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="20390b97-7870-499f-ae43-8c3e51b152d0" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.170469ms" resp=200
  I1009 09:44:56.877307  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/events/test-pod.17fcbfa7a29a6bc8" latency="3.602253ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="86174456-8d66-4e27-8021-336f52a01f68" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.328743ms" resp=200
  I1009 09:44:58.029992  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.650512ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4eafabf0-1364-4dd2-a858-19fcbced50a5" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.291808ms" resp=200
  I1009 09:45:00.034305  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.940313ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4d5edfc6-fcbf-4f32-b988-ae426cf13a45" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.605701ms" resp=200
  I1009 09:45:01.589979  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:01.590079  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:01.590101  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0015426002187371032 seatDemandStdev=0.03924564438638071 seatDemandSmoothed=7.358559085593379 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:01.590125  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:01.590147  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:01.590169  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:01.590191  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:01.590215  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:02.045360  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="8.096105ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="51b47320-841f-4747-8b34-36150abcc0f3" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="7.179089ms" resp=200
  I1009 09:45:02.305229  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="6.838947ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0ca8fbb0-6220-482d-9fa1-5d5eaeba14c5" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="6.546664ms" resp=200
  I1009 09:45:02.599389  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="3.950887ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="111b939a-5c5b-4f02-a17e-ff418035983c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.701026ms" resp=200
  I1009 09:45:02.603092  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="1.760282ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="336651dc-ccb0-474f-8c50-c6069f016b92" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.214769ms" resp=200
  I1009 09:45:03.736930  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.329007ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="60af8a7c-0c3b-485f-aac8-6a1a76e0801b" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.050204ms" resp=200
  I1009 09:45:04.052940  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.884683ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="ecdd9bd3-ee23-4281-a418-c7c44244f6a1" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.59694ms" resp=200
  I1009 09:45:06.055815  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.611861ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a66e93d7-b130-4752-8d5c-74ec48993745" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.322999ms" resp=200
  I1009 09:45:08.058664  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.609081ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="9fa4fb6d-d937-443e-b036-3c23ad87956d" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.36553ms" resp=200
  I1009 09:45:10.061656  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.812592ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="75fb587e-fdfc-4adc-85ee-36a342c519c1" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.567681ms" resp=200
  I1009 09:45:11.591204  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:11.591272  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:11.591298  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:11.591316  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:11.591338  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:11.591360  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.00274393582506719 seatDemandStdev=0.05231067425731677 seatDemandSmoothed=7.190578482656626 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:11.591377  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:11.591393  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:12.065225  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.474451ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b0124d36-53ee-4ef7-82e1-ba5c23d1ed18" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.196428ms" resp=200
  I1009 09:45:12.572054  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.021554ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="74e8a453-c5ac-457b-a47b-b7840d39b7c8" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.769732ms" resp=200
  I1009 09:45:12.597430  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.059817ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="072bc0e5-3f6a-4a2b-8815-b527d43e60bf" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="830.276µs" resp=200
  I1009 09:45:12.598716  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="588.264µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="24e54b08-d22c-435c-b76d-8943a763814b" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="502.333µs" resp=200
  I1009 09:45:13.891733  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.166125ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6b036045-cd3b-4f49-88d8-fd7f4c55957c" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.859103ms" resp=200
  I1009 09:45:14.069068  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.583891ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="57044a66-1b39-4ea7-8c6d-9806feb19127" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.286819ms" resp=200
  I1009 09:45:16.072622  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.790861ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="15db1baa-c2d3-476f-823b-bd2849d90391" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.483699ms" resp=200
  I1009 09:45:18.076324  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.490281ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="26703e82-2752-4ac7-a024-09673bb9db82" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.220368ms" resp=200
  I1009 09:45:20.079897  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.725423ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="ce885fa4-1b7d-4058-92f5-1d84ee855960" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.47182ms" resp=200
  I1009 09:45:21.510966  106109 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 09:45:21.511091  106109 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 09:45:21.511107  106109 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 09:45:21.511126  106109 local_available_controller.go:207] Updating v1.autoscaling
  I1009 09:45:21.511133  106109 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 09:45:21.511222  106109 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:45:21.511270  106109 remote_available_controller.go:466] Updating v1.apps
  I1009 09:45:21.511288  106109 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 09:45:21.511217  106109 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:45:21.511310  106109 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 09:45:21.511320  106109 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 09:45:21.511332  106109 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 09:45:21.511338  106109 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 09:45:21.511348  106109 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 09:45:21.511315  106109 local_available_controller.go:207] Updating v1.apps
  I1009 09:45:21.511360  106109 remote_available_controller.go:466] Updating v1.
  I1009 09:45:21.511376  106109 remote_available_controller.go:466] Updating v1.batch
  I1009 09:45:21.511383  106109 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 09:45:21.511406  106109 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 09:45:21.511363  106109 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 09:45:21.511420  106109 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 09:45:21.511438  106109 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 09:45:21.511451  106109 remote_available_controller.go:466] Updating v1.policy
  I1009 09:45:21.511516  106109 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 09:45:21.511523  106109 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 09:45:21.511491  106109 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 09:45:21.511536  106109 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 09:45:21.511552  106109 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 09:45:21.511577  106109 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 09:45:21.511559  106109 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 09:45:21.511594  106109 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 09:45:21.511612  106109 local_available_controller.go:207] Updating v1.
  I1009 09:45:21.511618  106109 local_available_controller.go:207] Updating v1.batch
  I1009 09:45:21.511625  106109 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 09:45:21.511638  106109 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 09:45:21.511660  106109 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 09:45:21.511670  106109 local_available_controller.go:207] Updating v1.policy
  I1009 09:45:21.511684  106109 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 09:45:21.511692  106109 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 09:45:21.511708  106109 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 09:45:21.511713  106109 local_available_controller.go:207] Updating v2.autoscaling
  I1009 09:45:21.591872  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:21.591941  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:21.591960  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0011785522849036092 seatDemandStdev=0.03430981345643485 seatDemandSmoothed=7.026011409967574 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:21.591977  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:21.591996  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:21.592013  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:21.592029  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:21.592061  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:22.083213  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.443ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="8822dfa7-6090-4341-86fa-8ab24065d221" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.157379ms" resp=200
  I1009 09:45:22.487284  106109 controller.go:106] OpenAPI AggregationController: Processing item k8s_internal_local_kube_aggregator_types
  I1009 09:45:22.487524  106109 controller.go:106] OpenAPI AggregationController: Processing item openapiv2converter
  I1009 09:45:22.597190  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="930.837µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ef8d7075-60ab-43d0-958e-5a525385bcba" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="743.934µs" resp=200
  I1009 09:45:22.598659  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="636.875µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0abe89ca-3a84-4f37-a6cb-2ba2a491d8eb" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="550.214µs" resp=200
  I1009 09:45:22.870669  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.705652ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="698d1de6-2a20-4c5a-baa5-646ddea372dc" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.48988ms" resp=200
  I1009 09:45:23.974910  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.249976ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ec962c4d-c85f-4d17-a74a-fb689f000ab4" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.994743ms" resp=200
  I1009 09:45:24.086846  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.853922ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="ada33582-6630-4eb6-a2d3-865aa794d135" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.55162ms" resp=200
  I1009 09:45:26.090070  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.891273ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="bcd6426c-ea81-48df-8c90-f0aadb50f1f6" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.594171ms" resp=200
  I1009 09:45:28.093769  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.52322ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a9b0b3d1-d7e7-438a-8d36-ca2e15749215" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.185847ms" resp=200
  I1009 09:45:30.097081  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.769552ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="81dd03af-99b0-46ba-90af-87112c4d68f0" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.412289ms" resp=200
  I1009 09:45:31.592685  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:31.592790  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0011922992558298072 seatDemandStdev=0.03450909558818305 seatDemandSmoothed=6.865234279619733 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:31.592828  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:31.592857  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:31.592887  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:31.592915  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:31.592953  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:31.592979  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:32.101205  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.904383ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="e059cbe3-d132-48d5-be1c-538362a4cf59" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.539091ms" resp=200
  I1009 09:45:32.598331  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="948.446µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="cc677243-89cb-4131-823e-4160de1379b3" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="662.295µs" resp=200
  I1009 09:45:32.600002  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="729.896µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c835a8e9-dff7-4786-a7e0-29bfa99ad75c" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="591.374µs" resp=200
  I1009 09:45:33.089392  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.832922ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fa40a69a-6424-4081-a0b8-f12456edd6cd" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.599892ms" resp=200
  I1009 09:45:34.104818  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.40608ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="bb428373-d76b-4358-ab1a-8171d9cb98c5" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.073358ms" resp=200
  I1009 09:45:34.257449  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.441036ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a712e135-66bf-439e-bfab-ff3a7d99c367" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.109174ms" resp=200
  I1009 09:45:36.108753  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.829722ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="52dcaa35-297e-40fe-ad14-30ec3699e7a8" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.554461ms" resp=200
  I1009 09:45:37.876711  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/events/test-pod.17fcbfa7a29a6bc8" latency="3.00979ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="662a974e-7014-4c6e-8631-a25727b7ce16" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.698818ms" resp=200
  I1009 09:45:38.112325  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.50504ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="ddf17c42-3186-4f34-848b-7838f526bb54" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.228249ms" resp=200
  I1009 09:45:40.115602  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.895983ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="3e0dadb2-0eec-4ef7-a709-7c8bf86d996e" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.54824ms" resp=200
  I1009 09:45:41.593359  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:41.593545  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:41.593579  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:41.593600  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:41.593616  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0014822633765327101 seatDemandStdev=0.03847162944060597 seatDemandSmoothed=6.708252830723273 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:41.593648  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:41.593664  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:41.593681  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:42.118931  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.303949ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4e4a1c2a-d6f3-4f43-aa53-4c06e19c8d2f" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.031977ms" resp=200
  I1009 09:45:42.598805  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.046957ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="99f59dd6-0d60-4d89-92f9-5b85a73b2b5d" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="822.915µs" resp=200
  I1009 09:45:42.600262  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="705.754µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="aed64eb5-15e1-4a1c-820d-65095667db6a" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="587.734µs" resp=200
  I1009 09:45:43.317890  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.746522ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a25b37f0-061e-4daf-909f-7e7718e4497e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.551201ms" resp=200
  I1009 09:45:44.122338  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.427049ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="0fb53fc1-3cd6-4bcb-b779-ba9c22c29fcb" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.160688ms" resp=200
  I1009 09:45:44.604840  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.954163ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f31931c8-35d5-4832-b34a-9ecd83a6a0fa" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.700191ms" resp=200
  I1009 09:45:46.125671  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.745711ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="10d1d993-b3cf-40e4-87a9-bbef589bfc77" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.45206ms" resp=200
  I1009 09:45:48.129333  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.933383ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="9bcc3ca2-f0ff-4e39-8ca9-d6e10949385f" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.546781ms" resp=200
  I1009 09:45:50.132211  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.45335ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="357f4848-ad36-4bd1-878d-dcdaac05f5ab" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.195639ms" resp=200
  I1009 09:45:51.512049  106109 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 09:45:51.512164  106109 remote_available_controller.go:466] Updating v1.batch
  I1009 09:45:51.512193  106109 local_available_controller.go:207] Updating v1.batch
  I1009 09:45:51.512201  106109 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 09:45:51.512241  106109 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 09:45:51.512258  106109 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 09:45:51.512266  106109 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 09:45:51.512324  106109 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 09:45:51.512354  106109 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 09:45:51.512348  106109 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 09:45:51.512371  106109 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 09:45:51.512410  106109 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 09:45:51.512433  106109 local_available_controller.go:207] Updating v1.
  I1009 09:45:51.512441  106109 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 09:45:51.512483  106109 remote_available_controller.go:466] Updating v1.
  I1009 09:45:51.512530  106109 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 09:45:51.512538  106109 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 09:45:51.512549  106109 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 09:45:51.512540  106109 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 09:45:51.512585  106109 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 09:45:51.512597  106109 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 09:45:51.512605  106109 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 09:45:51.512613  106109 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 09:45:51.512636  106109 local_available_controller.go:207] Updating v1.policy
  I1009 09:45:51.512639  106109 remote_available_controller.go:466] Updating v1.policy
  I1009 09:45:51.512646  106109 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 09:45:51.512663  106109 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 09:45:51.512677  106109 local_available_controller.go:207] Updating v2.autoscaling
  I1009 09:45:51.512682  106109 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 09:45:51.512695  106109 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 09:45:51.512689  106109 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 09:45:51.512709  106109 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 09:45:51.512718  106109 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 09:45:51.512725  106109 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 09:45:51.512729  106109 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 09:45:51.512752  106109 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:45:51.512765  106109 local_available_controller.go:207] Updating v1.autoscaling
  I1009 09:45:51.512770  106109 remote_available_controller.go:466] Updating v1.apps
  I1009 09:45:51.512773  106109 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 09:45:51.512788  106109 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 09:45:51.512806  106109 local_available_controller.go:207] Updating v1.apps
  I1009 09:45:51.594033  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:51.594121  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:45:51.594144  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0011206093662123118 seatDemandStdev=0.03345674223322811 seatDemandSmoothed=6.554758294703425 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:45:51.594162  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:45:51.594182  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:45:51.594200  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:45:51.594215  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:45:51.594248  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:45:52.136159  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="2.004185ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b59af73f-09cb-406c-a66e-3e922359d8ab" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.686882ms" resp=200
  I1009 09:45:52.600106  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.145857ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="610c1d40-aca6-44c2-ae66-4a71143f5b1e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="887.607µs" resp=200
  I1009 09:45:52.602021  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="809.655µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="241183e3-7185-42e1-8639-78969e879dda" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="684.944µs" resp=200
  I1009 09:45:53.645410  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.794512ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a8a65290-efdf-4cda-af34-1c83a7c74a61" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.55773ms" resp=200
  I1009 09:45:54.138723  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.53089ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="fd479ae6-11c5-44ce-9698-e3043bc16e54" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.229719ms" resp=200
  I1009 09:45:54.782063  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.301316ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dc91a3f5-0caf-4a9c-8fb4-37ffe243f3e6" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.015603ms" resp=200
  I1009 09:45:56.142494  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.629351ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="1baef75c-0901-497e-969d-98af79430cec" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.363279ms" resp=200
  I1009 09:45:58.146312  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.747111ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="2fc376ce-495f-4728-aa92-d984513fbae2" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.43612ms" resp=200
  I1009 09:46:00.150420  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="2.384916ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="26e008a4-3cf2-4308-9db3-044901d6aff6" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.053444ms" resp=200
  I1009 09:46:01.594778  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 09:46:01.594927  106109 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:46:01.594954  106109 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 09:46:01.594979  106109 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 09:46:01.595000  106109 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.001309806502799078 seatDemandStdev=0.03616753944802305 seatDemandSmoothed=6.404860832882115 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 09:46:01.595025  106109 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 09:46:01.595045  106109 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 09:46:01.595065  106109 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 09:46:02.154063  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="2.085573ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="3edb5bd4-a10b-4e93-892d-5cb8232a23ea" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.763003ms" resp=200
  I1009 09:46:02.600736  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="911.666µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2bd99f14-520f-41b8-99ff-486615a0d696" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="741.955µs" resp=200
  I1009 09:46:02.602490  106109 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="692.534µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="55b4eb88-2c5a-4c75-a10a-1f5d7c720b59" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="559.243µs" resp=200
  I1009 09:46:03.682076  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.346376ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7cb9fe4d-836b-4fcc-a387-6ad9936dac0e" srcIP="127.0.0.1:35884" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.010284ms" resp=200
  I1009 09:46:04.157812  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.793432ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="fd21867e-adbe-4f5b-bf44-72e103b009c2" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.49161ms" resp=200
  I1009 09:46:05.105062  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.874713ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4fcba218-9463-4cef-9391-f2a7ebfdadae" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.5022ms" resp=200
  I1009 09:46:05.111045  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod/status" latency="5.099333ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6327b2cd-7f91-48b8-933f-86efa9fb083e" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.865463ms" resp=200
  I1009 09:46:05.173086  106109 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="3.441043ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e6083a98-8489-41b5-aeff-d8bbf562212d" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.952871ms" resp=200
  I1009 09:46:06.161329  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.76349ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="1489c394-3a77-4d34-90a3-0817b88ef3e5" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.45815ms" resp=200
  I1009 09:46:07.109119  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.573551ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ce6a4dde-e8ef-4e12-af70-f53bf4e629df" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.268638ms" resp=200
  I1009 09:46:07.114110  106109 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod/status" latency="4.086708ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3de9fee6-5951-4e34-a40b-03e9450b3dbe" srcIP="127.0.0.1:36182" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.784886ms" resp=200
  I1009 09:46:08.164239  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.725462ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="80a2e21f-e9d8-4d65-ba5f-94f088cf2bf4" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.428559ms" resp=200
  I1009 09:46:08.166554  106109 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod" latency="1.418099ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="1b65c3ee-4c31-4789-aafa-a0cf8086d54f" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.261148ms" resp=200
  I1009 09:46:08.175068  106109 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/namespaces/prestop-hook-test-3521/pods/test-pod/log?container=regular-1&previous=false" latency="7.734591ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4f143e63-eb99-4104-8f83-ac2c8a8da612" srcIP="127.0.0.1:36258" resp=200
  I1009 09:46:08.179387  106109 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="2.753648ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="cdd7ac89-e545-4eb3-afe0-108d6433ea87" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.382095ms" resp=200
  I1009 09:46:08.184931  106109 httplog.go:134] "HTTP" verb="DELETE" URI="/api/v1/namespaces/prestop-hook-test-3521" latency="3.761375ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="80d1ff6b-f640-48f4-b2b1-06de22f493b7" srcIP="127.0.0.1:36258" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.507224ms" resp=200
  I1009 09:46:08.251126  106109 internal_services.go:81] Stopping e2e services...
  I1009 09:46:08.251161  106109 internal_services.go:84] Stopping namespace controller
  I1009 09:46:08.251181  106109 internal_services.go:91] Stopping API server
  I1009 09:46:08.251196  106109 internal_services.go:98] Stopping etcd
  I1009 09:46:08.251206  106109 internal_services.go:111] E2E services stopped.
  I1009 09:46:08.251298  106109 controller.go:128] Shutting down kubernetes service endpoint reconciler
  I1009 09:46:08.251319  106109 reflector.go:311] Stopping reflector *v1.Namespace (5m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 09:46:08.251379  106109 genericapiserver.go:546] "[graceful-termination] shutdown event" name="ShutdownInitiated"
  I1009 09:46:08.251391  106109 genericapiserver.go:549] "[graceful-termination] shutdown event" name="AfterShutdownDelayDuration"
  I1009 09:46:08.252205  106109 httplog.go:134] "HTTP" verb="WATCH" URI="/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=64&timeout=6m44s&timeoutSeconds=404&watch=true" latency="1m45.455165734s" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="8398693c-71e5-4187-8cae-3c2e72385017" srcIP="127.0.0.1:36158" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="285.082µs" apf_execution_time="286.332µs" resp=200
  W1009 09:46:08.252332  106109 logging.go:55] [core] [Channel #189 SubChannel #190]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:44081", ServerName: "localhost:44081", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:44081: connect: connection refused"
  W1009 09:46:08.252822  106109 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:44081", ServerName: "localhost:44081", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:44081: connect: connection refused"
  W1009 09:46:08.253075  106109 logging.go:55] [core] [Channel #177 SubChannel #178]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:44081", ServerName: "localhost:44081", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:44081: connect: connection refused"
  W1009 09:46:08.253488  106109 logging.go:55] [core] [Channel #173 SubChannel #174]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:44081", ServerName: "localhost:44081", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:44081: connect: connection refused"
  W1009 09:46:08.253693  106109 logging.go:55] [core] [Channel #125 SubChannel #126]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:44081", ServerName: "localhost:44081", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:44081: connect: connection refused"
PASS
