I1009 13:20:42.580257  137618 factory.go:193] Registered Plugin "containerd"
  I1009 13:20:42.602124  137618 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  I1009 13:20:42.603894  137618 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  I1009 13:20:42.605472  137618 mount_linux.go:334] Detected umount with safe 'not mounted' behavior
  W1009 13:20:42.632520  137618 test_context.go:538] Unable to find in-cluster config, using default host : https://127.0.0.1:6443
  I1009 13:20:42.632567  137618 test_context.go:553] Tolerating taints "node-role.kubernetes.io/control-plane" when considering if nodes are ready
  I1009 13:20:42.632578 137618 test_context.go:561] The --provider flag is not set. Continuing as if --provider=skeleton had been used.
  I1009 13:20:42.632728  137618 feature_gate.go:387] feature gates: {map[]}
  I1009 13:20:42.632799  137618 feature_gate.go:387] feature gates: {map[]}
  I1009 13:20:42.632904  137618 internal_services.go:62] Starting e2e services...
  I1009 13:20:42.632912  137618 internal_services.go:116] Starting etcd
  I1009 13:20:43.352744  137618 internal_services.go:125] Starting API server
  I1009 13:20:43.352901  137618 plugins.go:83] "Registered admission plugin" plugin="NamespaceLifecycle"
  I1009 13:20:43.352913  137618 plugins.go:83] "Registered admission plugin" plugin="ValidatingAdmissionWebhook"
  I1009 13:20:43.352924  137618 plugins.go:83] "Registered admission plugin" plugin="MutatingAdmissionWebhook"
  I1009 13:20:43.352935  137618 plugins.go:83] "Registered admission plugin" plugin="ValidatingAdmissionPolicy"
  I1009 13:20:43.352947  137618 plugins.go:83] "Registered admission plugin" plugin="AlwaysAdmit"
  I1009 13:20:43.352958  137618 plugins.go:83] "Registered admission plugin" plugin="AlwaysPullImages"
  I1009 13:20:43.352970  137618 plugins.go:83] "Registered admission plugin" plugin="LimitPodHardAntiAffinityTopology"
  I1009 13:20:43.352986  137618 plugins.go:83] "Registered admission plugin" plugin="DefaultTolerationSeconds"
  I1009 13:20:43.352997  137618 plugins.go:83] "Registered admission plugin" plugin="DefaultIngressClass"
  I1009 13:20:43.353011  137618 plugins.go:83] "Registered admission plugin" plugin="DenyServiceExternalIPs"
  I1009 13:20:43.353023  137618 plugins.go:83] "Registered admission plugin" plugin="AlwaysDeny"
  I1009 13:20:43.353033  137618 plugins.go:83] "Registered admission plugin" plugin="EventRateLimit"
  I1009 13:20:43.353041  137618 plugins.go:83] "Registered admission plugin" plugin="ExtendedResourceToleration"
  I1009 13:20:43.353049  137618 plugins.go:83] "Registered admission plugin" plugin="OwnerReferencesPermissionEnforcement"
  I1009 13:20:43.353058  137618 plugins.go:83] "Registered admission plugin" plugin="ImagePolicyWebhook"
  I1009 13:20:43.353065  137618 plugins.go:83] "Registered admission plugin" plugin="LimitRanger"
  I1009 13:20:43.353073  137618 plugins.go:83] "Registered admission plugin" plugin="NamespaceAutoProvision"
  I1009 13:20:43.353080  137618 plugins.go:83] "Registered admission plugin" plugin="NamespaceExists"
  I1009 13:20:43.353088  137618 plugins.go:83] "Registered admission plugin" plugin="NodeRestriction"
  I1009 13:20:43.353095  137618 plugins.go:83] "Registered admission plugin" plugin="TaintNodesByCondition"
  I1009 13:20:43.353103  137618 plugins.go:83] "Registered admission plugin" plugin="PodNodeSelector"
  I1009 13:20:43.353115  137618 plugins.go:83] "Registered admission plugin" plugin="PodTolerationRestriction"
  I1009 13:20:43.353132  137618 plugins.go:83] "Registered admission plugin" plugin="RuntimeClass"
  I1009 13:20:43.353144  137618 plugins.go:83] "Registered admission plugin" plugin="ResourceQuota"
  I1009 13:20:43.353156  137618 plugins.go:83] "Registered admission plugin" plugin="PodSecurity"
  I1009 13:20:43.353169  137618 plugins.go:83] "Registered admission plugin" plugin="Priority"
  I1009 13:20:43.353177  137618 plugins.go:83] "Registered admission plugin" plugin="ServiceAccount"
  I1009 13:20:43.353187  137618 plugins.go:83] "Registered admission plugin" plugin="DefaultStorageClass"
  I1009 13:20:43.353207  137618 plugins.go:83] "Registered admission plugin" plugin="PersistentVolumeClaimResize"
  I1009 13:20:43.353216  137618 plugins.go:83] "Registered admission plugin" plugin="StorageObjectInUseProtection"
  I1009 13:20:43.353226  137618 plugins.go:83] "Registered admission plugin" plugin="CertificateApproval"
  I1009 13:20:43.353236  137618 plugins.go:83] "Registered admission plugin" plugin="CertificateSigning"
  I1009 13:20:43.353251  137618 plugins.go:83] "Registered admission plugin" plugin="ClusterTrustBundleAttest"
  I1009 13:20:43.353266  137618 plugins.go:83] "Registered admission plugin" plugin="CertificateSubjectRestriction"
  I1009 13:20:43.353602  137618 util.go:48] Running readiness check for service "apiserver"
  I1009 13:20:43.353657  137618 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  W1009 13:20:43.353694  137618 registry.go:345] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
  I1009 13:20:43.353705  137618 registry.go:379] setting kube:feature gate emulation version to 1.32
  I1009 13:20:43.354091  137618 interface.go:432] Looking for default routes with IPv4 addresses
  I1009 13:20:43.354103  137618 interface.go:437] Default route transits interface "eth0"
  I1009 13:20:43.354194  137618 interface.go:209] Interface eth0 is up
  I1009 13:20:43.354232  137618 interface.go:257] Interface "eth0" has 3 addresses :[82.112.230.236/24 2a02:4780:12:eebf::1/48 fe80::be24:11ff:fe5a:b1d8/64].
  I1009 13:20:43.354244  137618 interface.go:224] Checking addr  82.112.230.236/24.
  I1009 13:20:43.354251  137618 interface.go:231] IP found 82.112.230.236
  I1009 13:20:43.354258  137618 interface.go:263] Found valid IPv4 address 82.112.230.236 for interface "eth0".
  I1009 13:20:43.354265  137618 interface.go:443] Found active IP 82.112.230.236 
  I1009 13:20:43.354299  137618 options.go:228] external host was not specified, using 82.112.230.236
  W1009 13:20:43.354307  137618 authentication.go:804] AnonymousAuth is not allowed with the AlwaysAllow authorizer. Resetting AnonymousAuth to false. You should use a different authorizer
  I1009 13:20:43.354537  137618 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  I1009 13:20:43.354569  137618 server.go:142] Version: v1.32.0-alpha.1.139+b2031b3cb46e94-dirty
  I1009 13:20:43.354583  137618 server.go:144] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
  I1009 13:20:43.354979  137618 dynamic_serving_content.go:116] "Loaded a new cert/key pair" name="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key"
  I1009 13:20:43.595428  137618 apf_controller.go:292] NewTestableController "Controller" with serverConcurrencyLimit=600, name=Controller, asFieldManager="api-priority-and-fairness-config-consumer-v1"
  I1009 13:20:43.595554  137618 apf_controller.go:992] No exempt PriorityLevelConfiguration found, imagining one
  I1009 13:20:43.595597  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","exempt"]
  I1009 13:20:43.595617  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","exempt"]
  I1009 13:20:43.595627  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["exempt"]
  I1009 13:20:43.595637  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["exempt"]
  I1009 13:20:43.595648  137618 apf_controller.go:992] No catch-all PriorityLevelConfiguration found, imagining one
  I1009 13:20:43.595666  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","catch-all"]
  I1009 13:20:43.595695  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","catch-all"]
  I1009 13:20:43.595708  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["catch-all"]
  I1009 13:20:43.595722  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["catch-all"]
  I1009 13:20:43.595736  137618 apf_controller.go:898] Introducing queues for priority level "exempt": config={"type":"Exempt","exempt":{"nominalConcurrencyShares":0,"lendablePercent":0}}, nominalCL=0, lendableCL=0, borrowingCL=600, currentCL=0, quiescing=false (shares=0xc0005375f0, shareSum=5)
  I1009 13:20:43.595831  137618 apf_controller.go:898] Introducing queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=600, lendableCL=0, borrowingCL=600, currentCL=600, quiescing=false (shares=0xc000385448, shareSum=5)
  I1009 13:20:43.595857  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=0 concurrencyDenominator=60 backstop=false
  I1009 13:20:43.595882  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=600 concurrencyDenominator=600 backstop=false
  I1009 13:20:43.595895  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["waiting","readOnly"]
  I1009 13:20:43.595904  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["waiting","mutating"]
  I1009 13:20:43.595912  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","readOnly"]
  I1009 13:20:43.595920  137618 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","mutating"]
  I1009 13:20:43.609891  137618 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
  I1009 13:20:43.612318  137618 plugins.go:157] Loaded 9 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
  I1009 13:20:43.612346  137618 plugins.go:160] Loaded 12 validating admission controller(s) successfully in the following order: LimitRanger,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
  I1009 13:20:43.612526  137618 options.go:305] Setting service IP to "10.0.0.1" (read-write).
  I1009 13:20:43.612546  137618 instance.go:232] Using reconciler: lease
  I1009 13:20:43.612677  137618 storage_factory.go:270] storing apiServerIPInfo in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.615370  137618 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 13:20:43.616821  137618 reflector.go:341] Listing and watching *apiextensions.CustomResourceDefinition from storage/cacher.go:/apiextensions.k8s.io/customresourcedefinitions
  I1009 13:20:43.617905  137618 cacher.go:463] cacher (customresourcedefinitions.apiextensions.k8s.io): initialized
  I1009 13:20:43.617942  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.617955  137618 reflector.go:368] Caches populated for *apiextensions.CustomResourceDefinition from storage/cacher.go:/apiextensions.k8s.io/customresourcedefinitions
  I1009 13:20:43.618949  137618 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 13:20:43.619211  137618 etcd.go:419] "Using watch cache" resource="customresourcedefinitions.apiextensions.k8s.io"
  I1009 13:20:43.619274  137618 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
  W1009 13:20:43.619287  137618 genericapiserver.go:765] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:43.622672  137618 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.624699  137618 deleted_kinds.go:60] NewResourceExpirationEvaluator with currentVersion: 1.32.
  I1009 13:20:43.624900  137618 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.624939  137618 etcd.go:416] "Not using watch cache" resource="events"
  I1009 13:20:43.625932  137618 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.626002  137618 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 13:20:43.626946  137618 reflector.go:341] Listing and watching *core.ResourceQuota from storage/cacher.go:/resourcequotas
  I1009 13:20:43.626942  137618 storage_factory.go:270] storing secrets in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.627076  137618 etcd.go:419] "Using watch cache" resource="secrets"
  I1009 13:20:43.627442  137618 cacher.go:463] cacher (resourcequotas): initialized
  I1009 13:20:43.627479  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.627488  137618 reflector.go:368] Caches populated for *core.ResourceQuota from storage/cacher.go:/resourcequotas
  I1009 13:20:43.627895  137618 reflector.go:341] Listing and watching *core.Secret from storage/cacher.go:/secrets
  I1009 13:20:43.627944  137618 storage_factory.go:270] storing configmaps in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.627971  137618 etcd.go:419] "Using watch cache" resource="configmaps"
  I1009 13:20:43.628367  137618 cacher.go:463] cacher (secrets): initialized
  I1009 13:20:43.628384  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.628394  137618 reflector.go:368] Caches populated for *core.Secret from storage/cacher.go:/secrets
  I1009 13:20:43.628975  137618 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.628997  137618 reflector.go:341] Listing and watching *core.ConfigMap from storage/cacher.go:/configmaps
  I1009 13:20:43.629012  137618 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 13:20:43.629504  137618 cacher.go:463] cacher (configmaps): initialized
  I1009 13:20:43.629540  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.629551  137618 reflector.go:368] Caches populated for *core.ConfigMap from storage/cacher.go:/configmaps
  I1009 13:20:43.629813  137618 reflector.go:341] Listing and watching *core.Namespace from storage/cacher.go:/namespaces
  I1009 13:20:43.629817  137618 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.629847  137618 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 13:20:43.630234  137618 cacher.go:463] cacher (namespaces): initialized
  I1009 13:20:43.630253  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.630261  137618 reflector.go:368] Caches populated for *core.Namespace from storage/cacher.go:/namespaces
  I1009 13:20:43.630690  137618 storage_factory.go:270] storing podtemplates in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.630711  137618 reflector.go:341] Listing and watching *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 13:20:43.630730  137618 etcd.go:419] "Using watch cache" resource="podtemplates"
  I1009 13:20:43.631162  137618 cacher.go:463] cacher (serviceaccounts): initialized
  I1009 13:20:43.631178  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.631185  137618 reflector.go:368] Caches populated for *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 13:20:43.631573  137618 storage_factory.go:270] storing limitranges in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.631607  137618 reflector.go:341] Listing and watching *core.PodTemplate from storage/cacher.go:/podtemplates
  I1009 13:20:43.631625  137618 etcd.go:419] "Using watch cache" resource="limitranges"
  I1009 13:20:43.632078  137618 cacher.go:463] cacher (podtemplates): initialized
  I1009 13:20:43.632093  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.632103  137618 reflector.go:368] Caches populated for *core.PodTemplate from storage/cacher.go:/podtemplates
  I1009 13:20:43.632546  137618 reflector.go:341] Listing and watching *core.LimitRange from storage/cacher.go:/limitranges
  I1009 13:20:43.632574  137618 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.632606  137618 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 13:20:43.633018  137618 cacher.go:463] cacher (limitranges): initialized
  I1009 13:20:43.633035  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.633043  137618 reflector.go:368] Caches populated for *core.LimitRange from storage/cacher.go:/limitranges
  I1009 13:20:43.633488  137618 reflector.go:341] Listing and watching *core.PersistentVolume from storage/cacher.go:/persistentvolumes
  I1009 13:20:43.633577  137618 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.633607  137618 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 13:20:43.633923  137618 cacher.go:463] cacher (persistentvolumes): initialized
  I1009 13:20:43.633938  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.633952  137618 reflector.go:368] Caches populated for *core.PersistentVolume from storage/cacher.go:/persistentvolumes
  I1009 13:20:43.634539  137618 reflector.go:341] Listing and watching *core.PersistentVolumeClaim from storage/cacher.go:/persistentvolumeclaims
  I1009 13:20:43.634534  137618 storage_factory.go:270] storing endpoints in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.634608  137618 etcd.go:419] "Using watch cache" resource="endpoints"
  I1009 13:20:43.634875  137618 cacher.go:463] cacher (persistentvolumeclaims): initialized
  I1009 13:20:43.634888  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.634895  137618 reflector.go:368] Caches populated for *core.PersistentVolumeClaim from storage/cacher.go:/persistentvolumeclaims
  I1009 13:20:43.635470  137618 reflector.go:341] Listing and watching *core.Endpoints from storage/cacher.go:/services/endpoints
  I1009 13:20:43.635549  137618 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.635593  137618 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 13:20:43.635908  137618 cacher.go:463] cacher (endpoints): initialized
  I1009 13:20:43.635922  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.635929  137618 reflector.go:368] Caches populated for *core.Endpoints from storage/cacher.go:/services/endpoints
  I1009 13:20:43.636625  137618 reflector.go:341] Listing and watching *core.Node from storage/cacher.go:/minions
  I1009 13:20:43.636731  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.636798  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.637096  137618 cacher.go:463] cacher (nodes): initialized
  I1009 13:20:43.637108  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.637116  137618 reflector.go:368] Caches populated for *core.Node from storage/cacher.go:/minions
  I1009 13:20:43.637808  137618 reflector.go:341] Listing and watching *core.Pod from storage/cacher.go:/pods
  I1009 13:20:43.637810  137618 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.637870  137618 etcd.go:419] "Using watch cache" resource="services"
  I1009 13:20:43.638279  137618 cacher.go:463] cacher (pods): initialized
  I1009 13:20:43.638298  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.638308  137618 reflector.go:368] Caches populated for *core.Pod from storage/cacher.go:/pods
  I1009 13:20:43.639119  137618 reflector.go:341] Listing and watching *core.Service from storage/cacher.go:/services/specs
  I1009 13:20:43.639163  137618 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.639218  137618 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 13:20:43.639517  137618 cacher.go:463] cacher (services): initialized
  I1009 13:20:43.639539  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.639547  137618 reflector.go:368] Caches populated for *core.Service from storage/cacher.go:/services/specs
  I1009 13:20:43.640285  137618 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.640327  137618 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 13:20:43.640394  137618 reflector.go:341] Listing and watching *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 13:20:43.640855  137618 cacher.go:463] cacher (serviceaccounts): initialized
  I1009 13:20:43.640880  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.640897  137618 reflector.go:368] Caches populated for *core.ServiceAccount from storage/cacher.go:/serviceaccounts
  I1009 13:20:43.641302  137618 reflector.go:341] Listing and watching *core.ReplicationController from storage/cacher.go:/controllers
  I1009 13:20:43.641684  137618 cacher.go:463] cacher (replicationcontrollers): initialized
  I1009 13:20:43.641707  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.641716  137618 reflector.go:368] Caches populated for *core.ReplicationController from storage/cacher.go:/controllers
  I1009 13:20:43.641864  137618 apis.go:118] Enabling API group "".
  I1009 13:20:43.648575  137618 storage_factory.go:270] storing bindings in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.648636  137618 etcd.go:419] "Using watch cache" resource="bindings"
  I1009 13:20:43.648766  137618 storage_factory.go:270] storing componentstatuses in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.648809  137618 etcd.go:419] "Using watch cache" resource="componentstatuses"
  I1009 13:20:43.649182  137618 storage_factory.go:270] storing configmaps in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.649211  137618 etcd.go:419] "Using watch cache" resource="configmaps"
  I1009 13:20:43.649593  137618 storage_factory.go:270] storing endpoints in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.649623  137618 etcd.go:419] "Using watch cache" resource="endpoints"
  I1009 13:20:43.650077  137618 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.650130  137618 etcd.go:416] "Not using watch cache" resource="events"
  I1009 13:20:43.654934  137618 storage_factory.go:270] storing limitranges in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.654983  137618 etcd.go:419] "Using watch cache" resource="limitranges"
  I1009 13:20:43.655168  137618 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.655216  137618 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 13:20:43.655306  137618 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.655349  137618 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 13:20:43.655482  137618 storage_factory.go:270] storing namespaces in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.655518  137618 etcd.go:419] "Using watch cache" resource="namespaces"
  I1009 13:20:43.655738  137618 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.655772  137618 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 13:20:43.656150  137618 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.656184  137618 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 13:20:43.656271  137618 storage_factory.go:270] storing nodes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.656295  137618 etcd.go:419] "Using watch cache" resource="nodes"
  I1009 13:20:43.656764  137618 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.656796  137618 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 13:20:43.656917  137618 storage_factory.go:270] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.656942  137618 etcd.go:419] "Using watch cache" resource="persistentvolumeclaims"
  I1009 13:20:43.657165  137618 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.657184  137618 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 13:20:43.657275  137618 storage_factory.go:270] storing persistentvolumes in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.657298  137618 etcd.go:419] "Using watch cache" resource="persistentvolumes"
  I1009 13:20:43.657639  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.657670  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.657769  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.657788  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.657868  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.657886  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.657984  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658002  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658053  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658074  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658158  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658176  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658231  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658247  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658311  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658328  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658707  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658738  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.658851  137618 storage_factory.go:270] storing pods in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.658878  137618 etcd.go:419] "Using watch cache" resource="pods"
  I1009 13:20:43.659139  137618 storage_factory.go:270] storing podtemplates in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.659159  137618 etcd.go:419] "Using watch cache" resource="podtemplates"
  I1009 13:20:43.659439  137618 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.659509  137618 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 13:20:43.659670  137618 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.659708  137618 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 13:20:43.659882  137618 storage_factory.go:270] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.659910  137618 etcd.go:419] "Using watch cache" resource="replicationcontrollers"
  I1009 13:20:43.660261  137618 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.660295  137618 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 13:20:43.660397  137618 storage_factory.go:270] storing resourcequotas in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.660415  137618 etcd.go:419] "Using watch cache" resource="resourcequotas"
  I1009 13:20:43.660748  137618 storage_factory.go:270] storing secrets in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.660790  137618 etcd.go:419] "Using watch cache" resource="secrets"
  I1009 13:20:43.661212  137618 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.661252  137618 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 13:20:43.661328  137618 storage_factory.go:270] storing serviceaccounts in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.661353  137618 etcd.go:419] "Using watch cache" resource="serviceaccounts"
  I1009 13:20:43.661634  137618 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.661774  137618 etcd.go:419] "Using watch cache" resource="services"
  I1009 13:20:43.662054  137618 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.662075  137618 etcd.go:419] "Using watch cache" resource="services"
  I1009 13:20:43.662174  137618 storage_factory.go:270] storing services in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.662195  137618 etcd.go:419] "Using watch cache" resource="services"
  I1009 13:20:43.662269  137618 handler.go:286] Adding GroupVersion  v1 to ResourceManager
  I1009 13:20:43.662501  137618 apis.go:105] API group "internal.apiserver.k8s.io" is not enabled, skipping.
  I1009 13:20:43.662553  137618 apis.go:118] Enabling API group "authentication.k8s.io".
  I1009 13:20:43.662604  137618 apis.go:118] Enabling API group "authorization.k8s.io".
  I1009 13:20:43.662824  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.662849  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.664301  137618 reflector.go:341] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 13:20:43.664377  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.664434  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.665055  137618 cacher.go:463] cacher (horizontalpodautoscalers.autoscaling): initialized
  I1009 13:20:43.665080  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.665091  137618 reflector.go:368] Caches populated for *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 13:20:43.665659  137618 apis.go:118] Enabling API group "autoscaling".
  I1009 13:20:43.665716  137618 reflector.go:341] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 13:20:43.665825  137618 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.665864  137618 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 13:20:43.666066  137618 cacher.go:463] cacher (horizontalpodautoscalers.autoscaling): initialized
  I1009 13:20:43.666084  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.666094  137618 reflector.go:368] Caches populated for *autoscaling.HorizontalPodAutoscaler from storage/cacher.go:/horizontalpodautoscalers
  I1009 13:20:43.677125  137618 reflector.go:341] Listing and watching *batch.Job from storage/cacher.go:/jobs
  I1009 13:20:43.677169  137618 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.677227  137618 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 13:20:43.677884  137618 cacher.go:463] cacher (jobs.batch): initialized
  I1009 13:20:43.677911  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.677921  137618 reflector.go:368] Caches populated for *batch.Job from storage/cacher.go:/jobs
  I1009 13:20:43.678586  137618 apis.go:118] Enabling API group "batch".
  I1009 13:20:43.678622  137618 reflector.go:341] Listing and watching *batch.CronJob from storage/cacher.go:/cronjobs
  I1009 13:20:43.678910  137618 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.678971  137618 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 13:20:43.679128  137618 cacher.go:463] cacher (cronjobs.batch): initialized
  I1009 13:20:43.679151  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.679161  137618 reflector.go:368] Caches populated for *batch.CronJob from storage/cacher.go:/cronjobs
  I1009 13:20:43.680177  137618 apis.go:118] Enabling API group "certificates.k8s.io".
  I1009 13:20:43.680288  137618 reflector.go:341] Listing and watching *certificates.CertificateSigningRequest from storage/cacher.go:/certificatesigningrequests
  I1009 13:20:43.680441  137618 storage_factory.go:270] storing leases.coordination.k8s.io in coordination.k8s.io/v1, reading as coordination.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.680519  137618 etcd.go:419] "Using watch cache" resource="leases.coordination.k8s.io"
  I1009 13:20:43.680730  137618 cacher.go:463] cacher (certificatesigningrequests.certificates.k8s.io): initialized
  I1009 13:20:43.680745  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.680772  137618 reflector.go:368] Caches populated for *certificates.CertificateSigningRequest from storage/cacher.go:/certificatesigningrequests
  I1009 13:20:43.681736  137618 apis.go:118] Enabling API group "coordination.k8s.io".
  I1009 13:20:43.681868  137618 reflector.go:341] Listing and watching *coordination.Lease from storage/cacher.go:/leases
  I1009 13:20:43.682010  137618 storage_factory.go:270] storing endpointslices.discovery.k8s.io in discovery.k8s.io/v1, reading as discovery.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.682072  137618 etcd.go:419] "Using watch cache" resource="endpointslices.discovery.k8s.io"
  I1009 13:20:43.682349  137618 cacher.go:463] cacher (leases.coordination.k8s.io): initialized
  I1009 13:20:43.682367  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.682374  137618 reflector.go:368] Caches populated for *coordination.Lease from storage/cacher.go:/leases
  I1009 13:20:43.683080  137618 apis.go:118] Enabling API group "discovery.k8s.io".
  I1009 13:20:43.683102  137618 reflector.go:341] Listing and watching *discovery.EndpointSlice from storage/cacher.go:/endpointslices
  I1009 13:20:43.683332  137618 storage_factory.go:270] storing networkpolicies.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.683389  137618 etcd.go:419] "Using watch cache" resource="networkpolicies.networking.k8s.io"
  I1009 13:20:43.684049  137618 cacher.go:463] cacher (endpointslices.discovery.k8s.io): initialized
  I1009 13:20:43.684100  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.684113  137618 reflector.go:368] Caches populated for *discovery.EndpointSlice from storage/cacher.go:/endpointslices
  I1009 13:20:43.684488  137618 reflector.go:341] Listing and watching *networking.NetworkPolicy from storage/cacher.go:/networkpolicies
  I1009 13:20:43.684675  137618 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.684795  137618 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 13:20:43.685015  137618 cacher.go:463] cacher (networkpolicies.networking.k8s.io): initialized
  I1009 13:20:43.685031  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.685038  137618 reflector.go:368] Caches populated for *networking.NetworkPolicy from storage/cacher.go:/networkpolicies
  I1009 13:20:43.685966  137618 reflector.go:341] Listing and watching *networking.Ingress from storage/cacher.go:/ingress
  I1009 13:20:43.686028  137618 storage_factory.go:270] storing ingressclasses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.686083  137618 etcd.go:419] "Using watch cache" resource="ingressclasses.networking.k8s.io"
  I1009 13:20:43.686720  137618 cacher.go:463] cacher (ingresses.networking.k8s.io): initialized
  I1009 13:20:43.686787  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.686800  137618 reflector.go:368] Caches populated for *networking.Ingress from storage/cacher.go:/ingress
  I1009 13:20:43.687152  137618 apis.go:118] Enabling API group "networking.k8s.io".
  I1009 13:20:43.687350  137618 reflector.go:341] Listing and watching *networking.IngressClass from storage/cacher.go:/ingressclasses
  I1009 13:20:43.687364  137618 storage_factory.go:270] storing runtimeclasses.node.k8s.io in node.k8s.io/v1, reading as node.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.687398  137618 etcd.go:419] "Using watch cache" resource="runtimeclasses.node.k8s.io"
  I1009 13:20:43.687819  137618 cacher.go:463] cacher (ingressclasses.networking.k8s.io): initialized
  I1009 13:20:43.687839  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.687847  137618 reflector.go:368] Caches populated for *networking.IngressClass from storage/cacher.go:/ingressclasses
  I1009 13:20:43.688427  137618 apis.go:118] Enabling API group "node.k8s.io".
  I1009 13:20:43.688556  137618 reflector.go:341] Listing and watching *node.RuntimeClass from storage/cacher.go:/runtimeclasses
  I1009 13:20:43.688823  137618 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.688883  137618 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 13:20:43.688928  137618 cacher.go:463] cacher (runtimeclasses.node.k8s.io): initialized
  I1009 13:20:43.688941  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.688948  137618 reflector.go:368] Caches populated for *node.RuntimeClass from storage/cacher.go:/runtimeclasses
  I1009 13:20:43.689667  137618 apis.go:118] Enabling API group "policy".
  I1009 13:20:43.689796  137618 reflector.go:341] Listing and watching *policy.PodDisruptionBudget from storage/cacher.go:/poddisruptionbudgets
  I1009 13:20:43.689811  137618 storage_factory.go:270] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.689843  137618 etcd.go:419] "Using watch cache" resource="roles.rbac.authorization.k8s.io"
  I1009 13:20:43.690194  137618 cacher.go:463] cacher (poddisruptionbudgets.policy): initialized
  I1009 13:20:43.690211  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.690221  137618 reflector.go:368] Caches populated for *policy.PodDisruptionBudget from storage/cacher.go:/poddisruptionbudgets
  I1009 13:20:43.690741  137618 reflector.go:341] Listing and watching *rbac.Role from storage/cacher.go:/roles
  I1009 13:20:43.690806  137618 storage_factory.go:270] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.690849  137618 etcd.go:419] "Using watch cache" resource="rolebindings.rbac.authorization.k8s.io"
  I1009 13:20:43.691248  137618 cacher.go:463] cacher (roles.rbac.authorization.k8s.io): initialized
  I1009 13:20:43.691277  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.691284  137618 reflector.go:368] Caches populated for *rbac.Role from storage/cacher.go:/roles
  I1009 13:20:43.691679  137618 storage_factory.go:270] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.691713  137618 reflector.go:341] Listing and watching *rbac.RoleBinding from storage/cacher.go:/rolebindings
  I1009 13:20:43.691725  137618 etcd.go:419] "Using watch cache" resource="clusterroles.rbac.authorization.k8s.io"
  I1009 13:20:43.692136  137618 cacher.go:463] cacher (rolebindings.rbac.authorization.k8s.io): initialized
  I1009 13:20:43.692150  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.692158  137618 reflector.go:368] Caches populated for *rbac.RoleBinding from storage/cacher.go:/rolebindings
  I1009 13:20:43.692477  137618 reflector.go:341] Listing and watching *rbac.ClusterRole from storage/cacher.go:/clusterroles
  I1009 13:20:43.692573  137618 storage_factory.go:270] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.692608  137618 etcd.go:419] "Using watch cache" resource="clusterrolebindings.rbac.authorization.k8s.io"
  I1009 13:20:43.692901  137618 cacher.go:463] cacher (clusterroles.rbac.authorization.k8s.io): initialized
  I1009 13:20:43.692920  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.692928  137618 reflector.go:368] Caches populated for *rbac.ClusterRole from storage/cacher.go:/clusterroles
  I1009 13:20:43.693442  137618 apis.go:118] Enabling API group "rbac.authorization.k8s.io".
  I1009 13:20:43.693473  137618 reflector.go:341] Listing and watching *rbac.ClusterRoleBinding from storage/cacher.go:/clusterrolebindings
  I1009 13:20:43.693931  137618 cacher.go:463] cacher (clusterrolebindings.rbac.authorization.k8s.io): initialized
  I1009 13:20:43.693950  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.693960  137618 reflector.go:368] Caches populated for *rbac.ClusterRoleBinding from storage/cacher.go:/clusterrolebindings
  I1009 13:20:43.694447  137618 hooks.go:96] skipping "rbac/bootstrap-roles" because it was explicitly disabled
  I1009 13:20:43.694687  137618 storage_factory.go:270] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io/v1, reading as scheduling.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.694720  137618 etcd.go:419] "Using watch cache" resource="priorityclasses.scheduling.k8s.io"
  I1009 13:20:43.695504  137618 apis.go:118] Enabling API group "scheduling.k8s.io".
  I1009 13:20:43.695688  137618 reflector.go:341] Listing and watching *scheduling.PriorityClass from storage/cacher.go:/priorityclasses
  I1009 13:20:43.695729  137618 storage_factory.go:270] storing storageclasses.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.695798  137618 etcd.go:419] "Using watch cache" resource="storageclasses.storage.k8s.io"
  I1009 13:20:43.696133  137618 cacher.go:463] cacher (priorityclasses.scheduling.k8s.io): initialized
  I1009 13:20:43.696147  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.696154  137618 reflector.go:368] Caches populated for *scheduling.PriorityClass from storage/cacher.go:/priorityclasses
  I1009 13:20:43.696745  137618 reflector.go:341] Listing and watching *storage.StorageClass from storage/cacher.go:/storageclasses
  I1009 13:20:43.696864  137618 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.696906  137618 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 13:20:43.697230  137618 cacher.go:463] cacher (storageclasses.storage.k8s.io): initialized
  I1009 13:20:43.697246  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.697254  137618 reflector.go:368] Caches populated for *storage.StorageClass from storage/cacher.go:/storageclasses
  I1009 13:20:43.697850  137618 reflector.go:341] Listing and watching *storage.VolumeAttachment from storage/cacher.go:/volumeattachments
  I1009 13:20:43.697924  137618 storage_factory.go:270] storing csinodes.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.697952  137618 etcd.go:419] "Using watch cache" resource="csinodes.storage.k8s.io"
  I1009 13:20:43.698276  137618 cacher.go:463] cacher (volumeattachments.storage.k8s.io): initialized
  I1009 13:20:43.698290  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.698297  137618 reflector.go:368] Caches populated for *storage.VolumeAttachment from storage/cacher.go:/volumeattachments
  I1009 13:20:43.698773  137618 reflector.go:341] Listing and watching *storage.CSINode from storage/cacher.go:/csinodes
  I1009 13:20:43.698823  137618 storage_factory.go:270] storing csidrivers.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.698850  137618 etcd.go:419] "Using watch cache" resource="csidrivers.storage.k8s.io"
  I1009 13:20:43.699235  137618 cacher.go:463] cacher (csinodes.storage.k8s.io): initialized
  I1009 13:20:43.699248  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.699254  137618 reflector.go:368] Caches populated for *storage.CSINode from storage/cacher.go:/csinodes
  I1009 13:20:43.699817  137618 reflector.go:341] Listing and watching *storage.CSIDriver from storage/cacher.go:/csidrivers
  I1009 13:20:43.699824  137618 storage_factory.go:270] storing csistoragecapacities.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.699862  137618 etcd.go:419] "Using watch cache" resource="csistoragecapacities.storage.k8s.io"
  I1009 13:20:43.700220  137618 cacher.go:463] cacher (csidrivers.storage.k8s.io): initialized
  I1009 13:20:43.700235  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.700244  137618 reflector.go:368] Caches populated for *storage.CSIDriver from storage/cacher.go:/csidrivers
  I1009 13:20:43.701037  137618 apis.go:118] Enabling API group "storage.k8s.io".
  I1009 13:20:43.701057  137618 reflector.go:341] Listing and watching *storage.CSIStorageCapacity from storage/cacher.go:/csistoragecapacities
  I1009 13:20:43.701058  137618 apis.go:105] API group "storagemigration.k8s.io" is not enabled, skipping.
  I1009 13:20:43.701280  137618 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.701308  137618 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.701426  137618 cacher.go:463] cacher (csistoragecapacities.storage.k8s.io): initialized
  I1009 13:20:43.701444  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.701482  137618 reflector.go:368] Caches populated for *storage.CSIStorageCapacity from storage/cacher.go:/csistoragecapacities
  I1009 13:20:43.702360  137618 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.702380  137618 reflector.go:341] Listing and watching *flowcontrol.FlowSchema from storage/cacher.go:/flowschemas
  I1009 13:20:43.702387  137618 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.702801  137618 cacher.go:463] cacher (flowschemas.flowcontrol.apiserver.k8s.io): initialized
  I1009 13:20:43.702822  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.702832  137618 reflector.go:368] Caches populated for *flowcontrol.FlowSchema from storage/cacher.go:/flowschemas
  I1009 13:20:43.703321  137618 apis.go:118] Enabling API group "flowcontrol.apiserver.k8s.io".
  I1009 13:20:43.703410  137618 reflector.go:341] Listing and watching *flowcontrol.PriorityLevelConfiguration from storage/cacher.go:/prioritylevelconfigurations
  I1009 13:20:43.703532  137618 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.703562  137618 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 13:20:43.703942  137618 cacher.go:463] cacher (prioritylevelconfigurations.flowcontrol.apiserver.k8s.io): initialized
  I1009 13:20:43.703969  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.703980  137618 reflector.go:368] Caches populated for *flowcontrol.PriorityLevelConfiguration from storage/cacher.go:/prioritylevelconfigurations
  I1009 13:20:43.704438  137618 reflector.go:341] Listing and watching *apps.Deployment from storage/cacher.go:/deployments
  I1009 13:20:43.704513  137618 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.704549  137618 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 13:20:43.704881  137618 cacher.go:463] cacher (deployments.apps): initialized
  I1009 13:20:43.704899  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.704907  137618 reflector.go:368] Caches populated for *apps.Deployment from storage/cacher.go:/deployments
  I1009 13:20:43.705436  137618 reflector.go:341] Listing and watching *apps.StatefulSet from storage/cacher.go:/statefulsets
  I1009 13:20:43.705449  137618 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.705498  137618 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 13:20:43.705893  137618 cacher.go:463] cacher (statefulsets.apps): initialized
  I1009 13:20:43.705911  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.705921  137618 reflector.go:368] Caches populated for *apps.StatefulSet from storage/cacher.go:/statefulsets
  I1009 13:20:43.706426  137618 reflector.go:341] Listing and watching *apps.DaemonSet from storage/cacher.go:/daemonsets
  I1009 13:20:43.706537  137618 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.706579  137618 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 13:20:43.706839  137618 cacher.go:463] cacher (daemonsets.apps): initialized
  I1009 13:20:43.706856  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.706864  137618 reflector.go:368] Caches populated for *apps.DaemonSet from storage/cacher.go:/daemonsets
  I1009 13:20:43.707486  137618 reflector.go:341] Listing and watching *apps.ReplicaSet from storage/cacher.go:/replicasets
  I1009 13:20:43.707548  137618 storage_factory.go:270] storing controllerrevisions.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.707577  137618 etcd.go:419] "Using watch cache" resource="controllerrevisions.apps"
  I1009 13:20:43.707904  137618 cacher.go:463] cacher (replicasets.apps): initialized
  I1009 13:20:43.707917  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.707924  137618 reflector.go:368] Caches populated for *apps.ReplicaSet from storage/cacher.go:/replicasets
  I1009 13:20:43.708527  137618 apis.go:118] Enabling API group "apps".
  I1009 13:20:43.708552  137618 reflector.go:341] Listing and watching *apps.ControllerRevision from storage/cacher.go:/controllerrevisions
  I1009 13:20:43.708722  137618 storage_factory.go:270] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.708802  137618 etcd.go:419] "Using watch cache" resource="validatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 13:20:43.709082  137618 cacher.go:463] cacher (controllerrevisions.apps): initialized
  I1009 13:20:43.709097  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.709105  137618 reflector.go:368] Caches populated for *apps.ControllerRevision from storage/cacher.go:/controllerrevisions
  I1009 13:20:43.709681  137618 reflector.go:341] Listing and watching *admissionregistration.ValidatingWebhookConfiguration from storage/cacher.go:/validatingwebhookconfigurations
  I1009 13:20:43.709684  137618 storage_factory.go:270] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.709725  137618 etcd.go:419] "Using watch cache" resource="mutatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 13:20:43.710081  137618 cacher.go:463] cacher (validatingwebhookconfigurations.admissionregistration.k8s.io): initialized
  I1009 13:20:43.710109  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.710117  137618 reflector.go:368] Caches populated for *admissionregistration.ValidatingWebhookConfiguration from storage/cacher.go:/validatingwebhookconfigurations
  I1009 13:20:43.710767  137618 reflector.go:341] Listing and watching *admissionregistration.MutatingWebhookConfiguration from storage/cacher.go:/mutatingwebhookconfigurations
  I1009 13:20:43.710809  137618 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.710837  137618 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 13:20:43.711297  137618 cacher.go:463] cacher (mutatingwebhookconfigurations.admissionregistration.k8s.io): initialized
  I1009 13:20:43.711314  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.711324  137618 reflector.go:368] Caches populated for *admissionregistration.MutatingWebhookConfiguration from storage/cacher.go:/mutatingwebhookconfigurations
  I1009 13:20:43.711939  137618 reflector.go:341] Listing and watching *admissionregistration.ValidatingAdmissionPolicy from storage/cacher.go:/validatingadmissionpolicies
  I1009 13:20:43.712002  137618 storage_factory.go:270] storing validatingadmissionpolicybindings.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.712043  137618 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicybindings.admissionregistration.k8s.io"
  I1009 13:20:43.712482  137618 cacher.go:463] cacher (validatingadmissionpolicies.admissionregistration.k8s.io): initialized
  I1009 13:20:43.712507  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.712514  137618 reflector.go:368] Caches populated for *admissionregistration.ValidatingAdmissionPolicy from storage/cacher.go:/validatingadmissionpolicies
  I1009 13:20:43.712935  137618 apis.go:118] Enabling API group "admissionregistration.k8s.io".
  I1009 13:20:43.713028  137618 reflector.go:341] Listing and watching *admissionregistration.ValidatingAdmissionPolicyBinding from storage/cacher.go:/validatingadmissionpolicybindings
  I1009 13:20:43.713577  137618 storage_factory.go:270] storing events in v1, reading as __internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.713615  137618 etcd.go:416] "Not using watch cache" resource="events"
  I1009 13:20:43.714410  137618 cacher.go:463] cacher (validatingadmissionpolicybindings.admissionregistration.k8s.io): initialized
  I1009 13:20:43.714434  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.714444  137618 reflector.go:368] Caches populated for *admissionregistration.ValidatingAdmissionPolicyBinding from storage/cacher.go:/validatingadmissionpolicybindings
  I1009 13:20:43.714720  137618 apis.go:118] Enabling API group "events.k8s.io".
  I1009 13:20:43.714737  137618 apis.go:105] API group "resource.k8s.io" is not enabled, skipping.
  I1009 13:20:43.728328  137618 storage_factory.go:270] storing selfsubjectreviews.authentication.k8s.io in authentication.k8s.io/v1, reading as authentication.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728406  137618 etcd.go:419] "Using watch cache" resource="selfsubjectreviews.authentication.k8s.io"
  I1009 13:20:43.728484  137618 storage_factory.go:270] storing tokenreviews.authentication.k8s.io in authentication.k8s.io/v1, reading as authentication.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728517  137618 etcd.go:419] "Using watch cache" resource="tokenreviews.authentication.k8s.io"
  I1009 13:20:43.728564  137618 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
  W1009 13:20:43.728571  137618 genericapiserver.go:765] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.728577  137618 genericapiserver.go:765] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.728684  137618 storage_factory.go:270] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728707  137618 etcd.go:419] "Using watch cache" resource="localsubjectaccessreviews.authorization.k8s.io"
  I1009 13:20:43.728800  137618 storage_factory.go:270] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728828  137618 etcd.go:419] "Using watch cache" resource="selfsubjectaccessreviews.authorization.k8s.io"
  I1009 13:20:43.728885  137618 storage_factory.go:270] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728903  137618 etcd.go:419] "Using watch cache" resource="selfsubjectrulesreviews.authorization.k8s.io"
  I1009 13:20:43.728949  137618 storage_factory.go:270] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io/v1, reading as authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.728970  137618 etcd.go:419] "Using watch cache" resource="subjectaccessreviews.authorization.k8s.io"
  I1009 13:20:43.728996  137618 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
  W1009 13:20:43.729002  137618 genericapiserver.go:765] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:43.729385  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.729413  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.729570  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.729600  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.729626  137618 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
  I1009 13:20:43.729988  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.730015  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.730125  137618 storage_factory.go:270] storing horizontalpodautoscalers.autoscaling in autoscaling/v2, reading as autoscaling/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.730144  137618 etcd.go:419] "Using watch cache" resource="horizontalpodautoscalers.autoscaling"
  I1009 13:20:43.730169  137618 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
  W1009 13:20:43.730175  137618 genericapiserver.go:765] Skipping API autoscaling/v2beta1 because it has no resources.
  W1009 13:20:43.730181  137618 genericapiserver.go:765] Skipping API autoscaling/v2beta2 because it has no resources.
  I1009 13:20:43.730582  137618 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.730613  137618 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 13:20:43.730712  137618 storage_factory.go:270] storing cronjobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.730732  137618 etcd.go:419] "Using watch cache" resource="cronjobs.batch"
  I1009 13:20:43.731037  137618 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.731066  137618 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 13:20:43.731210  137618 storage_factory.go:270] storing jobs.batch in batch/v1, reading as batch/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.731246  137618 etcd.go:419] "Using watch cache" resource="jobs.batch"
  I1009 13:20:43.731281  137618 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
  W1009 13:20:43.731288  137618 genericapiserver.go:765] Skipping API batch/v1beta1 because it has no resources.
  I1009 13:20:43.731567  137618 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.731807  137618 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 13:20:43.731974  137618 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.732002  137618 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 13:20:43.732100  137618 storage_factory.go:270] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io/v1, reading as certificates.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.732119  137618 etcd.go:419] "Using watch cache" resource="certificatesigningrequests.certificates.k8s.io"
  I1009 13:20:43.732162  137618 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
  W1009 13:20:43.732169  137618 genericapiserver.go:765] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.732187  137618 genericapiserver.go:765] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.732575  137618 storage_factory.go:270] storing leases.coordination.k8s.io in coordination.k8s.io/v1, reading as coordination.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.732608  137618 etcd.go:419] "Using watch cache" resource="leases.coordination.k8s.io"
  I1009 13:20:43.732636  137618 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
  W1009 13:20:43.732642  137618 genericapiserver.go:765] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.732648  137618 genericapiserver.go:765] Skipping API coordination.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.732947  137618 storage_factory.go:270] storing endpointslices.discovery.k8s.io in discovery.k8s.io/v1, reading as discovery.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.732970  137618 etcd.go:419] "Using watch cache" resource="endpointslices.discovery.k8s.io"
  I1009 13:20:43.732994  137618 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
  W1009 13:20:43.733002  137618 genericapiserver.go:765] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:43.733247  137618 storage_factory.go:270] storing ingressclasses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.733267  137618 etcd.go:419] "Using watch cache" resource="ingressclasses.networking.k8s.io"
  I1009 13:20:43.733558  137618 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.733608  137618 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 13:20:43.733763  137618 storage_factory.go:270] storing ingresses.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.733787  137618 etcd.go:419] "Using watch cache" resource="ingresses.networking.k8s.io"
  I1009 13:20:43.734133  137618 storage_factory.go:270] storing networkpolicies.networking.k8s.io in networking.k8s.io/v1, reading as networking.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.734158  137618 etcd.go:419] "Using watch cache" resource="networkpolicies.networking.k8s.io"
  I1009 13:20:43.734193  137618 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
  W1009 13:20:43.734199  137618 genericapiserver.go:765] Skipping API networking.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.734205  137618 genericapiserver.go:765] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.734524  137618 storage_factory.go:270] storing runtimeclasses.node.k8s.io in node.k8s.io/v1, reading as node.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.734562  137618 etcd.go:419] "Using watch cache" resource="runtimeclasses.node.k8s.io"
  I1009 13:20:43.734612  137618 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
  W1009 13:20:43.734620  137618 genericapiserver.go:765] Skipping API node.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.734626  137618 genericapiserver.go:765] Skipping API node.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.735015  137618 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.735047  137618 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 13:20:43.735145  137618 storage_factory.go:270] storing poddisruptionbudgets.policy in policy/v1, reading as policy/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.735164  137618 etcd.go:419] "Using watch cache" resource="poddisruptionbudgets.policy"
  I1009 13:20:43.735193  137618 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
  W1009 13:20:43.735199  137618 genericapiserver.go:765] Skipping API policy/v1beta1 because it has no resources.
  I1009 13:20:43.735532  137618 storage_factory.go:270] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.735564  137618 etcd.go:419] "Using watch cache" resource="clusterrolebindings.rbac.authorization.k8s.io"
  I1009 13:20:43.735787  137618 storage_factory.go:270] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.735809  137618 etcd.go:419] "Using watch cache" resource="clusterroles.rbac.authorization.k8s.io"
  I1009 13:20:43.736103  137618 storage_factory.go:270] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.736129  137618 etcd.go:419] "Using watch cache" resource="rolebindings.rbac.authorization.k8s.io"
  I1009 13:20:43.736484  137618 storage_factory.go:270] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io/v1, reading as rbac.authorization.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.736515  137618 etcd.go:419] "Using watch cache" resource="roles.rbac.authorization.k8s.io"
  I1009 13:20:43.736546  137618 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
  W1009 13:20:43.736555  137618 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.736565  137618 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.736859  137618 storage_factory.go:270] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io/v1, reading as scheduling.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.736893  137618 etcd.go:419] "Using watch cache" resource="priorityclasses.scheduling.k8s.io"
  I1009 13:20:43.736932  137618 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
  W1009 13:20:43.736940  137618 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.736945  137618 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.737229  137618 storage_factory.go:270] storing csidrivers.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.737270  137618 etcd.go:419] "Using watch cache" resource="csidrivers.storage.k8s.io"
  I1009 13:20:43.737561  137618 storage_factory.go:270] storing csinodes.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.737592  137618 etcd.go:419] "Using watch cache" resource="csinodes.storage.k8s.io"
  I1009 13:20:43.738062  137618 storage_factory.go:270] storing csistoragecapacities.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.738096  137618 etcd.go:419] "Using watch cache" resource="csistoragecapacities.storage.k8s.io"
  I1009 13:20:43.738398  137618 storage_factory.go:270] storing storageclasses.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.738427  137618 etcd.go:419] "Using watch cache" resource="storageclasses.storage.k8s.io"
  I1009 13:20:43.738709  137618 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.738748  137618 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 13:20:43.738883  137618 storage_factory.go:270] storing volumeattachments.storage.k8s.io in storage.k8s.io/v1, reading as storage.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.738911  137618 etcd.go:419] "Using watch cache" resource="volumeattachments.storage.k8s.io"
  I1009 13:20:43.738953  137618 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
  W1009 13:20:43.738960  137618 genericapiserver.go:765] Skipping API storage.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.738965  137618 genericapiserver.go:765] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.739329  137618 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.739368  137618 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.739588  137618 storage_factory.go:270] storing flowschemas.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.739626  137618 etcd.go:419] "Using watch cache" resource="flowschemas.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.739985  137618 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.740021  137618 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.740193  137618 storage_factory.go:270] storing prioritylevelconfigurations.flowcontrol.apiserver.k8s.io in flowcontrol.apiserver.k8s.io/v1, reading as flowcontrol.apiserver.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.740214  137618 etcd.go:419] "Using watch cache" resource="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io"
  I1009 13:20:43.740247  137618 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
  W1009 13:20:43.740254  137618 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
  W1009 13:20:43.740259  137618 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
  W1009 13:20:43.740264  137618 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:43.740683  137618 storage_factory.go:270] storing controllerrevisions.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.740710  137618 etcd.go:419] "Using watch cache" resource="controllerrevisions.apps"
  I1009 13:20:43.741119  137618 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.741147  137618 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 13:20:43.741285  137618 storage_factory.go:270] storing daemonsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.741313  137618 etcd.go:419] "Using watch cache" resource="daemonsets.apps"
  I1009 13:20:43.741701  137618 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.741728  137618 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 13:20:43.741875  137618 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.741895  137618 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 13:20:43.742014  137618 storage_factory.go:270] storing deployments.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.742032  137618 etcd.go:419] "Using watch cache" resource="deployments.apps"
  I1009 13:20:43.743222  137618 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.743254  137618 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 13:20:43.743374  137618 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.743401  137618 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 13:20:43.743535  137618 storage_factory.go:270] storing replicasets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.743582  137618 etcd.go:419] "Using watch cache" resource="replicasets.apps"
  I1009 13:20:43.744104  137618 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.744152  137618 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 13:20:43.744296  137618 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.744319  137618 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 13:20:43.744477  137618 storage_factory.go:270] storing statefulsets.apps in apps/v1, reading as apps/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.744504  137618 etcd.go:419] "Using watch cache" resource="statefulsets.apps"
  I1009 13:20:43.744541  137618 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
  W1009 13:20:43.744553  137618 genericapiserver.go:765] Skipping API apps/v1beta2 because it has no resources.
  W1009 13:20:43.744558  137618 genericapiserver.go:765] Skipping API apps/v1beta1 because it has no resources.
  I1009 13:20:43.745026  137618 storage_factory.go:270] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.745061  137618 etcd.go:419] "Using watch cache" resource="mutatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 13:20:43.745389  137618 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.745412  137618 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 13:20:43.745585  137618 storage_factory.go:270] storing validatingadmissionpolicies.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.745612  137618 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicies.admissionregistration.k8s.io"
  I1009 13:20:43.745934  137618 storage_factory.go:270] storing validatingadmissionpolicybindings.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.745963  137618 etcd.go:419] "Using watch cache" resource="validatingadmissionpolicybindings.admissionregistration.k8s.io"
  I1009 13:20:43.746329  137618 storage_factory.go:270] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io/v1, reading as admissionregistration.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.746366  137618 etcd.go:419] "Using watch cache" resource="validatingwebhookconfigurations.admissionregistration.k8s.io"
  I1009 13:20:43.746407  137618 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
  W1009 13:20:43.746414  137618 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
  W1009 13:20:43.746420  137618 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
  I1009 13:20:43.746876  137618 storage_factory.go:270] storing events.events.k8s.io in v1, reading as events.k8s.io/__internal from storagebackend.Config{Type:"etcd3", Prefix:"/registry", Transport:storagebackend.TransportConfig{ServerList:[]string{"http://localhost:39285"}, KeyFile:"", CertFile:"", TrustedCAFile:"", EgressLookup:(egressselector.Lookup)(nil), TracerProvider:(*tracing.noopTracerProvider)(0xc0009613f0)}, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:0, CountMetricPollPeriod:0, DBMetricPollInterval:0, HealthcheckTimeout:0, ReadycheckTimeout:0, LeaseManagerConfig:etcd3.LeaseManagerConfig{ReuseDurationSeconds:0, MaxObjectCount:0}, StorageObjectCountTracker:(*request.objectCountTracker)(0xc000bb0ff0)}
  I1009 13:20:43.746908  137618 etcd.go:416] "Not using watch cache" resource="events.events.k8s.io"
  I1009 13:20:43.746950  137618 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
  W1009 13:20:43.746957  137618 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:43.747795  137618 deleted_kinds.go:60] NewResourceExpirationEvaluator with currentVersion: 1.32.
  I1009 13:20:43.747850  137618 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 13:20:43.755322  137618 reflector.go:341] Listing and watching *apiregistration.APIService from storage/cacher.go:/apiregistration.k8s.io/apiservices
  I1009 13:20:43.756087  137618 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 13:20:43.756102  137618 cacher.go:463] cacher (apiservices.apiregistration.k8s.io): initialized
  I1009 13:20:43.756214  137618 watch_cache.go:614] Replaced watchCache (rev: 1) 
  I1009 13:20:43.756224  137618 reflector.go:368] Caches populated for *apiregistration.APIService from storage/cacher.go:/apiregistration.k8s.io/apiservices
  I1009 13:20:43.756278  137618 etcd.go:419] "Using watch cache" resource="apiservices.apiregistration.k8s.io"
  I1009 13:20:43.756337  137618 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
  W1009 13:20:43.756346  137618 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
  I1009 13:20:44.040291  137618 tlsconfig.go:203] "Loaded serving cert" certName="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key" certDetail="\"82.112.230.236@1728428978\" [serving] validServingFor=[82.112.230.236,10.0.0.1,127.0.0.1,kubernetes.default.svc,kubernetes.default,kubernetes] issuer=\"82.112.230.236-ca@1728428978\" (2024-10-08 22:09:38 +0000 UTC to 2025-10-08 22:09:38 +0000 UTC (now=2024-10-09 13:20:44.040256186 +0000 UTC))"
  I1009 13:20:44.040422  137618 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1728480043\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1728480043\" (2024-10-09 12:20:43 +0000 UTC to 2025-10-09 12:20:43 +0000 UTC (now=2024-10-09 13:20:44.040413217 +0000 UTC))"
  I1009 13:20:44.040294  137618 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/run/kubernetes/apiserver.crt::/var/run/kubernetes/apiserver.key"
  I1009 13:20:44.040468  137618 secure_serving.go:213] Serving securely on 127.0.0.1:6443
  I1009 13:20:44.040604  137618 genericapiserver.go:683] [graceful-termination] waiting for shutdown to be initiated
  I1009 13:20:44.040483  137618 tlsconfig.go:243] "Starting DynamicServingCertificateController"
  I1009 13:20:44.040828  137618 reflector.go:305] Starting reflector *v1.ValidatingAdmissionPolicyBinding (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.040849  137618 reflector.go:341] Listing and watching *v1.ValidatingAdmissionPolicyBinding from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.040895  137618 apiservice_controller.go:100] Starting APIServiceRegistrationController
  I1009 13:20:44.040912  137618 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
  I1009 13:20:44.041125  137618 aggregator.go:169] waiting for initial CRD sync...
  I1009 13:20:44.041163  137618 reflector.go:305] Starting reflector *v1.CustomResourceDefinition (5m0s) from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.041172  137618 reflector.go:341] Listing and watching *v1.CustomResourceDefinition from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.041311  137618 apf_controller.go:377] Starting API Priority and Fairness config controller
  I1009 13:20:44.041426  137618 controller.go:119] Starting legacy_token_tracking_controller
  I1009 13:20:44.041436  137618 shared_informer.go:313] Waiting for caches to sync for configmaps
  I1009 13:20:44.041477  137618 remote_available_controller.go:411] Starting RemoteAvailability controller
  I1009 13:20:44.041490  137618 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
  I1009 13:20:44.041506  137618 controller.go:80] Starting OpenAPI V3 AggregationController
  I1009 13:20:44.041561  137618 cluster_authentication_trust_controller.go:454] Starting cluster_authentication_trust_controller controller
  I1009 13:20:44.041570  137618 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
  I1009 13:20:44.041604  137618 gc_controller.go:78] Starting apiserver lease garbage collector
  I1009 13:20:44.041682  137618 customresource_discovery_controller.go:292] Starting DiscoveryController
  I1009 13:20:44.041704  137618 system_namespaces_controller.go:66] Starting system namespaces controller
  I1009 13:20:44.041737  137618 reflector.go:305] Starting reflector *v1.APIService (30s) from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.041746  137618 reflector.go:341] Listing and watching *v1.APIService from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.041913  137618 reflector.go:305] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.041924  137618 reflector.go:341] Listing and watching *v1.ServiceAccount from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042058  137618 reflector.go:305] Starting reflector *v1.Pod (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042068  137618 reflector.go:341] Listing and watching *v1.Pod from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042208  137618 reflector.go:305] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042218  137618 reflector.go:341] Listing and watching *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042370  137618 reflector.go:305] Starting reflector *v1.LimitRange (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042382  137618 reflector.go:341] Listing and watching *v1.LimitRange from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042395  137618 controller.go:78] Starting OpenAPI AggregationController
  I1009 13:20:44.042531  137618 reflector.go:305] Starting reflector *v1.MutatingWebhookConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042546  137618 reflector.go:341] Listing and watching *v1.MutatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042675  137618 reflector.go:305] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042684  137618 reflector.go:341] Listing and watching *v1.Secret from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042810  137618 reflector.go:305] Starting reflector *v1.IngressClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042820  137618 reflector.go:341] Listing and watching *v1.IngressClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042959  137618 reflector.go:305] Starting reflector *v1.ValidatingWebhookConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.042970  137618 reflector.go:341] Listing and watching *v1.ValidatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043105  137618 reflector.go:305] Starting reflector *v1.Endpoints (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043116  137618 reflector.go:341] Listing and watching *v1.Endpoints from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043235  137618 reflector.go:305] Starting reflector *v1.PriorityLevelConfiguration (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043245  137618 reflector.go:341] Listing and watching *v1.PriorityLevelConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043473  137618 reflector.go:305] Starting reflector *v1.FlowSchema (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043488  137618 reflector.go:341] Listing and watching *v1.FlowSchema from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043637  137618 reflector.go:305] Starting reflector *v1.Node (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043647  137618 reflector.go:341] Listing and watching *v1.Node from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043780  137618 reflector.go:305] Starting reflector *v1.StorageClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043790  137618 reflector.go:341] Listing and watching *v1.StorageClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043865  137618 reflector.go:305] Starting reflector *v1.ValidatingAdmissionPolicy (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043874  137618 reflector.go:341] Listing and watching *v1.ValidatingAdmissionPolicy from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043963  137618 reflector.go:305] Starting reflector *v1.ResourceQuota (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.043972  137618 reflector.go:341] Listing and watching *v1.ResourceQuota from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044049  137618 reflector.go:305] Starting reflector *v1.Service (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044058  137618 reflector.go:341] Listing and watching *v1.Service from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044126  137618 reflector.go:305] Starting reflector *v1.PriorityClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044142  137618 reflector.go:341] Listing and watching *v1.PriorityClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044227  137618 reflector.go:305] Starting reflector *v1.RuntimeClass (10m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044236  137618 reflector.go:341] Listing and watching *v1.RuntimeClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.044307  137618 crdregistration_controller.go:114] Starting crd-autoregister controller
  I1009 13:20:44.044315  137618 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
  I1009 13:20:44.044629  137618 reflector.go:305] Starting reflector *v1.ConfigMap (12h0m0s) from runtime/asm_amd64.s:1700
  I1009 13:20:44.044644  137618 reflector.go:341] Listing and watching *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 13:20:44.044780  137618 reflector.go:305] Starting reflector *v1.ConfigMap (12h0m0s) from runtime/asm_amd64.s:1700
  I1009 13:20:44.044789  137618 reflector.go:341] Listing and watching *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 13:20:44.044863  137618 reflector.go:305] Starting reflector *v1.Lease (0s) from runtime/asm_amd64.s:1700
  I1009 13:20:44.044872  137618 reflector.go:341] Listing and watching *v1.Lease from runtime/asm_amd64.s:1700
  I1009 13:20:44.044959  137618 controller.go:142] Starting OpenAPI controller
  I1009 13:20:44.044979  137618 controller.go:90] Starting OpenAPI V3 controller
  I1009 13:20:44.044994  137618 naming_controller.go:294] Starting NamingConditionController
  I1009 13:20:44.045010  137618 establishing_controller.go:81] Starting EstablishingController
  I1009 13:20:44.045026  137618 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
  I1009 13:20:44.045041  137618 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
  I1009 13:20:44.045055  137618 crd_finalizer.go:269] Starting CRDFinalizer
  I1009 13:20:44.042382  137618 local_available_controller.go:156] Starting LocalAvailability controller
  I1009 13:20:44.045113  137618 cache.go:32] Waiting for caches to sync for LocalAvailability controller
  I1009 13:20:44.048826  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicies?limit=500&resourceVersion=0" latency="237.971µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d8596292-7b24-4b9d-a911-fd84fdea023e" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="163.841µs" resp=200
  I1009 13:20:44.049227  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/resourcequotas?limit=500&resourceVersion=0" latency="264.242µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="558e4b55-f56a-4abd-95e3-722957b03dc8" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="189.041µs" resp=200
  I1009 13:20:44.049567  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services?limit=500&resourceVersion=0" latency="252.18µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="219b3c61-94f5-4746-8088-6d47c4246a3a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="178.47µs" resp=200
  I1009 13:20:44.049896  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/scheduling.k8s.io/v1/priorityclasses?limit=500&resourceVersion=0" latency="240.68µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fd06385e-e1e1-4be9-868d-e531677be4b7" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="171.301µs" resp=200
  I1009 13:20:44.050193  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0" latency="218.961µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="87c89df7-c691-4562-addc-6a1b226ad204" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="153.131µs" resp=200
  I1009 13:20:44.050710  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-system/configmaps?limit=500&resourceVersion=0" latency="166.661µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7b338a88-6534-4320-8835-b23ad988e43b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="102.141µs" resp=200
  I1009 13:20:44.051078  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases?labelSelector=apiserver.kubernetes.io%2Fidentity%3Dkube-apiserver&limit=500&resourceVersion=0" latency="270.381µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="58065b23-565f-45ef-80c4-2556c5526a97" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="161.721µs" resp=200
  I1009 13:20:44.051366  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0" latency="206.191µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="591b1164-216f-44d4-ba49-2c949f1e916d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="138.8µs" resp=200
  I1009 13:20:44.051653  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicybindings?limit=500&resourceVersion=0" latency="212.662µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d6a6f4bd-01d1-49e7-86cf-5caa4669520c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="124.131µs" resp=200
  I1009 13:20:44.052194  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/apiextensions.k8s.io/v1/customresourcedefinitions?limit=500&resourceVersion=0" latency="229.201µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="acea20d6-8cf0-43b7-bcc5-4ca58a1795e7" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="160.85µs" resp=200
  I1009 13:20:44.052450  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/apiregistration.k8s.io/v1/apiservices?limit=500&resourceVersion=0" latency="175.422µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5865b558-a01b-4a0a-b199-0468a4a216ca" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="106.311µs" resp=200
  I1009 13:20:44.052785  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/serviceaccounts?limit=500&resourceVersion=0" latency="225.75µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d0fa72e5-ab2e-4ee4-b4fa-9657a887ff55" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="155.5µs" resp=200
  I1009 13:20:44.053108  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/pods?limit=500&resourceVersion=0" latency="242.08µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="00d553e4-06f6-4cb7-808b-48016ff5097c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="171.161µs" resp=200
  I1009 13:20:44.053410  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces?limit=500&resourceVersion=0" latency="223.991µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a40b98ac-30ae-4332-b187-4bf48a3832ea" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="156.391µs" resp=200
  I1009 13:20:44.053715  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/limitranges?limit=500&resourceVersion=0" latency="201.661µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="57ed9a64-7b74-47cd-ac1a-a4d0ccb7b415" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="132.021µs" resp=200
  I1009 13:20:44.054034  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations?limit=500&resourceVersion=0" latency="227.09µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f65c348a-e6f5-47c4-a38a-42d9fe8817f4" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="158.65µs" resp=200
  I1009 13:20:44.054319  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/secrets?limit=500&resourceVersion=0" latency="206.252µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="81c6c1a7-0a3d-4a90-b804-4d14833812ef" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="135.341µs" resp=200
  I1009 13:20:44.054336  137618 reflector.go:368] Caches populated for *v1.PriorityClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.054402  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/networking.k8s.io/v1/ingressclasses?limit=500&resourceVersion=0" latency="249.541µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1f9c7f34-ccf6-4de4-82f5-3f62ccb75775" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="167.791µs" resp=200
  I1009 13:20:44.054405  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-apiserver-legacy-service-account-token-tracking&limit=500&resourceVersion=0" latency="4.121625ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a0e1dda2-ccba-409e-9639-eaadd6b85b87" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="168.631µs" resp=200
  I1009 13:20:44.054598  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.862377ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dcedfd43-cea0-40c3-885a-a7359ab38258" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.791356ms" resp=404
  I1009 13:20:44.054717  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations?limit=500&resourceVersion=0" latency="202.372µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="10dd8252-2c07-4215-ab17-c0b19d0aa93e" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="128.651µs" resp=200
  I1009 13:20:44.055012  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/endpoints?limit=500&resourceVersion=0" latency="206.631µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fd2abb09-e32a-453a-a777-d84831c78b2e" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="139.34µs" resp=200
  I1009 13:20:44.055347  137618 get.go:278] "Starting watch" path="/apis/scheduling.k8s.io/v1/priorityclasses" resourceVersion="1" labels="" fields="" timeout="9m31s"
  I1009 13:20:44.055495  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?limit=500&resourceVersion=0" latency="201.692µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7e7f46c6-a9dc-40c6-942a-223a24b198ef" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="115.33µs" resp=200
  I1009 13:20:44.055614  137618 reflector.go:368] Caches populated for *v1.RuntimeClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.055643  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes?limit=500&resourceVersion=0" latency="7.620465ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="08c23cc9-42ef-4ca2-8066-82bdaae38699" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="374.323µs" resp=200
  I1009 13:20:44.055796  137618 reflector.go:368] Caches populated for *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 13:20:44.055945  137618 reflector.go:368] Caches populated for *v1.Lease from runtime/asm_amd64.s:1700
  I1009 13:20:44.056055  137618 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.056180  137618 reflector.go:368] Caches populated for *v1.ValidatingAdmissionPolicyBinding from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.056313  137618 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.056408  137618 reflector.go:368] Caches populated for *v1.APIService from pkg/client/informers/externalversions/factory.go:141
  I1009 13:20:44.056556  137618 healthz.go:278] poststarthook/start-apiextensions-controllers,poststarthook/crd-informer-synced,poststarthook/start-service-ip-repair-controllers,poststarthook/scheduling/bootstrap-system-priority-classes,poststarthook/priority-and-fairness-config-producer,poststarthook/bootstrap-controller,poststarthook/apiservice-registration-controller,poststarthook/apiservice-discovery-controller,autoregister-completion check failed: healthz
  [-]poststarthook/start-apiextensions-controllers failed: not finished
  [-]poststarthook/crd-informer-synced failed: not finished
  [-]poststarthook/start-service-ip-repair-controllers failed: not finished
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  [-]poststarthook/priority-and-fairness-config-producer failed: not finished
  [-]poststarthook/bootstrap-controller failed: not finished
  [-]poststarthook/apiservice-registration-controller failed: not finished
  [-]poststarthook/apiservice-discovery-controller failed: not finished
  [-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v2.autoscaling]
  I1009 13:20:44.056628  137618 reflector.go:368] Caches populated for *v1.Pod from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.056671  137618 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="11.090604ms" userAgent="Go-http-client/2.0" audit-ID="d7a63eff-35fe-4a5c-8697-32ec72d4526c" srcIP="127.0.0.1:48506" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="10.829153ms" resp=500
  I1009 13:20:44.056780  137618 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.056910  137618 reflector.go:368] Caches populated for *v1.LimitRange from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.056984  137618 get.go:278] "Starting watch" path="/apis/node.k8s.io/v1/runtimeclasses" resourceVersion="1" labels="" fields="" timeout="6m19s"
  I1009 13:20:44.057118  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?limit=500&resourceVersion=0" latency="2.025992ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ba0e37f3-d7af-41e8-a1ff-f7d7a2b40a01" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="130.881µs" resp=200
  I1009 13:20:44.057235  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services" latency="3.583761ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="04ac674f-32ed-4a42-8be2-8337ee9f76dc" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.420708ms" resp=200
  I1009 13:20:44.057259  137618 get.go:278] "Starting watch" path="/api/v1/namespaces/kube-system/configmaps" resourceVersion="1" labels="" fields="" timeout="8m4s"
  I1009 13:20:44.057720  137618 get.go:278] "Starting watch" path="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" resourceVersion="1" labels="apiserver.kubernetes.io/identity=kube-apiserver" fields="" timeout="6m39s"
  I1009 13:20:44.057986  137618 get.go:278] "Starting watch" path="/apis/storage.k8s.io/v1/storageclasses" resourceVersion="1" labels="" fields="" timeout="9m46s"
  I1009 13:20:44.056991  137618 reflector.go:368] Caches populated for *v1.MutatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057015  137618 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057042  137618 reflector.go:368] Caches populated for *v1.IngressClass from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.058202  137618 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicybindings" resourceVersion="1" labels="" fields="" timeout="7m58s"
  I1009 13:20:44.058329  137618 get.go:278] "Starting watch" path="/apis/apiextensions.k8s.io/v1/customresourcedefinitions" resourceVersion="1" labels="" fields="" timeout="9m15s"
  I1009 13:20:44.057064  137618 reflector.go:368] Caches populated for *v1.ConfigMap from runtime/asm_amd64.s:1700
  I1009 13:20:44.058431  137618 reflector.go:368] Caches populated for *v1.PriorityLevelConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057828  137618 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057859  137618 reflector.go:368] Caches populated for *v1.ValidatingAdmissionPolicy from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.058587  137618 get.go:278] "Starting watch" path="/apis/apiregistration.k8s.io/v1/apiservices" resourceVersion="1" labels="" fields="" timeout="6m53s"
  I1009 13:20:44.057892  137618 reflector.go:368] Caches populated for *v1.ResourceQuota from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057915  137618 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057943  137618 reflector.go:368] Caches populated for *v1.ValidatingWebhookConfiguration from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.057982  137618 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.058005  137618 reflector.go:368] Caches populated for *v1.FlowSchema from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.058865  137618 get.go:278] "Starting watch" path="/api/v1/pods" resourceVersion="1" labels="" fields="" timeout="9m37s"
  I1009 13:20:44.056587  137618 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:44.059041  137618 get.go:278] "Starting watch" path="/api/v1/namespaces" resourceVersion="1" labels="" fields="" timeout="6m20s"
  I1009 13:20:44.059339  137618 get.go:278] "Starting watch" path="/api/v1/serviceaccounts" resourceVersion="1" labels="" fields="" timeout="6m34s"
  I1009 13:20:44.059473  137618 get.go:278] "Starting watch" path="/api/v1/limitranges" resourceVersion="1" labels="" fields="" timeout="8m27s"
  I1009 13:20:44.059762  137618 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingadmissionpolicies" resourceVersion="1" labels="" fields="" timeout="9m31s"
  I1009 13:20:44.059775  137618 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/validatingwebhookconfigurations" resourceVersion="1" labels="" fields="" timeout="8m15s"
  I1009 13:20:44.059974  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services" latency="824.955µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7e5ff5d4-d01f-4949-b9a8-c3d1d5272883" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="748.724µs" resp=200
  I1009 13:20:44.059989  137618 get.go:278] "Starting watch" path="/api/v1/resourcequotas" resourceVersion="1" labels="" fields="" timeout="8m50s"
  I1009 13:20:44.060061  137618 get.go:278] "Starting watch" path="/apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations" resourceVersion="1" labels="" fields="" timeout="6m22s"
  I1009 13:20:44.060121  137618 get.go:278] "Starting watch" path="/api/v1/endpoints" resourceVersion="1" labels="" fields="" timeout="9m58s"
  I1009 13:20:44.060157  137618 get.go:278] "Starting watch" path="/api/v1/services" resourceVersion="1" labels="" fields="" timeout="7m23s"
  I1009 13:20:44.060166  137618 get.go:278] "Starting watch" path="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas" resourceVersion="1" labels="" fields="" timeout="7m46s"
  I1009 13:20:44.060228  137618 get.go:278] "Starting watch" path="/api/v1/secrets" resourceVersion="1" labels="" fields="" timeout="5m5s"
  I1009 13:20:44.060361  137618 get.go:278] "Starting watch" path="/api/v1/nodes" resourceVersion="1" labels="" fields="" timeout="9m37s"
  I1009 13:20:44.060390  137618 get.go:278] "Starting watch" path="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations" resourceVersion="1" labels="" fields="" timeout="6m1s"
  I1009 13:20:44.060440  137618 get.go:278] "Starting watch" path="/apis/networking.k8s.io/v1/ingressclasses" resourceVersion="1" labels="" fields="" timeout="8m46s"
  I1009 13:20:44.060596  137618 get.go:278] "Starting watch" path="/api/v1/namespaces/kube-system/configmaps" resourceVersion="1" labels="" fields="metadata.name=kube-apiserver-legacy-service-account-token-tracking" timeout="9m46s"
  I1009 13:20:44.110709  137618 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
  I1009 13:20:44.110783  137618 policy_source.go:240] refreshing policies
  I1009 13:20:44.112237  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/kube-system" latency="1.159017ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="eb013e3d-b09e-4d65-ab15-34111b92b34b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="948.086µs" resp=404
  I1009 13:20:44.112689  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" latency="53.661015ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7f581cb3-d4a9-4bb7-a058-4cbb01bce8ad" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="53.472904ms" resp=404
  I1009 13:20:44.141388  137618 cache.go:39] Caches are synced for APIServiceRegistrationController controller
  I1009 13:20:44.141441  137618 apf_controller.go:382] Running API Priority and Fairness config worker
  I1009 13:20:44.141478  137618 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
  I1009 13:20:44.141481  137618 handler_discovery.go:451] Starting ResourceDiscoveryManager
  I1009 13:20:44.141484  137618 genericapiserver.go:533] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
  I1009 13:20:44.141494  137618 shared_informer.go:320] Caches are synced for configmaps
  I1009 13:20:44.141555  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=9 seatDemandAvg=0.16929659576394746 seatDemandStdev=0.5744838097086102 seatDemandSmoothed=0.7437804054725576 fairFrac=0 currentCL=9 concurrencyDenominator=9 backstop=false
  I1009 13:20:44.141627  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=0 currentCL=600 concurrencyDenominator=600 backstop=false
  I1009 13:20:44.141693  137618 cache.go:39] Caches are synced for RemoteAvailability controller
  I1009 13:20:44.141917  137618 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
  I1009 13:20:44.142868  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes,poststarthook/priority-and-fairness-config-producer,autoregister-completion check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  [-]poststarthook/priority-and-fairness-config-producer failed: not finished
  [-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v2.autoscaling]
  I1009 13:20:44.142990  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="727.814µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ab5fb8d9-60c9-46aa-8fe7-baea25634299" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="624.044µs" resp=500
  I1009 13:20:44.143054  137618 controller.go:615] quota admission added evaluator for: namespaces
  I1009 13:20:44.143521  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.78794ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="016a805c-fd31-4286-a279-66d8af554a58" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.59725ms" resp=201
  I1009 13:20:44.143767  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="system"
  I1009 13:20:44.143818  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","system"]
  I1009 13:20:44.143839  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","system"]
  I1009 13:20:44.143855  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["system"]
  I1009 13:20:44.143870  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["system"]
  I1009 13:20:44.143884  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.143894  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.143912  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=86, lendableCL=0, borrowingCL=600, currentCL=600, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=35)
  I1009 13:20:44.143944  137618 apf_controller.go:898] Introducing queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=515, lendableCL=170, borrowingCL=600, currentCL=430, quiescing=false (shares=0xc0008a0a18, shareSum=35)
  I1009 13:20:44.143973  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.394431554524362 currentCL=481 concurrencyDenominator=481 backstop=false
  I1009 13:20:44.144009  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=2.635977201847555 seatDemandStdev=1.3839913006527662 seatDemandSmoothed=4.019968502500321 fairFrac=1.394431554524362 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 13:20:44.144026  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.394431554524362 currentCL=120 concurrencyDenominator=120 backstop=false
  I1009 13:20:44.144294  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-system/resourcequotas" latency="973.726µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8ee5afd2-8200-4ce8-bfd0-62cd96665be9" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="887.985µs" resp=200
  I1009 13:20:44.144368  137618 shared_informer.go:320] Caches are synced for crd-autoregister
  I1009 13:20:44.144397  137618 aggregator.go:171] initial CRD sync complete...
  I1009 13:20:44.144414  137618 autoregister_controller.go:144] Starting autoregister controller
  I1009 13:20:44.144423  137618 cache.go:32] Waiting for caches to sync for autoregister controller
  I1009 13:20:44.144434  137618 cache.go:39] Caches are synced for autoregister controller
  I1009 13:20:44.145039  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.045616ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1994f895-68ab-4a8b-bc45-a56165723c05" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="945.025µs" resp=201
  I1009 13:20:44.145133  137618 cache.go:39] Caches are synced for LocalAvailability controller
  I1009 13:20:44.145348  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="3.160038ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e9f38c11-5539-4406-a29f-0fc5fcb23ce4" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.055417ms" resp=201
  I1009 13:20:44.145984  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","node-high"]
  I1009 13:20:44.146007  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","node-high"]
  I1009 13:20:44.146018  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["node-high"]
  I1009 13:20:44.146029  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["node-high"]
  I1009 13:20:44.146044  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.146062  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.146068  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="846.395µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6efd7baf-072e-4602-aa7f-0585c46b149d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="771.464µs" resp=201
  I1009 13:20:44.146102  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="node-high"
  I1009 13:20:44.146215  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.346068ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="589fb37f-09ca-4b5d-b8ef-c1dbe94c801d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.190297ms" resp=201
  I1009 13:20:44.146243  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.058286ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3ae9494b-ed1e-4375-a2bf-de7863d2eead" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="982.145µs" resp=201
  I1009 13:20:44.146073  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=40, lendableCL=0, borrowingCL=600, currentCL=120, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=75)
  I1009 13:20:44.146308  137618 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=240, lendableCL=79, borrowingCL=600, currentCL=481, quiescing=false, numPending=0 (shares=0xc0008a0a18, shareSum=75)
  I1009 13:20:44.146328  137618 apf_controller.go:898] Introducing queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=320, lendableCL=80, borrowingCL=600, currentCL=280, quiescing=false (shares=0xc0011e0f48, shareSum=75)
  I1009 13:20:44.146351  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=219 concurrencyDenominator=219 backstop=false
  I1009 13:20:44.146378  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=327 concurrencyDenominator=327 backstop=false
  I1009 13:20:44.146376  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.538ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="7411d223-9a40-4ec6-aa56-aea02bbe779b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.460899ms" resp=201
  I1009 13:20:44.146399  137618 remote_available_controller.go:456] Adding v1.
  I1009 13:20:44.146412  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=7 seatDemandAvg=4.582099854057774 seatDemandStdev=1.3320623764309205 seatDemandSmoothed=5.914162230488695 fairFrac=1.3605442176870748 currentCL=7 concurrencyDenominator=7 backstop=false
  I1009 13:20:44.146421  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.538849ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ff30ae3d-b9c7-4226-a383-5a3514135106" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.455088ms" resp=201
  I1009 13:20:44.146439  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3605442176870748 currentCL=54 concurrencyDenominator=54 backstop=false
  I1009 13:20:44.146431  137618 apiservice_controller.go:173] Adding v1.
  I1009 13:20:44.146482  137618 local_available_controller.go:201] Adding v1.
  I1009 13:20:44.146815  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 1 objects queued in incoming channel.
  I1009 13:20:44.146830  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 2 objects queued in incoming channel.
  I1009 13:20:44.146839  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 3 objects queued in incoming channel.
  I1009 13:20:44.146848  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 4 objects queued in incoming channel.
  I1009 13:20:44.146856  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 5 objects queued in incoming channel.
  I1009 13:20:44.146865  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 6 objects queued in incoming channel.
  I1009 13:20:44.147383  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.080476ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="de0a98c2-af76-4634-a3aa-2d2a6d7679ed" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="871.575µs" resp=201
  I1009 13:20:44.147554  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="825.635µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="cb7caf1c-f6d4-4713-878e-b6adef69e903" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="737.005µs" resp=201
  I1009 13:20:44.147665  137618 local_available_controller.go:201] Adding v1.apps
  I1009 13:20:44.147679  137618 local_available_controller.go:201] Adding v1.authentication.k8s.io
  I1009 13:20:44.147713  137618 remote_available_controller.go:456] Adding v1.apps
  I1009 13:20:44.147726  137618 apiservice_controller.go:173] Adding v1.apps
  I1009 13:20:44.147732  137618 apiservice_controller.go:173] Adding v1.authentication.k8s.io
  I1009 13:20:44.147740  137618 apiservice_controller.go:173] Adding v1.apiextensions.k8s.io
  I1009 13:20:44.147747  137618 local_available_controller.go:201] Adding v1.apiextensions.k8s.io
  I1009 13:20:44.147834  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","leader-election"]
  I1009 13:20:44.147851  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","leader-election"]
  I1009 13:20:44.147861  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["leader-election"]
  I1009 13:20:44.147871  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["leader-election"]
  I1009 13:20:44.147893  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.147914  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.147931  137618 apf_controller.go:898] Introducing queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=71, lendableCL=0, borrowingCL=600, currentCL=71, quiescing=false (shares=0xc0016e2e90, shareSum=85)
  I1009 13:20:44.147947  137618 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=212, lendableCL=70, borrowingCL=600, currentCL=219, quiescing=false, numPending=0 (shares=0xc0008a0a18, shareSum=85)
  I1009 13:20:44.147959  137618 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=283, lendableCL=71, borrowingCL=600, currentCL=327, quiescing=false, numPending=0 (shares=0xc0011e0f48, shareSum=85)
  I1009 13:20:44.147967  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.084696ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="408678a0-e88a-4e1e-a8b2-56e4ba2afa27" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.028786ms" resp=201
  I1009 13:20:44.147976  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=36, lendableCL=0, borrowingCL=600, currentCL=54, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=85)
  I1009 13:20:44.148039  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=93 concurrencyDenominator=93 backstop=false
  I1009 13:20:44.148071  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=185 concurrencyDenominator=185 backstop=false
  I1009 13:20:44.148089  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=277 concurrencyDenominator=277 backstop=false
  I1009 13:20:44.148104  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=6 seatDemandAvg=4.368033947942504 seatDemandStdev=1.415043667391371 seatDemandSmoothed=5.911147284340134 fairFrac=1.3058568329718003 currentCL=6 concurrencyDenominator=6 backstop=false
  I1009 13:20:44.148120  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.3058568329718003 currentCL=47 concurrencyDenominator=47 backstop=false
  I1009 13:20:44.148265  137618 remote_available_controller.go:456] Adding v1.authentication.k8s.io
  I1009 13:20:44.148298  137618 local_available_controller.go:201] Adding v1.admissionregistration.k8s.io
  I1009 13:20:44.148313  137618 remote_available_controller.go:456] Adding v1.apiextensions.k8s.io
  I1009 13:20:44.148319  137618 remote_available_controller.go:456] Adding v1.admissionregistration.k8s.io
  I1009 13:20:44.148340  137618 apiservice_controller.go:173] Adding v1.admissionregistration.k8s.io
  I1009 13:20:44.148362  137618 apiservice_controller.go:173] Adding v1.authorization.k8s.io
  I1009 13:20:44.148371  137618 local_available_controller.go:201] Adding v1.authorization.k8s.io
  I1009 13:20:44.148389  137618 remote_available_controller.go:456] Adding v1.authorization.k8s.io
  I1009 13:20:44.148497  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="885.855µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d95762c1-1db8-4266-8981-63d76807139c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="789.676µs" resp=201
  I1009 13:20:44.149030  137618 apiservice_controller.go:173] Adding v1.autoscaling
  I1009 13:20:44.149051  137618 apiservice_controller.go:173] Adding v2.autoscaling
  I1009 13:20:44.149067  137618 local_available_controller.go:201] Adding v1.autoscaling
  I1009 13:20:44.149074  137618 local_available_controller.go:201] Adding v2.autoscaling
  I1009 13:20:44.149102  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="leader-election"
  I1009 13:20:44.149165  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.440668ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="19c5b6d1-fc19-4a5e-9994-1f80f7b8375c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.352108ms" resp=201
  I1009 13:20:44.149108  137618 remote_available_controller.go:456] Adding v1.autoscaling
  I1009 13:20:44.149209  137618 remote_available_controller.go:456] Adding v2.autoscaling
  I1009 13:20:44.149402  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.265197ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5d95b694-f55f-4f03-94a5-e96bdfc6f9cc" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.180337ms" resp=201
  I1009 13:20:44.150066  137618 apiservice_controller.go:173] Adding v1.batch
  I1009 13:20:44.150512  137618 local_available_controller.go:201] Adding v1.batch
  I1009 13:20:44.150528  137618 local_available_controller.go:201] Adding v1.certificates.k8s.io
  I1009 13:20:44.150564  137618 remote_available_controller.go:456] Adding v1.batch
  I1009 13:20:44.150583  137618 apiservice_controller.go:173] Adding v1.certificates.k8s.io
  I1009 13:20:44.150824  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.320368ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dbc9ed1d-3a65-4985-bce3-e676c9f42b22" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.237357ms" resp=201
  I1009 13:20:44.150976  137618 remote_available_controller.go:456] Adding v1.certificates.k8s.io
  I1009 13:20:44.151128  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.813501ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="99113ac9-4b94-4f29-9b58-72493a4602b6" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.73245ms" resp=201
  I1009 13:20:44.151244  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.57925ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d5f1e778-2f29-4331-9254-3c71a5cc2f2c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.504299ms" resp=201
  I1009 13:20:44.151842  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.97618ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f657594d-5bcb-45b9-b6d5-96860534ba21" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.91108ms" resp=201
  I1009 13:20:44.151880  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.4312ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d7d99218-6ba5-4666-927d-f12a65d32813" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.317068ms" resp=201
  I1009 13:20:44.152010  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","workload-high"]
  I1009 13:20:44.152066  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","workload-high"]
  I1009 13:20:44.152085  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["workload-high"]
  I1009 13:20:44.152103  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["workload-high"]
  I1009 13:20:44.152044  137618 apiservice_controller.go:173] Adding v1.discovery.k8s.io
  I1009 13:20:44.152191  137618 apiservice_controller.go:173] Adding v1.coordination.k8s.io
  I1009 13:20:44.152208  137618 apiservice_controller.go:173] Adding v1.events.k8s.io
  I1009 13:20:44.152214  137618 apiservice_controller.go:173] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 13:20:44.152278  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="workload-high"
  I1009 13:20:44.152049  137618 local_available_controller.go:201] Adding v1.discovery.k8s.io
  I1009 13:20:44.152669  137618 local_available_controller.go:201] Adding v1.coordination.k8s.io
  I1009 13:20:44.152053  137618 remote_available_controller.go:456] Adding v1.discovery.k8s.io
  I1009 13:20:44.152709  137618 remote_available_controller.go:456] Adding v1.coordination.k8s.io
  I1009 13:20:44.152735  137618 local_available_controller.go:201] Adding v1.events.k8s.io
  I1009 13:20:44.152748  137618 local_available_controller.go:201] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 13:20:44.152128  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.152789  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.152804  137618 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=144, lendableCL=48, borrowingCL=600, currentCL=185, quiescing=false, numPending=0 (shares=0xc0008a0a18, shareSum=125)
  I1009 13:20:44.152824  137618 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=192, lendableCL=48, borrowingCL=600, currentCL=277, quiescing=false, numPending=0 (shares=0xc0011e0f48, shareSum=125)
  I1009 13:20:44.152834  137618 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=48, lendableCL=0, borrowingCL=600, currentCL=93, quiescing=false, numPending=0 (shares=0xc0016e2e90, shareSum=125)
  I1009 13:20:44.152854  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=24, lendableCL=0, borrowingCL=600, currentCL=47, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=125)
  I1009 13:20:44.152871  137618 apf_controller.go:898] Introducing queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=192, lendableCL=96, borrowingCL=600, currentCL=144, quiescing=false (shares=0xc001b004f0, shareSum=125)
  I1009 13:20:44.152879  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="1.878421ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bd83535b-9acc-45dd-82d3-67a59c5f3eea" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.779621ms" resp=201
  I1009 13:20:44.152907  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=212 concurrencyDenominator=212 backstop=false
  I1009 13:20:44.152943  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=71 concurrencyDenominator=71 backstop=false
  I1009 13:20:44.152975  137618 apiservice_controller.go:173] Adding v1.networking.k8s.io
  I1009 13:20:44.152988  137618 remote_available_controller.go:456] Adding v1.events.k8s.io
  I1009 13:20:44.153002  137618 remote_available_controller.go:456] Adding v1.flowcontrol.apiserver.k8s.io
  I1009 13:20:44.153008  137618 local_available_controller.go:201] Adding v1.networking.k8s.io
  I1009 13:20:44.152976  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=35 concurrencyDenominator=35 backstop=false
  I1009 13:20:44.153039  137618 remote_available_controller.go:456] Adding v1.networking.k8s.io
  I1009 13:20:44.153052  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=8 seatDemandAvg=5.520590807796701 seatDemandStdev=1.293250476368933 seatDemandSmoothed=6.813841284165634 fairFrac=1.4705882352941178 currentCL=8 concurrencyDenominator=8 backstop=false
  I1009 13:20:44.153087  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=141 concurrencyDenominator=141 backstop=false
  I1009 13:20:44.153140  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=1.4705882352941178 currentCL=141 concurrencyDenominator=141 backstop=false
  I1009 13:20:44.154613  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="2.035061ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fd7d19f7-1bd3-405a-bcf1-0d26fbe39c58" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.326288ms" resp=201
  I1009 13:20:44.154997  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="workload-low"
  I1009 13:20:44.155158  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.551736ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="00936e40-8946-4e99-9e1c-bd7bdd5c97ea" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.860222ms" resp=201
  I1009 13:20:44.155244  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","workload-low"]
  I1009 13:20:44.155266  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="898.865µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d75839f9-f2fb-43a7-a13d-3d2c3264571e" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="809.314µs" resp=201
  I1009 13:20:44.155283  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","workload-low"]
  I1009 13:20:44.155316  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["workload-low"]
  I1009 13:20:44.155334  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["workload-low"]
  I1009 13:20:44.155360  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.155373  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.155373  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="3.253529ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4af33e83-b818-432a-825e-8dda7a2cde1a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.012631ms" resp=201
  I1009 13:20:44.155395  137618 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=107, lendableCL=27, borrowingCL=600, currentCL=212, quiescing=false, numPending=0 (shares=0xc0011e0f48, shareSum=225)
  I1009 13:20:44.155422  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="3.153879ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="60d83840-1b34-4895-b982-89d21dfe0737" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.686039ms" resp=201
  I1009 13:20:44.155430  137618 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=27, lendableCL=0, borrowingCL=600, currentCL=71, quiescing=false, numPending=0 (shares=0xc0016e2e90, shareSum=225)
  I1009 13:20:44.155522  137618 apf_controller.go:906] Retaining queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=107, lendableCL=54, borrowingCL=600, currentCL=141, quiescing=false, numPending=0 (shares=0xc001b004f0, shareSum=225)
  I1009 13:20:44.155545  137618 apf_controller.go:898] Introducing queues for priority level "workload-low": config={"type":"Limited","limited":{"nominalConcurrencyShares":100,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":90}}, nominalCL=267, lendableCL=240, borrowingCL=600, currentCL=147, quiescing=false (shares=0xc001b00690, shareSum=225)
  I1009 13:20:44.155565  137618 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=80, lendableCL=26, borrowingCL=600, currentCL=141, quiescing=false, numPending=0 (shares=0xc0008a0a18, shareSum=225)
  I1009 13:20:44.155575  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=14, lendableCL=0, borrowingCL=600, currentCL=35, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=225)
  I1009 13:20:44.155601  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=125 concurrencyDenominator=125 backstop=false
  I1009 13:20:44.155628  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=64 concurrencyDenominator=64 backstop=false
  I1009 13:20:44.155638  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/apiregistration.k8s.io/v1/apiservices" latency="2.801996ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0151c16d-b7a4-401a-b888-1940cb8a51f1" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.416754ms" resp=201
  I1009 13:20:44.155690  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=127 concurrencyDenominator=127 backstop=false
  I1009 13:20:44.155709  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=33 concurrencyDenominator=33 backstop=false
  I1009 13:20:44.155731  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=9 seatDemandAvg=6.952062895578409 seatDemandStdev=1.897763701547215 seatDemandSmoothed=8.849826597125624 fairFrac=2.3607843137254902 currentCL=9 concurrencyDenominator=9 backstop=false
  I1009 13:20:44.155758  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=189 concurrencyDenominator=189 backstop=false
  I1009 13:20:44.155785  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3607843137254902 currentCL=64 concurrencyDenominator=64 backstop=false
  I1009 13:20:44.155981  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 7 objects queued in incoming channel.
  I1009 13:20:44.156002  137618 cacher.go:1028] cacher (apiservices.apiregistration.k8s.io): 8 objects queued in incoming channel.
  I1009 13:20:44.156408  137618 apiservice_controller.go:173] Adding v1.policy
  I1009 13:20:44.156428  137618 apiservice_controller.go:173] Adding v1.scheduling.k8s.io
  I1009 13:20:44.156437  137618 local_available_controller.go:201] Adding v1.policy
  I1009 13:20:44.156472  137618 local_available_controller.go:201] Adding v1.scheduling.k8s.io
  I1009 13:20:44.156522  137618 remote_available_controller.go:456] Adding v1.policy
  I1009 13:20:44.156535  137618 remote_available_controller.go:456] Adding v1.scheduling.k8s.io
  I1009 13:20:44.156565  137618 remote_available_controller.go:456] Adding v1.rbac.authorization.k8s.io
  I1009 13:20:44.156586  137618 remote_available_controller.go:456] Adding v1.node.k8s.io
  I1009 13:20:44.156602  137618 apiservice_controller.go:173] Adding v1.rbac.authorization.k8s.io
  I1009 13:20:44.156614  137618 local_available_controller.go:201] Adding v1.rbac.authorization.k8s.io
  I1009 13:20:44.156638  137618 local_available_controller.go:201] Adding v1.node.k8s.io
  I1009 13:20:44.156679  137618 apiservice_controller.go:173] Adding v1.node.k8s.io
  I1009 13:20:44.156688  137618 remote_available_controller.go:456] Adding v1.storage.k8s.io
  I1009 13:20:44.156698  137618 apiservice_controller.go:173] Adding v1.storage.k8s.io
  I1009 13:20:44.156706  137618 local_available_controller.go:201] Adding v1.storage.k8s.io
  I1009 13:20:44.156974  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.766971ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="00f51ebd-cdfe-4ddc-a4bb-f0a1cbc189fe" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.103316ms" resp=201
  I1009 13:20:44.157022  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["waiting","global-default"]
  I1009 13:20:44.157052  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_request_utilization" labelValues=["executing","global-default"]
  I1009 13:20:44.157080  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_priority_level_seat_utilization" labelValues=["global-default"]
  I1009 13:20:44.157105  137618 timing_ratio_histogram.go:196] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the efficient case" fqName="apiserver_flowcontrol_demand_seats" labelValues=["global-default"]
  I1009 13:20:44.157123  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.157132  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.157149  137618 apf_controller.go:906] Retaining queues for priority level "node-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":25}}, nominalCL=98, lendableCL=25, borrowingCL=600, currentCL=189, quiescing=false, numPending=0 (shares=0xc0011e0f48, shareSum=245)
  I1009 13:20:44.157175  137618 apf_controller.go:906] Retaining queues for priority level "leader-election": config={"type":"Limited","limited":{"nominalConcurrencyShares":10,"limitResponse":{"type":"Queue","queuing":{"queues":16,"handSize":4,"queueLengthLimit":50}},"lendablePercent":0}}, nominalCL=25, lendableCL=0, borrowingCL=600, currentCL=64, quiescing=false, numPending=0 (shares=0xc0016e2e90, shareSum=245)
  I1009 13:20:44.157204  137618 apf_controller.go:906] Retaining queues for priority level "catch-all": config={"type":"Limited","limited":{"nominalConcurrencyShares":5,"limitResponse":{"type":"Reject"},"lendablePercent":0}}, nominalCL=13, lendableCL=0, borrowingCL=600, currentCL=33, quiescing=false, numPending=0 (shares=0xc000385448, shareSum=245)
  I1009 13:20:44.157219  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="suggested" name="global-default"
  I1009 13:20:44.157236  137618 apf_controller.go:906] Retaining queues for priority level "workload-high": config={"type":"Limited","limited":{"nominalConcurrencyShares":40,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=98, lendableCL=49, borrowingCL=600, currentCL=125, quiescing=false, numPending=0 (shares=0xc001b004f0, shareSum=245)
  I1009 13:20:44.157277  137618 apf_controller.go:906] Retaining queues for priority level "workload-low": config={"type":"Limited","limited":{"nominalConcurrencyShares":100,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":90}}, nominalCL=245, lendableCL=221, borrowingCL=600, currentCL=64, quiescing=false, numPending=0 (shares=0xc001b00690, shareSum=245)
  I1009 13:20:44.157311  137618 apf_controller.go:898] Introducing queues for priority level "global-default": config={"type":"Limited","limited":{"nominalConcurrencyShares":20,"limitResponse":{"type":"Queue","queuing":{"queues":128,"handSize":6,"queueLengthLimit":50}},"lendablePercent":50}}, nominalCL=49, lendableCL=25, borrowingCL=600, currentCL=37, quiescing=false (shares=0xc001b01220, shareSum=245)
  I1009 13:20:44.157329  137618 apf_controller.go:906] Retaining queues for priority level "system": config={"type":"Limited","limited":{"nominalConcurrencyShares":30,"limitResponse":{"type":"Queue","queuing":{"queues":64,"handSize":6,"queueLengthLimit":50}},"lendablePercent":33}}, nominalCL=74, lendableCL=24, borrowingCL=600, currentCL=127, quiescing=false, numPending=0 (shares=0xc0008a0a18, shareSum=245)
  I1009 13:20:44.157361  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.157402  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.157500  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.157532  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.157550  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.157580  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.157611  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=3.6496698617973182 seatDemandStdev=0.47707329884365823 seatDemandSmoothed=8.741195678086477 fairFrac=2.3333333333333335 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 13:20:44.157634  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.159598  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.159618  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.159661  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.159689  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.159726  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.159747  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.159788  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.159808  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=3.844128218261924 seatDemandStdev=0.3627337444957066 seatDemandSmoothed=8.636906002633914 fairFrac=2.3333333333333335 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 13:20:44.159832  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.159854  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.159831  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="2.363403ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fc99b019-7319-40dc-b90b-09ae164a1705" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.001772ms" resp=201
  I1009 13:20:44.160065  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.159592264 +0000 UTC m=+1.623611475 Found This FlowSchema references the PriorityLevelConfiguration object named "system" and it exists} to FlowSchema system-nodes, which had ResourceVersion=31, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.159592264 +0000 UTC m=+1.623611475"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "system" and it exists`,
    }
  I1009 13:20:44.160196  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-nodes"
  I1009 13:20:44.162862  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.585149ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b90d82ec-c8c2-4e89-86d3-c0eb764b585c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.357789ms" resp=201
  I1009 13:20:44.163107  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-node-high"
  I1009 13:20:44.163690  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-nodes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.427144ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="92070ea3-1807-47f4-b35d-9cb0f44cd4cf" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.294674ms" resp=200
  I1009 13:20:44.163947  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.163966  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.164006  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.164031  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.164050  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.164087  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.164113  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.164136  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.164157  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.164175  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=3.9557572004115316 seatDemandStdev=0.8491621433049844 seatDemandSmoothed=8.548770309478815 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.164271  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.16394141 +0000 UTC m=+1.627960610 Found This FlowSchema references the PriorityLevelConfiguration object named "node-high" and it exists} to FlowSchema system-node-high, which had ResourceVersion=32, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.16394141 +0000 UTC m=+1.627960610"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "node-high" and it exists`,
    }
  I1009 13:20:44.165444  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.878751ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="85d3cba2-d233-40b1-b888-2ab61eeac58a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.65834ms" resp=201
  I1009 13:20:44.165940  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="probes"
  I1009 13:20:44.166037  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-node-high/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.48834ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1d4743d4-6033-427d-9906-cd8c1921c8d0" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.399219ms" resp=200
  I1009 13:20:44.166414  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.166427  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.166482  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.1869568190198505 seatDemandStdev=0.6516823416641407 seatDemandSmoothed=8.463437293056534 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.166510  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.166527  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.166546  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.166568  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.166586  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.166603  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.166619  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.166699  137618 apf_controller.go:609] Controller writing Condition {Dangling True 2024-10-09 13:20:44.166408344 +0000 UTC m=+1.630427554 NotFound This FlowSchema references the PriorityLevelConfiguration object named "exempt" but there is no such object} to FlowSchema probes, which had ResourceVersion=34, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "True",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.166408344 +0000 UTC m=+1.630427554"},
  - 	Reason:             "",
  + 	Reason:             "NotFound",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "exempt" but there is no such object`,
    }
  I1009 13:20:44.167232  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="949.685µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="cd66c44f-d8ec-4e4a-8ed5-6363a7b3e37f" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="852.615µs" resp=201
  I1009 13:20:44.167709  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="system-leader-election"
  I1009 13:20:44.168905  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/probes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.881491ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f647e9e1-09ec-435e-bc93-37cc959e7de8" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.747931ms" resp=200
  I1009 13:20:44.169079  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.169090  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.169123  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.169144  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.169162  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.169182  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.169176  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.155287ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d4d352b0-7b18-4693-8615-cd303de89d55" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.047327ms" resp=201
  I1009 13:20:44.169198  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.330906877415973 seatDemandStdev=0.47053960077211204 seatDemandSmoothed=8.37921150431456 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.169220  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.169238  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.169258  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.169335  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.16907449 +0000 UTC m=+1.633093700 Found This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists} to FlowSchema system-leader-election, which had ResourceVersion=36, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.16907449 +0000 UTC m=+1.633093700"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists`,
    }
  I1009 13:20:44.169398  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="workload-leader-election"
  I1009 13:20:44.171198  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.550269ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c77b731d-af11-4a50-8789-8154db5b1240" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.302637ms" resp=201
  I1009 13:20:44.171955  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="endpoint-controller"
  I1009 13:20:44.172005  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-leader-election/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.366784ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5197193d-9c44-4568-9f7d-c09caaffc4c0" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.239854ms" resp=200
  I1009 13:20:44.172661  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.172686  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.172765  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.172804  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.172829  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.172860  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.172885  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.172915  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.172937  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.033810095121458 seatDemandStdev=0.8756196746596892 seatDemandSmoothed=8.29940652442029 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.172954  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.173246  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.172621721 +0000 UTC m=+1.636640931 Found This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists} to FlowSchema workload-leader-election, which had ResourceVersion=38, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.172621721 +0000 UTC m=+1.636640931"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists`,
    }
  I1009 13:20:44.175001  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="2.492514ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9f9fd9bf-dc27-4e25-a396-269380dfb6b4" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.64565ms" resp=201
  I1009 13:20:44.175241  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-controller-manager"
  I1009 13:20:44.177022  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.538469ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dec1e8d1-bde0-428c-8906-62fe18c4f0d2" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.402028ms" resp=201
  I1009 13:20:44.177268  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-scheduler"
  I1009 13:20:44.177914  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/workload-leader-election/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="4.150184ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c35163c4-0855-4bef-8e73-886595e44f0c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.978304ms" resp=200
  I1009 13:20:44.178362  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.172623761 +0000 UTC m=+1.636642971 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema endpoint-controller, which had ResourceVersion=39, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.172623761 +0000 UTC m=+1.636642971"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 13:20:44.178772  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.259727ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="db37618c-1ebb-457c-91b0-7c0fda34b82a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.122037ms" resp=201
  I1009 13:20:44.179224  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="kube-system-service-accounts"
  I1009 13:20:44.180964  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/endpoint-controller/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.225233ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a843c8a0-5108-44ce-a2a8-09224391eddf" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.105982ms" resp=200
  I1009 13:20:44.181031  137618 cacher.go:1028] cacher (flowschemas.flowcontrol.apiserver.k8s.io): 1 objects queued in incoming channel.
  I1009 13:20:44.181049  137618 cacher.go:1028] cacher (flowschemas.flowcontrol.apiserver.k8s.io): 2 objects queued in incoming channel.
  I1009 13:20:44.181064  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.519938ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="17bcf345-b5fc-4984-999d-8efec8708ea9" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.393827ms" resp=201
  I1009 13:20:44.181261  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.172632271 +0000 UTC m=+1.636651480 Found This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists} to FlowSchema system-leader-election, which had ResourceVersion=36, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.172632271 +0000 UTC m=+1.636651480"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "leader-election" and it exists`,
    }
  I1009 13:20:44.181342  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="service-accounts"
  I1009 13:20:44.182913  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.234507ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1faae17a-9207-4029-a99a-2b05c7bcfba8" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.106766ms" resp=201
  I1009 13:20:44.183177  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="suggested" name="global-default"
  I1009 13:20:44.184447  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-leader-election/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.844506ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6024b48f-b796-4618-bc2c-e5dd408227af" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.692416ms" resp=200
  I1009 13:20:44.184910  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.525559ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2d0487e7-46c2-4c1d-8ffc-49fbe7463f42" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.355528ms" resp=201
  I1009 13:20:44.184960  137618 apf_controller.go:817] Retaining mandatory priority level "catch-all" despite lack of API object
  I1009 13:20:44.184978  137618 apf_controller.go:817] Retaining mandatory priority level "exempt" despite lack of API object
  I1009 13:20:44.185040  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.185082  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.185100  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.185126  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=4.376028024004079 seatDemandStdev=0.690376758469279 seatDemandSmoothed=8.225047484355509 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.185155  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.185164  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="mandatory" name="catch-all"
  I1009 13:20:44.185182  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.185201  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.185224  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.185339  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.184923672 +0000 UTC m=+1.648942912 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-low" and it exists} to FlowSchema service-accounts, which had ResourceVersion=46, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.184923672 +0000 UTC m=+1.648942912"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-low" and it exists`,
    }
  I1009 13:20:44.186661  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1" latency="1.245357ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="78aa6cd0-791b-450c-9aa8-deb445ba07f0" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.120857ms" resp=201
  I1009 13:20:44.186921  137618 strategy.go:270] "Successfully created *v1.PriorityLevelConfiguration" type="mandatory" name="exempt"
  I1009 13:20:44.188693  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/service-accounts/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.706455ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="550f4e88-d12a-4938-809b-a5b854cbeec3" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.571235ms" resp=200
  I1009 13:20:44.191147  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="4.004683ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="16abdc4a-be87-40cd-b6aa-f2e297c2c9bc" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.881163ms" resp=201
  I1009 13:20:44.191506  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="mandatory" name="exempt"
  I1009 13:20:44.191619  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.184933312 +0000 UTC m=+1.648952512 Found This FlowSchema references the PriorityLevelConfiguration object named "global-default" and it exists} to FlowSchema global-default, which had ResourceVersion=47, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.184933312 +0000 UTC m=+1.648952512"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "global-default" and it exists`,
    }
  I1009 13:20:44.193299  137618 admission.go:143] "Namespace existed in cache after waiting" namespace="kube-system"
  I1009 13:20:44.193967  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas?fieldManager=api-priority-and-fairness-config-producer-v1" latency="2.088241ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="916ea84e-acb2-44b3-82b0-22fff47bb427" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.878752ms" resp=201
  I1009 13:20:44.194379  137618 strategy.go:270] "Successfully created *v1.FlowSchema" type="mandatory" name="catch-all"
  I1009 13:20:44.194764  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/kube-system/configmaps" latency="52.84618ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dadfd75f-132a-4f44-b76b-bdcafaf19972" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="52.671479ms" resp=201
  I1009 13:20:44.195078  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/global-default/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="3.062608ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e39840fe-45a1-4d3f-8d27-4efb5b2d212c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.926127ms" resp=200
  I1009 13:20:44.195502  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.184945952 +0000 UTC m=+1.648965152 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-controller-manager, which had ResourceVersion=41, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.184945952 +0000 UTC m=+1.648965152"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 13:20:44.198314  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-controller-manager/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.460625ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="df3d2219-bb76-4f76-9f3e-a37fb3b4d171" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.314353ms" resp=200
  I1009 13:20:44.198681  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.184947332 +0000 UTC m=+1.648966542 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-scheduler, which had ResourceVersion=42, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.184947332 +0000 UTC m=+1.648966542"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 13:20:44.201169  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-scheduler/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.123453ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="78c0019e-c018-4859-881f-d6bd42233092" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.966202ms" resp=200
  I1009 13:20:44.201644  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.184948202 +0000 UTC m=+1.648967412 Found This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists} to FlowSchema kube-system-service-accounts, which had ResourceVersion=44, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.184948202 +0000 UTC m=+1.648967412"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "workload-high" and it exists`,
    }
  I1009 13:20:44.204713  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/kube-system-service-accounts/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="2.660416ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9cd0da9b-72a2-46ff-a7a0-e9bebd6761e6" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.495245ms" resp=200
  I1009 13:20:44.205219  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.205264  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.205285  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.205318  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.205338  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.205358  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.205376  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=5 seatDemandAvg=3.4362937593989953 seatDemandStdev=0.9785325320430138 seatDemandSmoothed=8.1374123969185 fairFrac=2.3333333333333335 currentCL=5 concurrencyDenominator=5 backstop=false
  I1009 13:20:44.205403  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.205533  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.205152141 +0000 UTC m=+1.669171340 Found This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists} to FlowSchema exempt, which had ResourceVersion=51, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.205152141 +0000 UTC m=+1.669171340"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists`,
    }
  I1009 13:20:44.207588  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/exempt/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.572299ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3bb300a6-b076-45c7-85f7-355536da6615" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.418498ms" resp=200
  I1009 13:20:44.207862  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.205154871 +0000 UTC m=+1.669174070 Found This FlowSchema references the PriorityLevelConfiguration object named "exempt" and it exists} to FlowSchema probes, which had ResourceVersion=37, because its previous value was {"type":"Dangling","status":"True","lastTransitionTime":"2024-10-09T13:20:44Z","reason":"NotFound","message":"This FlowSchema references the PriorityLevelConfiguration object named \"exempt\" but there is no such object"}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "True",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44 +0000 UTC"},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.205154871 +0000 UTC m=+1.669174070"},
  - 	Reason:             "NotFound",
  + 	Reason:             "Found",
    	Message: strings.Join({
    		"This FlowSchema references the PriorityLevelConfiguration object",
    		` named "exempt" `,
  - 		"but there is no such object",
  + 		"and it exists",
    	}, ""),
    }
  I1009 13:20:44.210054  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/probes/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.931112ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="83724430-0ddf-4165-8ede-a7e457b6befa" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.81092ms" resp=200
  I1009 13:20:44.210415  137618 apf_controller.go:609] Controller writing Condition {Dangling False 2024-10-09 13:20:44.205159031 +0000 UTC m=+1.669178241 Found This FlowSchema references the PriorityLevelConfiguration object named "catch-all" and it exists} to FlowSchema catch-all, which had ResourceVersion=52, because its previous value was {"type":"Dangling","lastTransitionTime":null}, diff:   v1.FlowSchemaCondition{
    	Type:               "Dangling",
  - 	Status:             "",
  + 	Status:             "False",
  - 	LastTransitionTime: v1.Time{},
  + 	LastTransitionTime: v1.Time{Time: s"2024-10-09 13:20:44.205159031 +0000 UTC m=+1.669178241"},
  - 	Reason:             "",
  + 	Reason:             "Found",
  - 	Message:            "",
  + 	Message:            `This FlowSchema references the PriorityLevelConfiguration object named "catch-all" and it exists`,
    }
  I1009 13:20:44.212092  137618 httplog.go:134] "HTTP" verb="APPLY" URI="/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/catch-all/status?fieldManager=api-priority-and-fairness-config-consumer-v1&force=true" latency="1.384708ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5bd602c1-8320-4c47-ad36-a8cd73ea6700" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.273958ms" resp=200
  I1009 13:20:44.212566  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:44.212624  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:44.212641  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.212674  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:44.212694  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:44.212712  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=3 seatDemandAvg=2.6247610953979503 seatDemandStdev=0.4841845403098938 seatDemandSmoothed=8.021757661410653 fairFrac=2.3333333333333335 currentCL=3 concurrencyDenominator=3 backstop=false
  I1009 13:20:44.212736  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:44.212799  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:44.242905  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.243027  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="419.023µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="50d592b9-5943-4c8e-ac23-361171fab598" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="223.241µs" resp=500
  I1009 13:20:44.244920  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-public/resourcequotas" latency="97.782402ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3a1d2072-1b9a-4e56-a2b9-e4e79be79f31" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="97.695672ms" resp=200
  I1009 13:20:44.246089  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="99.834435ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1ce8a51f-9589-4f52-ac08-67d35594cfb4" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="99.500073ms" resp=201
  I1009 13:20:44.315091  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.083536ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="14e1ca60-676c-4821-ac86-874c568cb74b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="855.744µs" resp=404
  I1009 13:20:44.316281  137618 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
  I1009 13:20:44.317198  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases" latency="1.55271ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e432d301-38b3-4e47-b847-8c668510b56d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.434668ms" resp=201
  I1009 13:20:44.343222  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.343364  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="473.363µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b2f770b1-b62f-4b79-9e38-1d238a2f6f3a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="256.632µs" resp=500
  I1009 13:20:44.345287  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/default/resourcequotas" latency="98.131544ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3f1a7a27-3242-4278-9e9a-5b8c98519458" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="98.025163ms" resp=200
  I1009 13:20:44.346814  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="100.332196ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5afd3e37-62eb-4e1d-9c77-c6ad3a643a95" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="100.228156ms" resp=201
  I1009 13:20:44.357794  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: healthz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.357912  137618 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="446.693µs" userAgent="Go-http-client/2.0" audit-ID="46d3d3e2-bc37-4880-985f-77208360fde9" srcIP="127.0.0.1:48672" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="223.732µs" resp=500
  W1009 13:20:44.358132  137618 util.go:106] Health check on "https://127.0.0.1:6443/healthz" failed, status=500
  I1009 13:20:44.443075  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.443173  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="434.223µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e9f714c1-9bda-40a4-aa4c-febb97bf3968" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="205.891µs" resp=500
  I1009 13:20:44.445907  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/kube-node-lease/resourcequotas" latency="97.921843ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9625219e-1207-437d-8589-cd41128cd144" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="97.801471ms" resp=200
  I1009 13:20:44.446912  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="99.729383ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="92662c92-cf92-4eb4-aa74-8afd0e1adfcc" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="99.602642ms" resp=201
  I1009 13:20:44.542820  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.542915  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="395.583µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3b2f868b-41de-4381-8015-be51c8615e48" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="221.482µs" resp=500
  I1009 13:20:44.643550  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.643646  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="382.603µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="02621506-108f-4580-9da8-86bff1d7586e" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="208.542µs" resp=500
  I1009 13:20:44.743110  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.743215  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="407.313µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3f1a587b-1364-4a33-a0ef-f378df6c5370" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="205.471µs" resp=500
  I1009 13:20:44.842792  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.842887  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="391.482µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3f0d1e3f-610e-4330-9093-7ca31e6a7784" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="209.341µs" resp=500
  I1009 13:20:44.943553  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:44.943655  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="443.973µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e4144feb-10d0-40a1-a0a4-f358753b8f1b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="239.181µs" resp=500
  I1009 13:20:45.040895  137618 controller.go:106] OpenAPI AggregationController: Processing item k8s_internal_local_kube_aggregator_types
  I1009 13:20:45.041487  137618 controller.go:106] OpenAPI AggregationController: Processing item openapiv2converter
  I1009 13:20:45.042251  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/scheduling.k8s.io/v1/priorityclasses/system-node-critical" latency="860.375µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="510970ca-afe8-4d56-b300-18e20fad78af" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="651.463µs" resp=404
  I1009 13:20:45.042448  137618 healthz.go:278] poststarthook/scheduling/bootstrap-system-priority-classes check failed: readyz
  [-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished
  I1009 13:20:45.042550  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="278.792µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4ae00f05-82b0-48ce-a5bb-ceb58e077364" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="186.901µs" resp=500
  I1009 13:20:45.044853  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/scheduling.k8s.io/v1/priorityclasses" latency="1.71505ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8b5c2b87-28ec-4f48-888c-64aecd0af466" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.582299ms" resp=201
  I1009 13:20:45.045110  137618 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
  I1009 13:20:45.045863  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/scheduling.k8s.io/v1/priorityclasses/system-cluster-critical" latency="601.513µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c236953e-1eab-4c7b-a5fa-20896e1e3f3c" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="508.773µs" resp=404
  I1009 13:20:45.047215  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/scheduling.k8s.io/v1/priorityclasses" latency="962.965µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="93ea9b34-f3dd-4a49-a8c9-fe65e6dd4f06" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="874.865µs" resp=201
  I1009 13:20:45.047368  137618 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
  I1009 13:20:45.047380  137618 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
  I1009 13:20:45.059020  137618 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="793.935µs" userAgent="Go-http-client/2.0" audit-ID="bc7bd747-fd3f-4f97-92c6-b882438bb6f8" srcIP="127.0.0.1:48506" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="597.094µs" resp=200
  I1009 13:20:45.139383  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0" latency="1.354127ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="589d82fb-7e79-4f54-9f80-b2422e982089" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.114687ms" resp=200
  I1009 13:20:45.139486  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes?fieldSelector=metadata.name%3Dsrv579909&limit=500&resourceVersion=0" latency="1.397269ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="54175191-9e46-415f-85bb-5bd67e0704ae" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.144907ms" resp=200
  I1009 13:20:45.139862  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909?resourceVersion=0" latency="831.265µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ec063676-d8d3-4a70-8a7d-54f87d3dcdf6" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="696.165µs" resp=404
  I1009 13:20:45.141157  137618 get.go:278] "Starting watch" path="/api/v1/services" resourceVersion="2" labels="" fields="spec.clusterIP!=None" timeout="8m28s"
  I1009 13:20:45.141497  137618 get.go:278] "Starting watch" path="/api/v1/nodes" resourceVersion="1" labels="" fields="metadata.name=srv579909" timeout="6m25s"
  I1009 13:20:45.144045  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="4.907129ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="aaf28b9f-0e43-4614-a7f0-54b433f65cde" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.840119ms" resp=201
  I1009 13:20:45.144602  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0" latency="1.98938ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2322f80e-4b40-40ea-a45c-a8f8f5454d8f" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.889691ms" resp=200
  I1009 13:20:45.144811  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="2.563596ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2a538ea1-78e2-4d75-b902-2dbe410ca650" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.776731ms" resp=404
  I1009 13:20:45.145310  137618 httplog.go:134] "HTTP" verb="GET" URI="/readyz" latency="310.772µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a44c49dd-ac83-4804-8a63-f269f2b359f2" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="256.042µs" resp=200
  I1009 13:20:45.146256  137618 get.go:278] "Starting watch" path="/apis/storage.k8s.io/v1/csidrivers" resourceVersion="1" labels="" fields="" timeout="8m39s"
  I1009 13:20:45.147175  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="716.624µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="097b373b-6751-4cd3-8533-23493a77ee5a" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="625.934µs" resp=404
  I1009 13:20:45.148405  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="4.181945ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="dfc6bfc8-d2fc-4251-bf24-1fb58822ba51" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.107994ms" resp=404
  I1009 13:20:45.148992  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="3.463681ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c7d6e87f-3649-4d1f-a243-a94842bda9e0" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.39194ms" resp=201
  I1009 13:20:45.151194  137618 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.0.0.1"}
  I1009 13:20:45.151366  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/services" latency="2.185153ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e53f060a-de56-4751-beb7-041a510865c3" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.066972ms" resp=201
  I1009 13:20:45.153342  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="539.284µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="95b0f42d-87c3-4ebe-846d-feef67ebea0f" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="437.313µs" resp=404
  W1009 13:20:45.154631  137618 lease.go:265] Resetting endpoints for master service "kubernetes" to [82.112.230.236]
  I1009 13:20:45.155588  137618 controller.go:615] quota admission added evaluator for: endpoints
  I1009 13:20:45.156414  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/endpoints" latency="1.555679ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="59461e67-848d-4eac-8061-39db0beb07a4" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.419159ms" resp=201
  I1009 13:20:45.157400  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="467.683µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="fa7c8a23-bcdb-4e7d-8f95-d9a59a46a3bd" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="346.002µs" resp=404
  I1009 13:20:45.158502  137618 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
  I1009 13:20:45.159319  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices" latency="1.515688ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a0a842a7-5b1b-4078-9449-b4e6a81b2811" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.386048ms" resp=201
  I1009 13:20:45.160304  137618 httplog.go:134] "HTTP" verb="LIST" URI="/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0" latency="366.002µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a3a00836-5fa6-45dd-a1ca-2f052dc54214" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="168.331µs" resp=200
  I1009 13:20:45.161120  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909?timeout=10s" latency="559.614µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="cda4eafd-1ea6-4f92-bf7a-638f3c0d1221" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="483.873µs" resp=404
  I1009 13:20:45.162579  137618 get.go:278] "Starting watch" path="/apis/node.k8s.io/v1/runtimeclasses" resourceVersion="1" labels="" fields="" timeout="7m23s"
  I1009 13:20:45.162914  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="897.855µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0b0f9727-bf81-4aac-b16f-d2625c4b8106" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="785.544µs" resp=404
  I1009 13:20:45.164110  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="544.022µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="793f5dcd-4e3f-4195-9ce0-0cd02b6b9b31" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="428.532µs" resp=404
  I1009 13:20:45.166154  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.575449ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="32fe3d0e-32b4-4a77-85ec-1f91b49787e1" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.482109ms" resp=201
  I1009 13:20:45.167799  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.239927ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5f13a4ba-cba9-4c60-ae1c-5deb19d44e05" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.137836ms" resp=201
  I1009 13:20:45.169303  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="946.885µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d46649d8-90c8-4d53-a633-37017990b48e" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="861.075µs" resp=201
  I1009 13:20:45.170998  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="1.248707ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3cd2f1b4-656d-4ce8-8769-578d3b67ca6c" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.090216ms" resp=201
  I1009 13:20:45.219940  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="1.121886ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6b73fcf4-23b1-4c34-81da-ea56d2df1a44" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="853.525µs" resp=404
  I1009 13:20:45.221328  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="956.395µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d07fb7c8-9931-4f1e-86df-24677ec05beb" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="747.685µs" resp=404
  I1009 13:20:45.271423  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/nodes" latency="2.445874ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="d0e2d3bf-fe8d-4a1b-86d1-16b15bc4b04a" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.165123ms" resp=201
  I1009 13:20:45.272411  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fccb71755c808d" latency="3.142059ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3f59801c-99e9-4a11-bb58-b18656b42745" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.043098ms" resp=200
  I1009 13:20:45.275820  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fccb71755ca0c6" latency="2.686647ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="313d4be7-d045-4a64-b34b-876e2f3ca0a8" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.515645ms" resp=200
  I1009 13:20:45.277147  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/nodes/srv579909/status?timeout=10s" latency="2.854356ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="43e616bc-5e4a-44fd-a8dd-1e63d5d456f2" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.230052ms" resp=200
  I1009 13:20:45.279206  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/default/events/srv579909.17fccb71755cb71e" latency="1.983601ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="48f1a729-e106-48cb-b552-a48d601d7c42" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.853181ms" resp=200
  I1009 13:20:45.359939  137618 httplog.go:134] "HTTP" verb="GET" URI="/healthz" latency="379.512µs" userAgent="Go-http-client/2.0" audit-ID="b57cafb2-8087-4324-84b7-8aee48d5ba3c" srcIP="127.0.0.1:48672" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="112.911µs" resp=200
  I1009 13:20:45.364283  137618 httplog.go:134] "HTTP" verb="GET" URI="/api" latency="649.793µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="5054a56d-2982-4a7e-a907-8d60053c5ba0" srcIP="127.0.0.1:48706" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="527.753µs" resp=200
  I1009 13:20:45.365600  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis" latency="751.984µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="fc817447-66be-4b56-9956-a95dd830c819" srcIP="127.0.0.1:48706" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="627.023µs" resp=200
  I1009 13:20:45.366167  137618 internal_services.go:75] E2E services started.
  I1009 13:20:45.366229  137618 reflector.go:305] Starting reflector *v1.Namespace (5m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:45.366245  137618 reflector.go:341] Listing and watching *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:45.366248  137618 shared_informer.go:313] Waiting for caches to sync for namespace
  I1009 13:20:45.366939  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces?limit=500&resourceVersion=0" latency="493.693µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="b690895c-0322-4aaf-9431-09d528b55552" srcIP="127.0.0.1:48706" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="346.741µs" resp=200
  I1009 13:20:45.367334  137618 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go/informers/factory.go:160
  I1009 13:20:45.367829  137618 get.go:278] "Starting watch" path="/api/v1/namespaces" resourceVersion="64" labels="" fields="" timeout="7m49s"
  I1009 13:20:45.467016  137618 shared_informer.go:320] Caches are synced for namespace
  I1009 13:20:45.484585  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/storage.k8s.io/v1/csinodes/srv579909" latency="1.108336ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="65503fa1-dc95-4498-b2a2-1746a8ddd7c7" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="802.854µs" resp=404
  I1009 13:20:45.486233  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909" latency="1.084265ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="43ffca70-acdc-4c90-8223-60be80b9c8a8" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="978.235µs" resp=200
  I1009 13:20:45.488159  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/storage.k8s.io/v1/csinodes" latency="1.252077ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5662d887-b920-4760-8a00-0395267f852f" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.138556ms" resp=201
  I1009 13:20:45.589494  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/default/events" latency="2.071482ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f76830c8-31f8-40c7-a601-9bdb751e2448" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.66617ms" resp=201
  I1009 13:20:45.589497  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/nodes/srv579909/status?timeout=10s" latency="1.950381ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0d2098d6-b873-44d5-b918-217976e2c8c6" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.790119ms" resp=200
  I1009 13:20:46.088786  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.408669ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e6caf544-13ab-4b16-9d74-7f0f6ea1da1a" srcIP="127.0.0.1:48714" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.197777ms" resp=200
  I1009 13:20:46.095001  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.426359ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6fdc2a73-49a4-4316-94e1-9b1f62b3e707" srcIP="127.0.0.1:48714" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.137707ms" resp=200
  I1009 13:20:46.103849  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="5.15372ms" userAgent="Go-http-client/1.1" audit-ID="9aebe93e-31e0-4882-9825-2baf37406ffc" srcIP="127.0.0.1:48722" resp=200
  I1009 13:20:46.119022  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.081427ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="06d581be-b4ef-42f0-a619-3377887bfdac" srcIP="127.0.0.1:48750" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="872.076µs" resp=200
  I1009 13:20:46.121137  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="784.185µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2d81255c-8814-42da-a573-6eb27890897a" srcIP="127.0.0.1:48742" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="646.304µs" resp=200
  I1009 13:20:46.121845  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="997.835µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e7b1c8a8-7add-4b8d-bef5-dc4db5da6fdf" srcIP="127.0.0.1:48726" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="841.575µs" resp=200
  I1009 13:20:46.127646  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="4.085464ms" userAgent="Go-http-client/1.1" audit-ID="3060d9ee-00c4-41a3-812c-c2634e5d2a4d" srcIP="127.0.0.1:48752" resp=200
  I1009 13:20:46.129356  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="1.308228ms" userAgent="Go-http-client/1.1" audit-ID="5d874bc8-65f0-4284-82cb-6eb94c500011" srcIP="127.0.0.1:48764" resp=200
  I1009 13:20:46.134301  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="1.653099ms" userAgent="Go-http-client/1.1" audit-ID="9998ef87-05bf-474d-83b5-d0b0e5d01a27" srcIP="127.0.0.1:48758" resp=200
  I1009 13:20:46.136739  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/pods?fieldSelector=spec.nodeName%3Dsrv579909&limit=500&resourceVersion=0" latency="517.143µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="87c0b198-73e4-47b7-ac3f-d183962660cf" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="306.562µs" resp=200
  I1009 13:20:46.137312  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="971.025µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6d714bbf-c207-4131-8d09-67f07c44ca3d" srcIP="127.0.0.1:48772" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="786.745µs" resp=200
  I1009 13:20:46.137354  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.026996ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="21071d6e-623c-47c8-bfad-3543ace7b906" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="633.073µs" resp=200
  I1009 13:20:46.138315  137618 get.go:278] "Starting watch" path="/api/v1/pods" resourceVersion="1" labels="" fields="spec.nodeName=srv579909" timeout="6m56s"
  I1009 13:20:46.141265  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.043325ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1c9f8693-7736-4736-88fa-7786b2e0f4f8" srcIP="127.0.0.1:48794" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="893.515µs" resp=200
  I1009 13:20:46.150486  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="1.485899ms" userAgent="Go-http-client/1.1" audit-ID="5cabb3f6-c50c-4ace-be59-56684c665ae2" srcIP="127.0.0.1:48818" resp=200
  I1009 13:20:46.160928  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="7.017621ms" userAgent="Go-http-client/1.1" audit-ID="4bc35a13-dffd-4d99-966c-2a66d3878dfa" srcIP="127.0.0.1:48832" resp=200
  I1009 13:20:46.161552  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.573241ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="17b9b56c-6350-4cfe-b4be-79f65192c842" srcIP="127.0.0.1:48834" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.380956ms" resp=200
  I1009 13:20:46.162935  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="17.863974ms" userAgent="Go-http-client/1.1" audit-ID="7756053a-553b-48dd-9409-77cdf6011f8c" srcIP="127.0.0.1:48804" resp=200
  I1009 13:20:46.173937  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/nodes/srv579909/proxy/configz" latency="3.541731ms" userAgent="Go-http-client/1.1" audit-ID="e1f43b85-8255-4140-9b83-727c3cca53b0" srcIP="127.0.0.1:48850" resp=200
  I1009 13:20:46.214632  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/prestop-hook-test-2352/resourcequotas" latency="1.717301ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="59449fc3-f066-46ee-a257-d795bb0afb26" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.58896ms" resp=200
  I1009 13:20:46.216011  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces" latency="4.763488ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="39ccf3c3-d14b-4bf2-b856-40d51beb3ae0" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.183354ms" resp=201
  I1009 13:20:46.227551  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/namespaces/prestop-hook-test-2352/limitranges" latency="2.726776ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4958068a-9546-4f64-a113-6c1e6d62849a" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.607766ms" resp=200
  I1009 13:20:46.232029  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/pods" latency="10.195959ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="93d9c5f5-a60c-4d82-aaa2-38fab8396a6a" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="10.013209ms" resp=201
  I1009 13:20:46.234847  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.473128ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="bd441cb7-93ae-47e1-846a-a7d0beadb5a6" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="971.046µs" resp=200
  I1009 13:20:46.235290  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="3.39161ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c1097961-9aa3-4d6a-b8b8-5c11d529db56" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.167978ms" resp=200
  I1009 13:20:46.246573  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod/status" latency="8.67914ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1c223507-0e84-4cf1-a966-0823122e1008" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="8.5415ms" resp=200
  I1009 13:20:46.653928  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/events" latency="2.013412ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b8306e3c-e68e-4841-939f-8849336f82ee" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.78245ms" resp=201
  I1009 13:20:46.667340  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/events" latency="1.66968ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0aa9dbdc-ebcf-434d-ac29-1a9c4d275e74" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.427948ms" resp=201
  I1009 13:20:46.713813  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/events" latency="3.08899ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e25f2401-d1c0-4751-b6ac-7535e058a94a" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.760629ms" resp=201
  I1009 13:20:47.174365  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.380337ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="5893704e-6210-4c3f-ae5b-7a07a8c34c8f" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.127784ms" resp=200
  I1009 13:20:47.178139  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod/status" latency="2.964156ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="26edd60c-1678-4c74-8e11-0178b036c9d4" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.807444ms" resp=200
  I1009 13:20:48.240087  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.980054ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="1293e822-b8f1-49a7-9b1d-27fa0ae10df7" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.683151ms" resp=200
  I1009 13:20:50.242994  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.494068ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="01f42add-89d8-4b0a-9f00-ad80352f535c" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.249235ms" resp=200
  I1009 13:20:52.246092  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.379306ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="769fe996-fcf1-4bb9-b3a9-d5cd8e0b0b6f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.090053ms" resp=200
  I1009 13:20:54.142320  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:20:54.142406  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:20:54.142429  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:20:54.142472  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:20:54.142495  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:54.142517  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:20:54.142552  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:20:54.142573  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=4 seatDemandAvg=0.05844541076212522 seatDemandStdev=0.3274365153742954 seatDemandSmoothed=7.8461325194993465 fairFrac=2.3333333333333335 currentCL=4 concurrencyDenominator=4 backstop=false
  I1009 13:20:54.249411  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.743311ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="45a7adc5-9987-4f0e-b38e-1ae13f733321" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.387707ms" resp=200
  I1009 13:20:54.632996  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.471981ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b5093692-b086-4e18-b267-916b6bd0b840" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.205886ms" resp=200
  I1009 13:20:55.150770  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="870.931µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="860c7a3c-c9ec-4a60-b6f2-02e8314dd87b" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="693.789µs" resp=200
  I1009 13:20:55.152077  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="632.937µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="0c2b4991-f93b-4e3d-8946-123403068e48" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="494.996µs" resp=200
  I1009 13:20:55.251224  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.135684ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="8f9b64d2-657a-4ab2-a678-10ef4f67fdd1" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="873.131µs" resp=404
  I1009 13:20:55.252665  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/nodes/srv579909?timeout=10s" latency="812.699µs" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="88a789cc-35a9-4e5f-9fdb-0158b3ca2909" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="710.249µs" resp=200
  I1009 13:20:55.254730  137618 httplog.go:134] "HTTP" verb="POST" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases?timeout=10s" latency="1.375866ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="74b58f22-7df2-424c-8b7c-259b75c894d4" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.131674ms" resp=201
  I1009 13:20:56.252952  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.532038ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="2d28775f-8d4a-4fc9-86dd-2229882ae3cf" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.245215ms" resp=200
  I1009 13:20:58.256134  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.334007ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="366a813c-c34d-4189-a915-e3b061c4dfca" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.092253ms" resp=200
  I1009 13:21:00.259756  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.69001ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b2c507f4-a8ad-47ca-b0fa-9387179e5ee4" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.408947ms" resp=200
  I1009 13:21:00.793637  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/events" latency="3.044176ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="437d0d1c-93b5-4e3b-ae34-13880e27c2b7" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.740013ms" resp=201
  I1009 13:21:00.796659  137618 httplog.go:134] "HTTP" verb="POST" URI="/api/v1/namespaces/prestop-hook-test-2352/events" latency="2.099276ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="3edd1519-db62-4aff-8d9d-6b5f0055e312" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.924254ms" resp=201
  I1009 13:21:02.262914  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.891843ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="2c9f6390-c568-49c7-af36-1b193d9dbb3d" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.613739ms" resp=200
  I1009 13:21:04.143285  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:04.143390  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:04.143414  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0017734120379294283 seatDemandStdev=0.04207454156224587 seatDemandSmoothed=7.666679974483666 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:04.143435  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:04.143483  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:04.143509  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:04.143529  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:04.143549  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:04.265900  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.739561ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="26eccaa2-9a1f-4eec-a142-85b1f6688d14" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.392907ms" resp=200
  I1009 13:21:04.810602  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.759811ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="30838eba-84e2-47ca-aca0-2863cdfd75e6" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.504878ms" resp=200
  I1009 13:21:05.151599  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="806.389µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="84145510-4091-44f2-8c2d-dad6ba3d0ab8" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="608.848µs" resp=200
  I1009 13:21:05.152743  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="486.415µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="2f8d1eb0-1465-41a8-9256-8bf12c76cc02" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="392.215µs" resp=200
  I1009 13:21:05.655532  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.908723ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="da8a1b27-63de-4518-a293-f744fe8022d7" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.6333ms" resp=200
  I1009 13:21:06.269523  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.733172ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="c6e44291-8175-4eed-a3ee-a5d265c9e966" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.314125ms" resp=200
  I1009 13:21:08.272231  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.438247ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a20b0cba-f125-4d4f-bed3-a0db96a048d7" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.169554ms" resp=200
  I1009 13:21:10.275338  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.66266ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="5cc1f4fe-0330-400a-80fd-827cbbfcce4c" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.403787ms" resp=200
  I1009 13:21:12.278692  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.520948ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="278144c9-bda8-401a-b957-1c294f03ec4f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.259445ms" resp=200
  I1009 13:21:14.057327  137618 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 13:21:14.057447  137618 remote_available_controller.go:466] Updating v1.batch
  I1009 13:21:14.057513  137618 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 13:21:14.057526  137618 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:21:14.057507  137618 local_available_controller.go:207] Updating v1.batch
  I1009 13:21:14.057552  137618 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 13:21:14.057611  137618 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:21:14.057618  137618 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 13:21:14.057638  137618 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 13:21:14.057641  137618 local_available_controller.go:207] Updating v1.
  I1009 13:21:14.057688  137618 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 13:21:14.057702  137618 remote_available_controller.go:466] Updating v1.
  I1009 13:21:14.057754  137618 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 13:21:14.057723  137618 local_available_controller.go:207] Updating v1.autoscaling
  I1009 13:21:14.057782  137618 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 13:21:14.057822  137618 local_available_controller.go:207] Updating v1.policy
  I1009 13:21:14.057829  137618 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 13:21:14.057835  137618 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 13:21:14.057860  137618 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 13:21:14.057847  137618 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 13:21:14.057885  137618 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 13:21:14.057875  137618 remote_available_controller.go:466] Updating v1.policy
  I1009 13:21:14.057902  137618 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 13:21:14.057912  137618 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 13:21:14.057917  137618 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 13:21:14.057944  137618 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 13:21:14.057946  137618 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 13:21:14.057971  137618 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 13:21:14.057958  137618 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 13:21:14.058006  137618 local_available_controller.go:207] Updating v1.apps
  I1009 13:21:14.058045  137618 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 13:21:14.058036  137618 remote_available_controller.go:466] Updating v1.apps
  I1009 13:21:14.058074  137618 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 13:21:14.058062  137618 local_available_controller.go:207] Updating v2.autoscaling
  I1009 13:21:14.058090  137618 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 13:21:14.058098  137618 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 13:21:14.058092  137618 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 13:21:14.058115  137618 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 13:21:14.058117  137618 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 13:21:14.058128  137618 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 13:21:14.058138  137618 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 13:21:14.144134  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0010812383880541559 seatDemandStdev=0.03286440797583845 seatDemandSmoothed=7.4911270849369105 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:14.144209  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:14.144240  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:14.144262  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:14.144283  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:14.144321  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:14.144338  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:14.144363  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:14.281128  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.281715ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="e6062f95-2db6-4034-8ad7-845395751a2d" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.013983ms" resp=200
  I1009 13:21:15.120412  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.180906ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f99db9d2-fc60-4a24-875a-14d4810ee5fc" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.954403ms" resp=200
  I1009 13:21:15.152230  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="840.2µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c3a2f7e9-3129-4a84-9cc1-24b4c36c95ca" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="617.556µs" resp=200
  I1009 13:21:15.154375  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="1.176184ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bffe49de-70ca-4f96-90c0-fe6c293ebdf8" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="984.473µs" resp=200
  I1009 13:21:15.686608  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.060775ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="baf4b147-634b-4991-acc2-5d064065fd2b" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.797701ms" resp=200
  I1009 13:21:16.284265  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.464617ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="5f865f48-57a9-484b-813b-dd005d68568b" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.199755ms" resp=200
  I1009 13:21:17.158762  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/events/test-pod.17fccb7518bc752e" latency="2.442658ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="b1ce8839-0983-4a67-a30c-a658514943e2" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.175066ms" resp=200
  I1009 13:21:18.287412  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.753431ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="a409aca6-2fa9-472d-ae45-4266381192e6" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.487159ms" resp=200
  I1009 13:21:20.289929  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.261685ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="15cf6049-f568-4484-9147-aa7bf1964101" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.049862ms" resp=200
  I1009 13:21:22.293235  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.446848ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="2f17ab3d-681a-45fb-b0ad-1ee233d469bd" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.168415ms" resp=200
  I1009 13:21:24.145202  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:24.145306  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:24.145326  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0013615900796165987 seatDemandStdev=0.03687460036490821 seatDemandSmoothed=7.319710594363586 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:24.145343  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:24.145361  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:24.145385  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:24.145415  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:24.145435  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:24.295803  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.383826ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="d87b044b-8376-4018-bae3-d22dcfdcdeab" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.083783ms" resp=200
  I1009 13:21:25.151922  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="782.949µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="71feea56-86a3-4196-957c-1d9be7ca3af5" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="623.108µs" resp=200
  I1009 13:21:25.153256  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="535.907µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="54e2cb0c-9b3f-4b0b-b53e-dc22dbf97a32" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="447.555µs" resp=200
  I1009 13:21:25.208555  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.830272ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="17602e42-800b-4cc1-89dc-3b8d21cd3e01" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.622419ms" resp=200
  I1009 13:21:25.762321  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.883692ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="33ef5ae0-dac0-4733-93a8-bb1f1ce87b1d" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.617489ms" resp=200
  I1009 13:21:26.299045  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.372756ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="3af073db-1c8d-4218-ad12-cff6e3e558cb" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.122673ms" resp=200
  I1009 13:21:28.302225  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.419297ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b19d8390-b39c-478d-af23-7ad5731ff5d2" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.182234ms" resp=200
  I1009 13:21:30.304969  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.236225ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="6e5b883b-6c7e-4aa0-8021-6fe8b5de79cf" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.026312ms" resp=200
  I1009 13:21:32.308443  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.556039ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="78d61941-8ae5-43f5-8d91-1866c47ee98e" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.298465ms" resp=200
  I1009 13:21:34.146497  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:34.146584  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0010143836756553637 seatDemandStdev=0.03183323265730214 seatDemandSmoothed=7.15211274586888 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:34.146603  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:34.146621  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:34.146642  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:34.146657  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:34.146673  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:34.146690  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:34.311681  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.68531ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="e91ed6cf-6344-4f5b-9cca-e5248d13f47d" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.423205ms" resp=200
  I1009 13:21:35.152280  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.167363ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e833deb1-abb8-47b9-bc91-31cfa2a71a65" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="971.651µs" resp=200
  I1009 13:21:35.153573  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="605.087µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="77adcd71-029a-4c86-8692-660cbfe6535d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="506.236µs" resp=200
  I1009 13:21:35.363760  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.830141ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="29dadb2d-7e81-40df-b7b4-7776c48f7df3" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.576118ms" resp=200
  I1009 13:21:36.158528  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.70705ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ec14994b-57b9-4b06-8048-8512745c9516" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.462427ms" resp=200
  I1009 13:21:36.314828  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.256035ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="3d3df457-7e23-4716-aa09-9e0c03075c42" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.025453ms" resp=200
  I1009 13:21:38.317935  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.346086ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="ae36c30e-a3d1-4565-80c9-1919fd89a103" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.114174ms" resp=200
  I1009 13:21:40.320534  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.506357ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="c96b2c4e-47dc-494e-a7d7-268d9f09d911" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.194245ms" resp=200
  I1009 13:21:42.323670  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.400557ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="f3b52cd5-9e47-4d9a-8281-f1f2f0d191f9" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.130593ms" resp=200
  I1009 13:21:44.058282  137618 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 13:21:44.058401  137618 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 13:21:44.058434  137618 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 13:21:44.058442  137618 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 13:21:44.058512  137618 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 13:21:44.058519  137618 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 13:21:44.058537  137618 local_available_controller.go:207] Updating v1.policy
  I1009 13:21:44.058545  137618 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 13:21:44.058560  137618 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 13:21:44.058556  137618 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 13:21:44.058610  137618 remote_available_controller.go:466] Updating v1.policy
  I1009 13:21:44.058566  137618 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 13:21:44.058635  137618 local_available_controller.go:207] Updating v2.autoscaling
  I1009 13:21:44.058620  137618 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 13:21:44.058652  137618 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 13:21:44.058673  137618 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 13:21:44.058684  137618 local_available_controller.go:207] Updating v1.apps
  I1009 13:21:44.058687  137618 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 13:21:44.058749  137618 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 13:21:44.058757  137618 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 13:21:44.058696  137618 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 13:21:44.058769  137618 remote_available_controller.go:466] Updating v1.apps
  I1009 13:21:44.058784  137618 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 13:21:44.058778  137618 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 13:21:44.058794  137618 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 13:21:44.058804  137618 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 13:21:44.058800  137618 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 13:21:44.058819  137618 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 13:21:44.058813  137618 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 13:21:44.058855  137618 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 13:21:44.058870  137618 local_available_controller.go:207] Updating v1.autoscaling
  I1009 13:21:44.058878  137618 local_available_controller.go:207] Updating v1.batch
  I1009 13:21:44.058899  137618 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 13:21:44.058907  137618 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:21:44.058912  137618 remote_available_controller.go:466] Updating v1.batch
  I1009 13:21:44.058923  137618 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 13:21:44.058932  137618 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 13:21:44.058943  137618 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:21:44.058934  137618 local_available_controller.go:207] Updating v1.
  I1009 13:21:44.058960  137618 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 13:21:44.058969  137618 remote_available_controller.go:466] Updating v1.
  I1009 13:21:44.147578  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:44.147676  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:44.147704  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:44.147726  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:44.147750  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:44.147767  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:44.147795  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:44.147824  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.001053960185646177 seatDemandStdev=0.03244764018496953 seatDemandSmoothed=6.9883846895224195 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:44.327000  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.219794ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="df540760-b711-4531-9e6b-698c9fe67b29" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="986.982µs" resp=200
  I1009 13:21:45.041837  137618 controller.go:106] OpenAPI AggregationController: Processing item k8s_internal_local_kube_aggregator_types
  I1009 13:21:45.042025  137618 controller.go:106] OpenAPI AggregationController: Processing item openapiv2converter
  I1009 13:21:45.153168  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="937.762µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4fb190a4-2047-4af6-8563-627dd0f3c242" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="787.799µs" resp=200
  I1009 13:21:45.155297  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="1.023422ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="271653b5-ee25-4082-8d6f-6002f5aa89ce" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="771.368µs" resp=200
  I1009 13:21:45.406328  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.998244ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="ff4942c4-62a6-404a-8ea2-d1fcb1dd011d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.74268ms" resp=200
  I1009 13:21:46.321425  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.054974ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="22113aad-e1e3-464e-84f7-5b2d912cea65" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.769641ms" resp=200
  I1009 13:21:46.330017  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.464258ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="7902f5c3-1b00-4511-a463-1bd5742187ae" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.255885ms" resp=200
  I1009 13:21:48.333816  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.870212ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="639fdb75-558d-41d0-997b-3ddcfaa98a3a" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.539948ms" resp=200
  I1009 13:21:50.337592  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.784941ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="41e9f825-ce1c-41a8-952b-9c301a5db90f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.493377ms" resp=200
  I1009 13:21:52.341381  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.620028ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="5d677d11-4782-40fe-9385-044454c6c755" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.361785ms" resp=200
  I1009 13:21:54.148906  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:21:54.148992  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0011885922334765884 seatDemandStdev=0.034455471002137056 seatDemandSmoothed=6.828471655117823 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:21:54.149011  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:21:54.149029  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:21:54.149047  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:21:54.149071  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:21:54.149088  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:54.149115  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:21:54.344900  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.77039ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="155eb8f3-5313-4c5e-bbb0-49da37c4426f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.435177ms" resp=200
  I1009 13:21:55.153026  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="867.141µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="6931c345-b586-4bd0-99ec-8beca83200d5" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="669.298µs" resp=200
  I1009 13:21:55.154605  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="677.238µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="1832feea-3678-459e-aff0-2dc141bf6579" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="577.107µs" resp=200
  I1009 13:21:55.550656  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.872361ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="f46a417f-b7d0-41fa-96e4-34f6036524d6" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.633139ms" resp=200
  I1009 13:21:56.348420  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.326905ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="0a1d172f-37b9-4f12-8fa3-b5e8e0f006f9" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.064052ms" resp=200
  I1009 13:21:56.693586  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.67785ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="79c94425-7c5e-45dd-9a44-cbcd8e578e29" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.427046ms" resp=200
  I1009 13:21:58.352055  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.571298ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="b0e39e63-8a78-4548-91be-ddae118a4089" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.254134ms" resp=200
  I1009 13:22:00.355271  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.644269ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="40e93f2d-bce6-4b61-9e91-25cad14c34b8" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.384995ms" resp=200
  I1009 13:22:01.160419  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/events/test-pod.17fccb7518bc752e" latency="3.154976ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="eeef7441-9749-41e0-89e1-d674d91d5cdf" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.911683ms" resp=200
  I1009 13:22:02.358721  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.692748ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="61420a0f-f333-40a2-a0fc-76755bfb4987" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.377335ms" resp=200
  I1009 13:22:04.149630  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:22:04.149723  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:22:04.149760  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:04.149815  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:04.149848  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:22:04.149872  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0013898316621439133 seatDemandStdev=0.0372545303298111 seatDemandSmoothed=6.672305627375928 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:22:04.149890  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:22:04.149926  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:22:04.362187  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="2.143154ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="9d2a8689-5178-40b3-92d5-fbbbbb99bd3f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.755929ms" resp=200
  I1009 13:22:05.153403  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="724.868µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="4e5400df-0517-4964-92c5-b89dcc806260" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="554.837µs" resp=200
  I1009 13:22:05.154936  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="806.93µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="02ca0199-1bcb-492b-98c2-e4f61ff4fffd" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="715.868µs" resp=200
  I1009 13:22:05.873580  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.84604ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="bcd4e71d-b97c-42ae-a941-49d44472dad1" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.633508ms" resp=200
  I1009 13:22:06.366388  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.622129ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="cac5ae04-6844-4e19-af39-89891f045f06" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.377705ms" resp=200
  I1009 13:22:07.085010  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="1.911822ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c333e484-e9b0-4820-96a7-c8ce648f1a95" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.590518ms" resp=200
  I1009 13:22:08.369680  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.816301ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="fcb2e0a4-66d9-4454-90ba-9746ec64bb34" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.541547ms" resp=200
  I1009 13:22:10.372329  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.467896ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4b5f2e13-69d7-4c2f-a466-8f1759a4c5ab" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.221614ms" resp=200
  I1009 13:22:12.376094  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.938313ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="6a1a7147-7959-4421-a246-ab1af3a84fdf" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.611329ms" resp=200
  I1009 13:22:14.059233  137618 reflector.go:389] pkg/client/informers/externalversions/factory.go:141: forcing resync
  I1009 13:22:14.059521  137618 remote_available_controller.go:466] Updating v1.discovery.k8s.io
  I1009 13:22:14.059542  137618 remote_available_controller.go:466] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:22:14.059568  137618 local_available_controller.go:207] Updating v1.discovery.k8s.io
  I1009 13:22:14.059589  137618 local_available_controller.go:207] Updating v1.flowcontrol.apiserver.k8s.io
  I1009 13:22:14.059620  137618 local_available_controller.go:207] Updating v1.node.k8s.io
  I1009 13:22:14.059653  137618 local_available_controller.go:207] Updating v1.
  I1009 13:22:14.059680  137618 remote_available_controller.go:466] Updating v1.node.k8s.io
  I1009 13:22:14.059687  137618 remote_available_controller.go:466] Updating v1.
  I1009 13:22:14.059699  137618 remote_available_controller.go:466] Updating v1.admissionregistration.k8s.io
  I1009 13:22:14.059691  137618 local_available_controller.go:207] Updating v1.admissionregistration.k8s.io
  I1009 13:22:14.059715  137618 remote_available_controller.go:466] Updating v1.autoscaling
  I1009 13:22:14.059737  137618 local_available_controller.go:207] Updating v1.autoscaling
  I1009 13:22:14.059760  137618 local_available_controller.go:207] Updating v1.batch
  I1009 13:22:14.059782  137618 local_available_controller.go:207] Updating v1.policy
  I1009 13:22:14.059792  137618 remote_available_controller.go:466] Updating v1.batch
  I1009 13:22:14.059801  137618 local_available_controller.go:207] Updating v1.rbac.authorization.k8s.io
  I1009 13:22:14.059809  137618 local_available_controller.go:207] Updating v1.apiextensions.k8s.io
  I1009 13:22:14.059811  137618 remote_available_controller.go:466] Updating v1.policy
  I1009 13:22:14.059841  137618 remote_available_controller.go:466] Updating v1.rbac.authorization.k8s.io
  I1009 13:22:14.059820  137618 local_available_controller.go:207] Updating v1.authorization.k8s.io
  I1009 13:22:14.059856  137618 local_available_controller.go:207] Updating v1.coordination.k8s.io
  I1009 13:22:14.059860  137618 remote_available_controller.go:466] Updating v1.apiextensions.k8s.io
  I1009 13:22:14.059874  137618 remote_available_controller.go:466] Updating v1.authorization.k8s.io
  I1009 13:22:14.059899  137618 remote_available_controller.go:466] Updating v1.coordination.k8s.io
  I1009 13:22:14.059866  137618 local_available_controller.go:207] Updating v1.networking.k8s.io
  I1009 13:22:14.059919  137618 local_available_controller.go:207] Updating v1.apps
  I1009 13:22:14.059951  137618 remote_available_controller.go:466] Updating v1.networking.k8s.io
  I1009 13:22:14.059974  137618 remote_available_controller.go:466] Updating v1.apps
  I1009 13:22:14.059988  137618 remote_available_controller.go:466] Updating v1.authentication.k8s.io
  I1009 13:22:14.059995  137618 remote_available_controller.go:466] Updating v2.autoscaling
  I1009 13:22:14.059973  137618 local_available_controller.go:207] Updating v1.authentication.k8s.io
  I1009 13:22:14.060012  137618 local_available_controller.go:207] Updating v2.autoscaling
  I1009 13:22:14.060032  137618 remote_available_controller.go:466] Updating v1.events.k8s.io
  I1009 13:22:14.060040  137618 remote_available_controller.go:466] Updating v1.certificates.k8s.io
  I1009 13:22:14.060050  137618 local_available_controller.go:207] Updating v1.events.k8s.io
  I1009 13:22:14.060058  137618 local_available_controller.go:207] Updating v1.certificates.k8s.io
  I1009 13:22:14.060060  137618 remote_available_controller.go:466] Updating v1.scheduling.k8s.io
  I1009 13:22:14.060077  137618 remote_available_controller.go:466] Updating v1.storage.k8s.io
  I1009 13:22:14.060078  137618 local_available_controller.go:207] Updating v1.scheduling.k8s.io
  I1009 13:22:14.060094  137618 local_available_controller.go:207] Updating v1.storage.k8s.io
  I1009 13:22:14.150631  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:22:14.150722  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.001214723255143967 seatDemandStdev=0.034831705421316646 seatDemandSmoothed=6.51967166580584 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:22:14.150763  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:22:14.150787  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:22:14.150813  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:22:14.150835  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:22:14.150856  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:14.150879  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:14.379525  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.536557ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="03849951-6471-4985-91bf-c8db54e3285f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.232914ms" resp=200
  I1009 13:22:15.154220  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.313025ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="9a4eb53e-462c-49c3-89ff-d7fc93a1da35" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.004531ms" resp=200
  I1009 13:22:15.156493  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="757.449µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="963ea46d-4c8c-4f62-8d63-24b08d695c69" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="633.998µs" resp=200
  I1009 13:22:16.175090  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="1.883671ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="e5b1f9a0-aa16-4989-a510-89ebc92c249d" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.634278ms" resp=200
  I1009 13:22:16.383151  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.73685ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="f3afe699-b22e-45a7-81ea-5953d61f6308" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.376435ms" resp=200
  I1009 13:22:17.198028  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.047384ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="629ae1c1-da00-44bc-9e77-1adfdabf7ceb" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.76459ms" resp=200
  I1009 13:22:18.386575  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.531107ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="44f95088-aba6-4267-b6b6-dae1eba44881" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.308925ms" resp=200
  I1009 13:22:20.389660  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.433977ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="7899087e-2afb-4bbb-93d2-8aaf918f782e" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.167384ms" resp=200
  I1009 13:22:22.392528  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.655098ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="cf0c7aee-ba4a-48ff-90f4-bbab04550871" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.425836ms" resp=200
  I1009 13:22:24.151381  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-low" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:24.151526  137618 apf_controller.go:493] "Update CurrentCL" plName="global-default" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=56 concurrencyDenominator=56 backstop=false
  I1009 13:22:24.151567  137618 apf_controller.go:493] "Update CurrentCL" plName="catch-all" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=30 concurrencyDenominator=30 backstop=false
  I1009 13:22:24.151587  137618 apf_controller.go:493] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0011740685130494071 seatDemandStdev=0.034244562724264314 seatDemandSmoothed=6.370533846010763 fairFrac=2.3333333333333335 currentCL=1 concurrencyDenominator=1 backstop=false
  I1009 13:22:24.151607  137618 apf_controller.go:493] "Update CurrentCL" plName="system" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=117 concurrencyDenominator=117 backstop=false
  I1009 13:22:24.151634  137618 apf_controller.go:493] "Update CurrentCL" plName="node-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=170 concurrencyDenominator=170 backstop=false
  I1009 13:22:24.151656  137618 apf_controller.go:493] "Update CurrentCL" plName="leader-election" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=58 concurrencyDenominator=58 backstop=false
  I1009 13:22:24.151673  137618 apf_controller.go:493] "Update CurrentCL" plName="workload-high" seatDemandHighWatermark=0 seatDemandAvg=0 seatDemandStdev=0 seatDemandSmoothed=0 fairFrac=2.3333333333333335 currentCL=114 concurrencyDenominator=114 backstop=false
  I1009 13:22:24.395687  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.880951ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="dfbebd4d-ff9d-45b9-8b18-b58e7599e41d" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.546968ms" resp=200
  I1009 13:22:25.158236  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/default/endpoints/kubernetes" latency="1.404715ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="a6461b51-3a2c-4c17-9995-3b8e69869583" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.036172ms" resp=200
  I1009 13:22:25.160112  137618 httplog.go:134] "HTTP" verb="GET" URI="/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes" latency="993.711µs" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c1436045-7355-4762-8a94-5b91fbc16f38" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="745.439µs" resp=200
  I1009 13:22:26.399404  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.734209ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="5d5e4382-24bd-4197-ad73-0e267fb7d174" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.453806ms" resp=200
  I1009 13:22:26.572977  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-4avpqqf6pv5xy3a2dw2qimt4jq" latency="2.044583ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="41a37df8-c5a4-4922-95b0-450d8ace9603" srcIP="127.0.0.1:48512" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.838351ms" resp=200
  I1009 13:22:27.256650  137618 httplog.go:134] "HTTP" verb="PUT" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/srv579909?timeout=10s" latency="2.005602ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="c42fa1a2-a562-41e8-87d4-1d05c697024e" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.729139ms" resp=200
  I1009 13:22:27.368783  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.383705ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="999e6077-56d2-409d-82ca-20af9fa95059" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.107482ms" resp=200
  I1009 13:22:27.373053  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod/status" latency="3.606441ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="13ac3399-9d0b-4e83-81ea-a700819cb86e" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.429978ms" resp=200
  I1009 13:22:28.402731  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.714789ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="4cbd47a4-bf79-479b-ab55-f62b3024002c" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.451336ms" resp=200
  I1009 13:22:29.372991  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.662379ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="15fa9f91-db9a-4995-9f99-8489e24691ed" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.363405ms" resp=200
  I1009 13:22:29.376786  137618 httplog.go:134] "HTTP" verb="PATCH" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod/status" latency="3.062564ms" userAgent="kubelet/v1.32.0 (linux/amd64) kubernetes/b2031b3" audit-ID="927b82e3-acad-4949-b3c9-da02e6ccc146" srcIP="127.0.0.1:48680" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.916063ms" resp=200
  I1009 13:22:30.406013  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.973222ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="66eecfcb-3deb-41db-9b14-08d91728a787" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.6652ms" resp=200
  I1009 13:22:30.408768  137618 httplog.go:134] "HTTP" verb="GET" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod" latency="1.075272ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="3813dac0-730b-400a-9c0c-60ecdf6c223f" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="900.23µs" resp=200
  I1009 13:22:30.417695  137618 httplog.go:134] "HTTP" verb="CONNECT" URI="/api/v1/namespaces/prestop-hook-test-2352/pods/test-pod/log?container=regular-1&previous=false" latency="7.657746ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="676ee6af-b846-4b87-9632-7952035aea4d" srcIP="127.0.0.1:48782" resp=200
  I1009 13:22:30.421086  137618 httplog.go:134] "HTTP" verb="LIST" URI="/api/v1/nodes" latency="1.979172ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="e8af053f-24c7-49bd-a1a2-130866290c1e" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.706689ms" resp=200
  I1009 13:22:30.424339  137618 httplog.go:134] "HTTP" verb="DELETE" URI="/api/v1/namespaces/prestop-hook-test-2352" latency="2.069763ms" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3 -- PreStop Hook Test Final should call the container's preStop hook and terminate it if its startup probe fails" audit-ID="7f9267d5-fb2b-48d4-b0e8-fe531139be59" srcIP="127.0.0.1:48782" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.82636ms" resp=200
  I1009 13:22:30.481752  137618 internal_services.go:81] Stopping e2e services...
  I1009 13:22:30.481814  137618 internal_services.go:84] Stopping namespace controller
  I1009 13:22:30.481825  137618 internal_services.go:91] Stopping API server
  I1009 13:22:30.481840  137618 internal_services.go:98] Stopping etcd
  I1009 13:22:30.481849  137618 internal_services.go:111] E2E services stopped.
  W1009 13:22:30.483084  137618 logging.go:55] [core] [Channel #93 SubChannel #94]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  I1009 13:22:30.483671  137618 reflector.go:311] Stopping reflector *v1.Namespace (5m0s) from k8s.io/client-go/informers/factory.go:160
  I1009 13:22:30.483738  137618 controller.go:128] Shutting down kubernetes service endpoint reconciler
  I1009 13:22:30.483818  137618 genericapiserver.go:546] "[graceful-termination] shutdown event" name="ShutdownInitiated"
  I1009 13:22:30.483831  137618 genericapiserver.go:549] "[graceful-termination] shutdown event" name="AfterShutdownDelayDuration"
  W1009 13:22:30.484882  137618 logging.go:55] [core] [Channel #29 SubChannel #30]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.485336  137618 logging.go:55] [core] [Channel #14 SubChannel #15]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.485445  137618 logging.go:55] [core] [Channel #41 SubChannel #42]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.485749  137618 logging.go:55] [core] [Channel #45 SubChannel #46]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.484885  137618 logging.go:55] [core] [Channel #141 SubChannel #142]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.485775  137618 logging.go:55] [core] [Channel #173 SubChannel #174]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.485950  137618 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  I1009 13:22:30.485930  137618 httplog.go:134] "HTTP" verb="WATCH" URI="/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=64&timeout=7m49s&timeoutSeconds=469&watch=true" latency="1m45.118324607s" userAgent="e2e_node.test/v1.32.0 (linux/amd64) kubernetes/b2031b3/namespace-controller" audit-ID="32234ff6-8611-4dc5-a305-4ebb2f472278" srcIP="127.0.0.1:48706" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="298.172µs" apf_execution_time="298.872µs" resp=200
  W1009 13:22:30.486064  137618 logging.go:55] [core] [Channel #69 SubChannel #70]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486170  137618 logging.go:55] [core] [Channel #105 SubChannel #106]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486202  137618 logging.go:55] [core] [Channel #81 SubChannel #82]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486245  137618 logging.go:55] [core] [Channel #165 SubChannel #166]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486271  137618 logging.go:55] [core] [Channel #217 SubChannel #218]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486315  137618 logging.go:55] [core] [Channel #57 SubChannel #58]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486441  137618 logging.go:55] [core] [Channel #241 SubChannel #242]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  I1009 13:22:30.486506  137618 namespace_controller.go:214] "Shutting down namespace controller" logger="TestE2eNode leaked goroutine"
  W1009 13:22:30.486517  137618 logging.go:55] [core] [Channel #177 SubChannel #178]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486533  137618 logging.go:55] [core] [Channel #77 SubChannel #78]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486568  137618 logging.go:55] [core] [Channel #229 SubChannel #230]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486603  137618 logging.go:55] [core] [Channel #221 SubChannel #222]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486585  137618 logging.go:55] [core] [Channel #197 SubChannel #198]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486652  137618 logging.go:55] [core] [Channel #113 SubChannel #114]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486680  137618 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486687  137618 logging.go:55] [core] [Channel #213 SubChannel #214]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486721  137618 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486735  137618 logging.go:55] [core] [Channel #225 SubChannel #226]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486765  137618 logging.go:55] [core] [Channel #129 SubChannel #130]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486839  137618 logging.go:55] [core] [Channel #13 SubChannel #16]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486905  137618 logging.go:55] [core] [Channel #185 SubChannel #186]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
  W1009 13:22:30.486936  137618 logging.go:55] [core] [Channel #21 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:39285", ServerName: "localhost:39285", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:39285: connect: connection refused"
PASS
